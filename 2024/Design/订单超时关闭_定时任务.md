
2024-03-27 11:14

[[toc]]

---
---

# ALi:订单超时

bilibili

原文：
https://developer.aliyun.com/article/1175891

## 超时的用处

订单的3个地方需要用到超时处理
1. 等待付款，超时则 取消订单
2. 等待发货，超时则 取消订单，退钱给消费者
3. 等待收货确认，超时则 确认订单，打钱到商家

## JDK，DelayQueue

简单，只用JVM，不需要第三方

缺点
- 占用内存大
- 无法分布式处理
- 不适合订单量较大的场景 (。。占用内存大，重启后需要扫描数据库 +加载。 无法分布式)


## RabbitMQ 延时消息

2种
- RabbitMQ Delayed Message Plugin
  官方插件，但 非 高可用，结点宕机，消息丢失

- TTL + 死信
发送消息到 DelayExchange， 交换机 发送到 DelayQueue_5s, DelayQueue_10s...， DLXExchange绑定前面的 DelayQueue，发给 BizQueue，消费者从 BizQueue获得 延迟后的 消息
可以 支持 海量 延时消息，支持分布式处理
但，不灵活，延迟时间是固定的；使用复杂，需要配置很多延时队列


## RocketMQ 定时消息

自带延时消息

内部使用 时间论 实现

优点
- 精度高，支持任意时刻
- 开发简单，和普通消息一样

缺点
- 最大定时 24小时
- 成本高，每个订单需要一个 定时消息，且不会立刻消费，MQ的存储成本很大。 。。但无论如何都要存储。
- 如果大量消息 在同一时刻 释放， 需要处理大量的消息，会导致 消息分发延迟。 。。 无法避免


## Redis 过期监听
开启配置："notify-keyspace-events Ex"

生产不推荐
因为redis 过期是 主动过期 和 被动过期，所以 定时的时间 不准。
宕机 会丢失


## 定时任务 分布式批处理

定时任务 不停轮询数据库的订单，获得 超时的订单，分发给不同的机器 分布式处理。


![8ea1becfb55ab7b2644dfa493550e5f4.png](../_resources/8ea1becfb55ab7b2644dfa493550e5f4.png)

优点
- 稳定性强
  MQ，Redis 存在宕机，消息丢失。 数据库不会，而且 处理时失败，则下一次轮询还会 继续获得 这个订单。
- 效率高
  MQ，Redis，都是 一个订单一条消息， 而数据库本身就存储了订单，不需要额外的 成本 来存储对象。 并且 是 一次 从DB 查询一批数据，不是一条一条的，减少了sql查询次数
- 可运维
  修改数据库非常方便。
- 成本低
  不需要第三方中间件，不需要 额外的 存储空间。

缺点
- 精度
  定时任务周期大，精度低
  周期小，数据库压力大


一般要 抽离出 超时中心 和 超时库 来单独作 订单的超时调度，在阿里内部，几乎所有业务都是这样，使用基于定时任务分布式批处理的超时中心来做订单超时处理。 SLA可以做到30秒内

![39dc1cab353e07e0924df8aad4589703.png](../_resources/39dc1cab353e07e0924df8aad4589703.png)


分布式任务调度使用了 阿里自研的 SchedulerX ，兼容 主流开源任务调度系统 和 Spring的 @Scheduled。有轻量级的MapReduce。






## 总结

对于超时精度较高，超时时间24小时内，且不会有峰值压力，推荐 RocketMQ的 定时消息

在电商业务下，许多订单 超时场景都在 24小时 以上，精度要求不高，且有海量订单需要处理，推荐使用 基于 定时任务的 跑批解决方案
















# 订单超时关闭

bilibili

## 扫表轮询

定时器，周期性(10s)地扫描： 待支付 && createTime < 30分钟前

由于是周期性的，所以可能30分钟09秒 才撤单。

优点
- 简单，支持集群操作

缺点
- 业务大，数据量大，频繁读取 会是 瓶颈。
- 存在延迟，就是上面的 30分钟09秒
- 服务器内存消耗大


。。集群是指 一台服务器 0分开始 每隔2分钟， 另一台 1分开始 每隔2分钟， 还是说， 一台服务器就可以 处理 集群的 全部的订单超时？   感觉是后者， 因为前者的话，可能导致并发， 不过有 乐观锁 应该问题不大。

。。服务器内存消耗大，无法理解，可以sql limit啊。 而且 订单 不支付 应该不多吧。


。。有视频说，淘宝内部是这种。
。。减少了 中间件，增加了可靠性，(中间件越多，只要一个出问题，就影响整体)
。。而且 分库分表，使得 每个任务 面对的数据量不会很大。
。。不过视频里说， 没有那么大的体量的话，还是 MQ。。 无法理解。


## 懒删除

用户查询他的订单时，判断下 是否有订单处于 超时，如果有，就 撤单。

撤单会将 库存 +1.

缺点很严重，用户不查询的话，库存就被锁住了。 

。。不太可能单独使用。
。。或者 商家 也可以啊。 商家查询订单的时候，也可以来 判断是否超时。这样的话，感觉 可以用。


## 消息队列实现 (主流)

RocketMQ 的 延迟队列
RocketMQ的延迟消息并不支持任意时长的延迟，而是限定在一些预定义的时长内，如1秒、5秒、10秒、30秒、1分钟、2分钟等（RocketMQ商业版支持任意时长，这也是他们的一贯作风）

RabbitMQ 没有延迟队列，需要通过 TTL + 死信队列 ， 或者 官方的延迟插件( rabbitmq_delayed_message_exchange)。


### RabbitMQ
- 对queue 和 message 设置 x-message-tt，来控制 生存时间，超时则变成 deal letter
- queue可以配置 x-dead-letter-exchange，x-dead-letter-routing-key，当 queue出现 dead letter，按照 这2个参数 进行 重新 路由。



## JDK DelayQueue

哪台服务器insert了订单，哪台服务器就把这个订单放到自己的 DelayQueue中。 然后处理。

优点
- 高效，任务触发时间延迟低

缺点，
- 服务器重启，数据丢失。
- 集群扩展麻烦
- 如果下单未付款的订单太多，容易OOM
- 代码复杂度高


。。st
重启，消失， 这个确实，要配合其他机制， 比如 懒删除/扫表轮询 之类的。

不太理解集群关这个什么事， 我认为 JDK 的DelayQueue，那么肯定是单机的，单机处理的，扩展了干嘛？

OOM，应该不会，保存个 ID 就可以了，怎么可能OOM， 而且可以 nginx 分流。 而且最好的是 支付的时候 也是 同一台服务器 (就是 nginx使用 hash(ip))，这样 支付的时候 就可以 从 DelayQueue中删除， 不过 DelayQueue 好像没有这个功能 (我没看代码，但是Queue啊)， 可以 保存到 set，以后 DelayQueue 返回数据后，先 set里判断下，没有，那么 db中的状态 大概率是 未支付。 

set还可以优化，如果 订单ID自增，那么可以 使用 queue， 因为 DelayQueue， 应该能确保(。这个需要确认，必须要确保出来的是最早的。如果不是的话，性能会很差【1】。)出来的 肯定是 最早的。 所以 直接 和 queue的头比较，就像 merge 一样，queue的头 小就pop掉，直到 >= DelayQueue的头 。  但是可能 相同时间的订单，顺序未知，会导致性能下降【1】。
.==queue 不行的==， 支付时间 不同的 (下单后，有些人5秒就支付了，有些人1分钟支付)， 得用 priority_queue。 这样能保证 最小的订单号 在最前面。 而且 性能应该比 set 好很多， 因为 set的 增删，挺费事的， heap的增删就很简单。

【1】 性能下降是因为 和 queue的头 无法比较，所以导致 去数据库 查看了。

要配合其他机制的话，就必须 上事务，乐观锁

。。en

### Netty时间轮

。。也是单机的，所以就 放到 JDK 一起了。
。。在这里，时间轮 和 DelayQueue 没有什么区别啊。
。。就是 收到 订单后， 在时间轮里增加一个 30分钟后的 操作， 这个操作里 就是 判断 是否需要 撤单。

用的是 Netty 的 HashedWheelTimer

优缺点 和 JDK 的 DelayQueue 差不多。



## Redis实现


### zset

用一个线程一直判断 zset的 头 是否 过期。  
redis保存 (时间戳，订单ID)
没有数据就 sleep 500ms
有数据，就取出 score(即时间戳)，判断是否 已超时， 已超时 就 从redis删除 ( jedis.zrem )，并 去数据库 判断下 是否 未支付。

集群下，会出现 多个线程 消费 同一个 (时间戳，订单ID) 的情况。
- 使用分布式锁，性能会下降。
- 判断zrem 的返回值，如果删除了，才 去数据库判断是否已支付。。。 就是 score 已超时，那么就 zrem， 如果 zrem 返回值 > 0, 说明 被本线程 删除了， 那么 本线程 需要 去数据库判断 是否未支付。
。。不过这个有问题， 如果 zrem 后， 改订单状态前 宕机/异常。 岂不是 这个订单 就 不会再处理了？

。。所以还是需要 和 其他的机制 连用。  或者分布式锁，  反正要保证  zrem前，db的状态 已经是 撤单。


### Redis 过期监听回调

在 redis.conf 中，增加
`notify-keyspace-events Ex`

`jedis.getResource().subscribe(redisSub, "__keyevent@0__:expired");`


优点
- 服务器重启，数据不丢失 。。 不过redis 会丢啊， 得 aof,rdp 啊。
- 集群扩展方便
- 时间准确度高

缺点
- 需要额外的 redis 维护

### Redisson的 RDelayedQueue


---




# 京东-定时任务通用优化方案(从半小时到秒)

bilibili

## 整体优化思路

- 按需查询
- 分小批次游标查询
- JED场景下按数据库分片分组更新
- 精准定位要处理的数据
- 负载均衡

。JED是完全容器化部署的 分布式 数据库 中间件,兼容MySQL协议,适合海量数据事务处理的场景、支持动态在线扩展、自动备份恢复等
。。京东内部的。


## 场景

站外广告投放平台，使用了4个定时任务，
- 单元时间段更新任务
- 计划时间段更新任务
- 单元预算撞线恢复任务
- 计划预算撞线恢复任务

单元上可以设置分时段投放，最小粒度半小时。
所以需要通过定时任务，判断每个单元在当前时间段是否需要更新。


## 数据库

64分片，一主三从，分片键user_id


## 定时任务数据源

选择了只有站外广告使用的表 dsp_show_status ，大约有8500万记录
包含，3层物料，层级分别是 计划，单元，创意通过type字段区分，包含4大媒体(字节，腾讯，百度，快手)和京东播放的物料，通过 campaignType 区分
。。？


配置
jvm 8g

## 定时任务处理逻辑

1. 查询 dsp_show_status 最大主键区间 MaxAutoPk 和 最小区间 MinAutoPk
2. 根据 Ducc 中设置的 步长，和条件，去查询 dsp_show_status 得到数据。
```text
start = MinAutoPk
while (start <= maxAutoPk)
{
  end = Math.min(start + step, maxAutoPk);
  List<A> list = find(start, end, 其他条件);
  start = end + 1
}
```
3. 遍历处理第二步的数据


## 任务处理优化过程

最开始，任务执行时间长，且CPU使用率高
30分钟，80%

### 1. 分析数据，调大步长，缩短任务运行时间

表中有很多不需要的数据，导致每次find出来的数据很少，很稀疏。
所以调大步长，防止空跑

。。步长，应该放在 limit 上吧，不然 确实 可能 返回空的。 带上 主键， limit 应该不慢的。

执行时间缩短，CPU使用率更高
5分钟，100%

### 2. 减少临时对象大小和无效日志，避免多次 young gc

调试时增加了 大量日志，java序列化生成了很多临时变量。 所以去掉 无效日志
修改find，只返回想要的数据列

5分钟，CPU 60%


### 3. 基于游标查询数据，基于数据库分片批量更新，降低数据库交互次数。

使用主键作为游标。

看sql
```text
where id between #{startAutoPk} and #{endAutoPk}
and id > #{param.maxId}
order by id
limit #{param.batchSize}
```

limit 10000000, 100  会有问题，深分页
id > xxx limit 100， 不会有问题

---

没有携带 分片键 时，jed网关会发给所有的 64个分片。

增加 分片键

30s，CPU 65%， DB qps 降低99% (百万 到 万)



### 4. 异构要更新状态的数据源，降低查询出来的数据量，降低CPU使用率

表增加一个 冗余字段 nextTime，表示 下一次 要被 定时任务抓取 的时间戳。

CPU28%


### 5. 负载均衡
多台机器，拆分任务








