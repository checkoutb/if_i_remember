2024-12-15

[[toc]]

---



# ch01 绪论

本章要学习
- 理解系统的性能，人员，活动和挑战
- 理解观测和实验工具之间的区别
- 对性能观测有一个基本的了解: 统计，剖析，火焰图，跟踪，静态观测，动态观测
- 了解方法的作用和linux的60秒检查表


系统性能包括 硬件性能和软件性能。 从存储设备到 应用软件 发生的所有事情都包括在内。

系统性能涉及各种活动，下面是 软件项目生命周期中 从构思 到 开发 到 生产部署 的理想步骤。本书涵盖了帮助实施这些活动的方法和工具
1. 对未来的产品设置性能目标，建立性能模型
2. 基于软件或硬件原型进行性能特征归纳
3. 在测试环境中对正在开发的产品进行性能分析
4. 对新版本产品做非回归性测试
5. 针对软件发布版本的基准测试
6. 目标生产环境中的概念验证测试
7. 生产环境中的性能调优
8. 监测生产环境中运行的软件
9. 生产环境中的问题的性能分析
10. 对生产环境中的问题做事件回顾
11. 开发性能工具以加强生产环境分析

1-5 是传统软件产品开发过程的一部分
如果碰到问题(6-9)，说明该问题在 软件开发阶段没有 发现和修复

理想情况下， 性能工程 在 硬件选项和软件开发之前。 作为工作的第一步，可以设定性能目标并建立一个性能模型，但是，在产品开发过程中 常常缺失这一步， 性能工程 的工作 被推迟到 问题出现。

云计算为 概念验证测试(第6步) 提供了新的技术。
- 金丝雀测试，在单个实例上测试新软件，它的工作负载只是 生产工作负载的一小部分
- 蓝绿部署，流量逐渐转移到 新的实例池， 同时 旧的实例池 作为备份。
有了这些选项，新的软件通常不需要做任何性能分析，直接在生产中测试，并且在必要时回退。

---

性能分析的视角
- 负载分析，应用开发人员的视角，对最终实现的负载性能负责。
- 资源分析，系统管理员的视角，因为他是系统资源的负责人


---

性能工程总是充满挑战:
- 主观性: 磁盘平均IO是1ms 是好还是坏?
- 复杂性: 系统的复杂性， 还有 常常缺少一个明确的分析起点。 性能问题可能出现在 子系统之间复杂的互联上。


有些性能问题没有单一的原因，而是 多个促成因素


软件中可能有 多个性能问题 ( 一个调用，设计不合理，慢一点; 代码写的不合适，慢了一点; 数据库故有的缺点，慢了一点; 磁盘又慢了一点 ) 。要识别哪些是最重要的，因此 性能问题必须 量化，还要评估修复后 可以带来的增速。

---

延时(。一个请求要多少时间才收到响应) 是非常有用的指标。

随着新的基于 BPF 的观测工具的出现，现在可以从任意感兴趣的点 测量延时。

---

1.7 可观测性

指通过观测来理解一个系统。
工具包括 计数器，剖析，跟踪。  不包括 基准测试工具，基准测试工具 是实验性的，是 通过 执行工作负载来 修改系统的状态。

对于生产环境，尽可能先尝试 可观测性工具， 实验性工具 会通过 资源争夺 来扰乱生产环境。

---

计数器，统计数据，指标
vmstat
Grafann

---

剖析
剖析通常指 使用工具来采样，取得一个测量的子集 来描绘目标的粗略情况。
火焰图
iperf

---

跟踪

strace
tcpdump
ftrace
BCC
bpftrace

分为3类: 静态检测，动态检测，bpf

静态检测 是 源码中 硬编码的 软件检测点。 linux内核中，有数百个这样的点，用于检测 磁盘IO，调度器事件，系统调用等。
linux中 内核静态检测技术 被称为 tracepoint， 还有一种 针对用户空间软件的 静态检测技术，叫做 用户静态定义跟踪 ( USDT )。 USDT 被软件库(如 libc) 用来检测库调用。
例子: execsnoop 通过对 execve 的 tracepoint 进行检测，打印出 系统创建的新进程。

动态检测 是软件运行起来后，通过修改内存指令 插入检测程序 来创建检测点。 类似于 调试器 可以在 运行中的软件 的任何函数上 插入断点。 触发时，动态检测 运行一个例程，然后继续运行目标软件。
工具: DTrace

BPF (berkeley packet filter)。 最初是 内核中的一个 迷你虚拟机，用于 加快 tcpdump 的执行速度。
2013年，BPF已经被扩展 (有时称为 eBPF)，成为一个通用的内核执行环境，一个安全的能快速访问资源的环境， 所以 eBPF 的 新用途 中就有了 作为跟踪工具。
BPF 为 BPF编译器集合(BCC) 和 bpftrace 前端 提供了 可编程的能力。 前面的 execsnoop 就是 一个BCC工具

---

1.8 实验

实验工具，如 iperf 进行 TCP网络吞吐量的 微观基准测试

---

1.9 云计算

云计算是一种 按需部署资源 的方式。 降低了对 容量规划的精确程度的要求， 实现了 app的快速扩展。

对性能分析的要求更高了: 性能的优势可以带来系统使用数量的减少，从而节约成本。

云计算和虚拟化技术 也带来了新的难题: 如何管理其他租户带来的性能影响，如何让每个租户都能对物理系统做观测。    磁盘IO很可能因为 邻近租户间的竞争而下降。

---

## 1.10 方法

方法是 系统性能领域 执行各种任务的建议步骤记录下来的方式。
如果没有方法，那么性能调查就会变成: 随意尝试一些东西，希望能成功。

第二章 介绍系统性能的方法库。

以下是 我在处理任何性能问题时 都会 首先 采用的方法: 一个基于工具的检查表

1.10.1 linux性能分析60秒

下面是 基于linux工具的一个检查表， 可以在 调查性能问题的 头 60秒执行

|工具|检查|章节|
|--|--|--|
|uptime|平均负载，可以识别负载的增加或减少( 对比1分钟，5分钟，15分钟的平均值)|6.6.1|
|dmesg -T | tail|包括OOM 事件的内核错误|7.5.11|
|vmstat -SM 1|系统级别统计: 运行队列长度，交换，CPU总体使用情况|7.5.1|
|mpstat -P ALL 1|CPU平衡情况: 单个CPU很忙，意味着线程扩展性糟糕|6.6.3|
|pidstat 1|每个进程的CPU使用情况: 识别意外的CPU消费者，以及每个进程的用户/系统CPU时间|6.6.7|
|iostat -sxz 1|磁盘IO统计: IOPS 和吞吐量，平均等待时间，忙碌百分比|9.6.1|
|free -m|内存使用情况，包括文件系统的缓存|8.6.2|
|sar -n DEV 1|网络设备IO: 数据包和吞吐量|10.6.6|
|sar -n TCP,ETCP 1|TCP统计: 连接率，重传|10.6.6|
|top|检查概览|6.6.6|

后续会介绍更多性能分析方法。如: USE方法(2.5.9)，工作负载特征归纳，延时分析 等。

。使用USE方法 制作可视化工具。


## 1.11 案例

### 缓慢的磁盘

。很行云流水。

mpstat, iostat

磁盘使用率很高(80%)

/sys目录中 磁盘错误数

offcputime

文件系统碎片 会在 文件系统空间使用 接近100%时出现。 现在 磁盘空间使用率才 30%

文件系统缓存(页缓存) 未命中

cachestat 查看 文件系统缓存的命中率， 91%，看起来很好，但是没有历史数据比较。 登录其他 相似工作负载的服务器上，发现它们的 超过 97%。  同时发现 问题服务器上的 文件系统缓存 要比其他服务器大很多。

### 软件变更

app 新开发了一个 新的核心功能， 在部署生产浅，决定做一次 非回归性测试 ( 非回归性测试 是用来确定 软件或硬件的变更 没有让性能倒退)。

。。好多东西

执行了线程状态分析，发现 这是一个 单线程软件， 单线程100%的执行时间都花在一个CPU上。

。。就是 压测， 达到上限， 但是 CPU不满载。



# ch02 方法

如何从发现信号到找到解决方案

学习目标
- 了解关键性能指标: 延时，使用率，饱和度
- 培养对观测时间尺度的感觉，精确到纳秒
- 学习调优的权衡，目标，以及何时停止分析
- 识别工作负载问题 和 架构问题
- 考虑资源分析与 工作负载分析
- 使用不同的性能方法，包括
  - USE方法
  - 工作负载特征归纳
  - 延时分析
  - 静态性能调优
  - 性能箴言
- 理解统计学和排队理论的基础知识


## 2.1 术语

- IOPS， 每秒的输入，输出操作的次数
- 吞吐量， 字节/秒， 某些情况 (如 数据库) 指 操作的速度(每秒操作数 或每秒业务数)
- 响应时间， 完成一次操作的时间
- 延时， 操作中 用于等待服务的 时间
- 使用率， 对于服务所请求的资源，是 给定时间区间内，资源的繁忙程度; 对于提供存储的资源，是 消耗的存储容量
- 饱和度， 某一资源无法提供服务的工作的排队程度
- 瓶颈， 限制系统性能的那个资源
- 工作负载， 系统的输入 或 对系统所施加的负载
- 缓存， 用于复制或缓冲的 高速存储区域，目的是为了避免对较慢的存储层级的直接访问，从而提高性能。


---

## 模型

### 受测系统SUT

system under test

扰动是会影响结果的，包括 定时执行的系统任务，系统中的其他用户 及 其他工作负载导致的结果。

扰动的来源很难判断，在云环境中，更复杂，因为 单客户无法观测到物理主机系统的 其他活动。

另一个困难是 系统可能由若干个网络化的组件组成，它们都用于处理 工作负载，如 负载平衡，web服务器，数据库服务器，app服务器，存储系统。


### 排队系统

某些组件和资源可以模型化为排队系统，这样在不同情况下 它们的性能就可以根据 模型被预测出来。

磁盘通常被模型化为 排队系统。 

排队系统可以 预测 响应时间在负载下 如何退化的。




## 2.3 概念

### 延时

对于某些环境，延时是 被唯一关注的性能焦点。 对于其他环境，它会是 除了吞吐量之外，数一数二的分析要点。

HTTP GET请求 的网络传输，其响应时间被分为 连接延时 和 数据传输时间 2部分。  
延时 是 操作执行之前 所花费的 等待时间。 
在这个例子中，操作时 网络服务的 数据传输请求。 在这个操作发生之前，系统必须等待 建立网络连接，这就是 这个操作的延时。 
响应时间 包含了 延时 和操作时间

延时在可以在不同点测量，所以通常会执行延时测量的对象。
比如，网络载入时间 由3个不同点测得的不同时间组成: DNS延时，TCP连接延时，TCP数据传输时间。
在更高的层级，所有这些，包括 TCP数据传输时间， 会被当做另一种延时。例如，从用户单击网站链接 到 网页完全载入 都可以被当做延时， 其中包括 浏览器渲染生成网页的时间。
所以 延时需要加上 限定词 解释 它测量的是什么。

### ==不同操作的时间度量==

3.5GHz的CPU

|事件|延时|相对时间比例|
|--|--|--|
|1个CPU周期|0.3ns|1s|
|L1缓存访问|0.9ns|3s|
|L2|3ns|10s|
|L3|10ns|33s|
|主存访问(从CPU访问DRAM)|100ns|6min|
|固态硬盘IO|10-100us|9-90h|
|机械硬盘IO|1-10ms|1-12月|
|互联网:旧金山-纽约|40ms|4年|
|互联网:旧金山-英国|81ms|8年|
|轻量级硬件虚拟化重启|100ms|11年|
|互联网:旧金山-澳大利亚|183ms|19年|
|操作系统虚拟化系统重启|<1s|105年|
|基于TCP定时器的重传|1-3s|105-317年|
|SCSI命令超时|30s|3千年|
|硬件虚拟化系统重启|40s|4千年|
|物理系统重启|5min|32千年|


---

调优的影响

|层级|调优对象|
|--|--|
|app|app逻辑，请求队列大小，执行的数据库请求|
|数据库|数据库表的布局，索引，缓冲|
|系统调用|内存映射或读写，同步或异步IO|
|文件系统|记录尺寸，缓存尺寸，文件系统可调参数，日志|
|存储|RAID级别，磁盘类型和数目，存储可调参数|

app级别的 调优效果 最显著

OS，数据库 的 观测效果 最好。


### 2.3.6 何时停止分析

当你已经解释了大部分性能问题的时候

当潜在的投资回报率低于分析的成本的时候

当其他地方有更大的投资回报率的时候

---

性能推荐 仅在一段时间有效， 经过 一次 软件，硬件 升级后， 可能就失效了

改变 可调整参数的时候， 存储到 版本控制系统中 是 很有帮助的


---

扩展性

在刚开始的阶段，扩展性是 线性变化的， 当达到某一点时，对于资源的争夺开始 影响性能，过了这一点，吞吐量就 不在是 线性， 最终 争夺加剧的开销和一致性导致 吞吐量 下降。

![90a2ae9e6f4adec3878be0bff96cb87b.png](../_resources/90a2ae9e6f4adec3878be0bff96cb87b.png)

![a7fda71181d0079dfc511a4e9ab35c1d.png](../_resources/a7fda71181d0079dfc511a4e9ab35c1d.png)

通常拐点是资源使用率达到100%时。

---

使用率

基于时间的: 服务器或资源 繁忙时间的均值
U = B/T
U是使用率，B 是T时间内系统的繁忙时间，T是观测周期。

。。应该是 100%使用率 就是繁忙。

当一个组件的使用率达到100%时，资源发生竞争，性能会有严重的下降。 这时可以检查 确认该组件是不是已经成为系统瓶颈。

某些组件能并行地为 多个操作提供服务， 对于这些组件，100%使用率时，性能下降的幅度可能不大，因为它们仍然能接受更多的工作。  比如 电梯，一个人是100%繁忙，10个人也是100%繁忙。
。。不过 感觉应该没有这种， 不然 肯定举例举出来了，不可能找一个 非计算机的东西 来举例。
。。下面就是。。

100%忙碌的磁盘也可以接受并处理更多的工作，例如 通过把写入的数据 放入磁盘内部的缓存中，稍候 完成写入，就可以做到这一点。


基于容量
用容量 而不是 时间来定义使用率。 此时 100%使用率的磁盘 不能再接受更多的工作。


---

饱和度

随着工作流增加 而对资源的请求 超过资源所能处理的程度 叫饱和度。

饱和度发生在 100% 使用率时 (基于容量)， 这时多出的工作无法被处理，开始排队。


---

剖析 profiling

计算机领域，剖析通常是 按照特定时间间隔 对系统的状态进行采样，然后对这些样本进行研究。



---

2.3.14 缓存

用来提高性能

一般使用多级缓存。CPU的 L1 L2 L3

3.2.11 节 有一张系统缓存的列表。

缓存的重要指标是 命中率: 命中率 = 命中次数 / (命中次数 + 失效次数)

随着缓存命中率上升，预期的性能提升是==曲线==
98% 与 99% 之间的性能差异 比 10%与11% 之间的 大很多。
。。按照未命中率算: 98-99，直接减少一半的未命中
。命中 和失效 之间的速度差异，导致了 这是一条非线性曲线。
。。命中1ms，失效100ms， 98%: 0.98+2，99%: 0.99+1, 10%: 0.1+90, 11: 0.11+89


缓存的另一个指标是 缓存的失效率，即 每秒缓存失效的次数。 这个对性能是 线性影响
。。?


### 缓存算法

MRU 最近最常使用， 缓存保留策略
LRU 最近最少使用， 回收策略
MFU 最常使用
LFU 最不常使用
NFU 不常使用

### 缓存的热，冷，温

冷: 缓存中空的，或是无用的数据。 命中率接近0
热: 缓存中都是有用的数据，且 命中率很高(99%)
温: 填充了有用的数据，但是命中率还不够理想
热度: 就是命中率

如果缓存较大 或 下一级缓存的速度较慢， 则需要较长的时间来填充缓存。

例如，一台存储服务器，128g的DRAM作为文件系统的缓存，600g的闪存作为二级缓存，物理磁盘作为存储器。 在随机读取的工作负载下，磁盘 每秒的读操作 约2000次。 按照8kb的IO大小，意味着 缓存的 变温速度只有 16MB/s (800k * 2000)。 需要2小时 让DRAM变热， 超时10小时 让闪存变热


2.3.15 已知的未知


---

2.4 视角

性能分析有2个常用视角。
- 资源分析，以系统资源的分析为起点，涉及的系统资源有:CPU，内存，磁盘，网卡，总线。 指标: IOPS，吞吐量，使用率，饱和度。 工具: vmstat, iostat, mpstat
- 工作负载分析，施加的负载 和 app的响应。 关注 吞吐量，延时


## 2.5 ==方法==

下面有20个通用的性能分析方法。 
后续还会有: 2.6.5 排队理论， 2.7 容量规划， 2.8.1 量化性能收益， 2.9 检测

还有几个额外的性能分析方法
- 1.10.1 Linux性能分析60秒
- 5.4.1 CPU剖析
- 5.4.2 Off-CPU分析
- 6.5.5 周期分析
- 6.5.8 优先级调用
- 6.5.10 CPU绑定
- 7.4.6 泄漏检测
- 7.4.10 内存收缩
- 8.5.1 磁盘分析
- 8.5.7 工作负载分离
- 9.5.10 扩展
- 10.5.6 包嗅探
- 10.5.7 TCP分析
- 12.3.1 被动基准测试
- 12.3.2 主动基准测试
- 12.3.7 逐渐增加负载
- 12.3.8 合理性检查


### 2.5.1 街灯伪方法

不是一个深思熟虑的方法。

用户通常选择熟悉的观测工具来分析性能。

来自街灯效应: 醉汉在路灯下找东西，警察问他在找什么，醉汉说找钥匙。警察也帮忙找，找不到，问醉汉，你确定是这里丢的? 醉汉: 不，但这里光最亮。

相当于 查看 top。 没有理由，只是因为 不知道怎么用其他工具。

---

### 2.5.2 随机变动伪方法

实验性的， 用户随机猜测 可能存在问题的位置， 然后做修改，直到问题消失。

1. 任意选择一个项目进行改动
2. 朝某个方向做修改
3. 测量性能
4. 朝另一个方向做修改
5. 测量性能
6. 3,5的性能是否好于基准值，是就保留修改。返回步骤1

---

### 2.5.3 责怪他人伪方法

1. 找到不是你负责的系统组件
2. 假定问题和这个组件有关
3. 把问题扔给负责那个组件的团队
4. 如果证明错了，返回步骤1


---

### 2.5.4 Ad Hoc 核对清单法

检查和调试系统时， 技术支持人员 通常会花一点时间 一步步地 过一遍核对清单。

典型场景: 生产 部署新服务器或应用时， 技术支持人员会花半天时间来 检查系统在真实压力下的场景问题。
这类核对清单是 ad hoc的，基于对该系统类型的经验 和 之前遇到过的问题。

比如: 检查清单中一项: 运行 `iostat -x 1` 检查 r_await 列，如果该列在负载下持续超过10ms，那么说明磁盘太慢 或磁盘过载

这种清单能在 最短时间内提供最大价值。

---

### 2.5.5 问题陈述法

明确如何陈述问题是支持人员反映问题时的例行工作。
可以询问客户以下问题来完成
1. 是什么让你认为有性能问题?
2. 系统之前运行得好吗?
3. 最近有什么改动? 软件，硬件，负载?
4. 问题能用 延时 或运行时间来表述吗?
5. 问题影响其他人或app吗?
6. 环境是什么样的?用了哪些软件和硬件?是什么版本?怎么配置的?


---

### 2.5.6 科学法

通过假设和测试来研究未知的问题

有以下步骤
1. 问题
2. 假设
3. 预测
4. 测试
5. 分析


问题: app在迁移到一个内存较小的系统时性能会下降
假设: 文件系统缓存较小导致性能下降
测试: 测量2个系统的缓存失效率，预测: 内存较小的系统的 缓存失效率更高
测试: 增加缓存大小，预测 性能会提升


问题: 数据库查询慢
假设: 吵闹的邻居(其他云计算租户) 在执行磁盘IO
预测: 数据库查询过程中 查看 文件系统IO延时
测试: 发现在文件系统上等待的时间是 整个查询延时的 5%以内
分析: 和文件系统没有关系


问题: 为什么http请求从主机A到主机C 比 从主机B到主机C 的时间长
假设: 主机A，B在不同的数据中心
预测: 将主机A移到 B所在的数据中心 将修复这个问题
测试: 移动主机A 并测试性能
分析: 性能得到修复

问题: 为什么随着文件系统缓存尺寸变大，文件系统的性能会下降
假设: 缓存越大，存储的东西越多，需要更多时间来操作
预测: 将数据的大小变小，使得相同大小的内存 保存下更多的数据，性能会变差
测试: 逐步减小数据的大小，测试同样的工作负载
分析: 结果绘图后与预测一致。 可以继续分析 缓存管理的逻辑


---

### 2.5.7 诊断循环

与科学方法类似:
假设 - 仪器检验 - 数据 - 新假设

通过收集数据来验证假设， 这里强调 数据可以快速引发新的假设。


### 2.5.8 工具法

以工具为导向

1. 列出所有可用的性能工具
2. 对于每种工具，列出它提供的有用的指标
3. 对于每个指标，列出阐释该指标可能的规则

受 自己知道的工具的 影响。

工具少， 视野不完整
工具多， 要费心挑选

### 2.5.9 ==USE方法==

utilization，saturation，errors
。。利用，饱和，错误。。???

用来识别系统瓶颈

术语
- 资源， 所有服务器的物理元器件(CPU，总线等)，某些软件资源也能被计算在内，提供有用的指标
- 使用率， 在规定的时间间隔内，资源用于服务工作的 时间百分比。
- 饱和度， 不能再服务更多额外工作的程度，通常有 等待队列。 也被称为 压力
- 错误， 错误事件的个数

对于某些资源类型，包括内存，使用率指 资源所用的容量。 而非基于时间的定义

错误需要被调差，因为它们会损害性能。

USE方法列举的是系统资源 而不是工具

USE方法会将分析引导到一定数量的关键指标上，这样可以尽快地核实所有的系统资源。

USE方法的使用如下图

先检查错误，因为错误很快就可以解释。
然后饱和度检查，它比使用率解释得更快，任何级别的饱和都可能是问题

- 使用率: 一定时间间隔内的百分比值 (例如，单个CPU运行在90%的使用率上)
- 饱和度: 等待队列的长度 (如，CPU的平均运行队列长度是4)
- 错误: 报告出的错误数目 (如，这块磁盘设备有50个错误)

![e00ecb76a958ce962345b2398b84c004.png](../_resources/e00ecb76a958ce962345b2398b84c004.png)

很多时候，使用率，饱和度 满 是瞬时的。 很多监测工具都是5分钟的平均值，可能无法看到 这种瞬时的 满。

---

资源列表

USE的第一步是 创建资源列表，要尽可能完整。 下面是 服务器通常的资源列表 及例子
- CPU， 插槽，核，硬件线程(虚拟CPU)
- 内存， DRAM
- 网络接口， 以太网端口，无限带宽技术(。真的是 无限，不是无线。InfiniBand)
- 存储设备， 磁盘，存储适配器
- 加速器， GPU，TPU，FPGA等
- 控制器， 存储，网络
- 互联， CPU，内存，IO

资源类型， 内存是容量资源，网络接口是IO资源。 存储设备 既是IO资源 也是 容量资源。

IO资源可以 进一步 被当做 排队系统来研究。

---

原理框图

另一种遍历所有资源的方法是 找到 或画 一张系统的原理框图。

![f0d7fbbcd802302d3af0b42bce5d34f9.png](../_resources/f0d7fbbcd802302d3af0b42bce5d34f9.png)


CPU，内存，IO互联 和 总线 常常被忽视。 所幸的是， 它们不是系统的常见瓶颈。

---

指标

有了资源的列表，就可以考虑这3类指标了: 使用率，饱和度 以及错误。

一些例子

|资源|类型|指标|
|--|--|--|
|CPU|使用率|CPU使用率(单CPU使用率或系统级均值)|
|CPU|饱和度|运行队列长度，调度器延时，CPU压力|
|内存|使用率|可用空闲内存(系统级)|
|内存|饱和度|交换(匿名分页)，页面扫描，内存缺失事件，内存压力(linux PSI)|
|网络接口|使用率|接受/发送 的 吞吐量/最大带宽|
|存储设备IO|使用率|设备繁忙百分比|
|存储设配IO|饱和度|等待队列长度，IO压力(Linux PSI)|
|存储设配IO|错误|设备错误(硬错误，软错误)|

对所有用到的资源 尝试获得 使用率，饱和度，错误 的数据。

一些较难的组合:
CPU - 错误: 如，机器检查异常，CPU缓存错误
内存 - 错误: 如，失败的malloc
网络 - 饱和度: 如，与饱和度相关的网络接口或OS 错误，如 Linux 的漫溢(overrun)
存储控制器 - 使用率: 取决于控制器，针对当前活动可能最大IOPS或吞吐量
CPU互联 - 使用率: 每个端口的 吞吐量/最大带宽 (CPU性能计数器)
内存互联- 饱和度: 内存停滞周期数，偏高的平均指令周期数 (CPU性能计数器)
IO互联 - 使用率: 总线吞吐量/最大带宽

某些指标是 无法通过OS的标准工具获得的， 可能需要 动态跟踪 或 使用 CPU性能计数工具

---

软件资源

- 互斥锁， 锁被 持有 的时间 是使用率，饱和度是指 有线程排队等待 锁
- 线程池， 线程处于 工作的时间 是 使用率，饱和度是指 等待线程池服务的 请求数目
- 进程/线程容量， 系统的进程/线程 的总数是有上限的 (。max_user_processes)， 当前的使用数是 使用率， 等待分配的数目是 饱和度，错误是 分配失败 (如，cannot fork)
- 文件描述符容量， 同上，只不过针对的是 fd

---

使用建议

使用率: 100%通常是瓶颈的信号 (检查饱和度并确认其影响)。 使用率超过 60% 很可能是问题，因为: 均值 会掩盖 100%的瞬间， 另外，一些资源，如硬盘，通常在 操作期间是不能被中断的。 随着使用率上升，排队延时会变得更频繁和明显。
饱和度: 任何程序的饱和度都是问题。 饱和度可以用 排队长度 或 排队花费的时间 来度量
错误: 错误都是值得深究的。


#### ==附录A USE方法: Linux==



![4e08f0273c91edb03d54af1f00285375.png](../_resources/4e08f0273c91edb03d54af1f00285375.png)
![f156b95666e6f66e932902a174d10a3d.png](../_resources/f156b95666e6f66e932902a174d10a3d.png)
![b6ac44256c548284d6a42a03bbbf5584.png](../_resources/b6ac44256c548284d6a42a03bbbf5584.png)

---

![880cd91c0bea86511fee33c40c0337ce.png](../_resources/880cd91c0bea86511fee33c40c0337ce.png)
![ce360fae4948c77e74e25d22f6d81cfb.png](../_resources/ce360fae4948c77e74e25d22f6d81cfb.png)



### 2.5.10 RED方法

该方法的重点是服务，通常是 微服务架构中的 云服务

从用户的角度度量 3个指标: 对于每个服务，检查 请求率，错误，持续时间。

- 请求率: 每秒的服务请求数
- 错误: 失败的请求数
- 持续时间: 请求完成的时间 (除了平均数，还要考虑分布统计，如百分位数。) (。。tp99, tp95)
 
你的任务时 画出你的微服务架构图，并确保对每个服务的这3个指标都进行监测。 (分布式跟踪工具可以为你提供这样的图)

在调差中考量 请求率 能提供一个 早期线索: 性能问题是 由于负载 还是 由于架构。 如果请求率一直很稳定，但是请求持续时间却增加了，这就说明 架构有问题。  如果请求率 和 请求持续时间都增加，那么问题可能来自 某部分施加的负载。



### 2.5.11 工作负载特征归纳

简单且高效

用于识别 由负载导致的问题。

这个方法关注系统的输入。

工作负载 可以由下列问题进行 特征归纳
- 负载 是 谁 产生的，进程ID，用户ID，远端的IP地址
- 负载为什么会被调用，代码路径，栈跟踪
- 负载的特征是什么， IOPS，吞吐量，方向类型(读入or写入)，包含变动(标准方程)
- 负载是 怎样随时间变化的， 有日常模式吗

最好的性能来自消灭不必要的工作。

如果被识别出的问题无法解决，那么可以用 系统资源控制 来限制它。


### 2.5.12 向下钻取分析

开始于 检查高层次的问题，然后根据 之前的发现 缩小关注的范围。

3个阶段
- 监测， 持续记录高级别的统计数据，如果问题出现，予以辨别和告警
- 识别， 对于给定问题，缩小研究的范围，找到可能的瓶颈
- 分析， 对特定的系统部分 进一步检查，找到问题根源并量化问题

监测可以全公司范围，将所有服务器 或云实例的结果汇总。 可以做到这一点的技术 是 ==简单网络监测协议(SNMP)==

识别，通过 直接分析服务器和检查系统组件来进行，系统组件包括: CPU，磁盘，内存 等。 过去是通过命令行工具完成的，如 vmstat，iostat，mpstat， 现在，有很多 GUI仪表盘 可以展示同样的指标。

分析工具 包括 基于跟踪或剖析的工具，用于对 可疑区域 进行更深入的检查。 linux上的工具有 strace，perf，BCC工具，bpftrace，Ftrace

---

5个why

在向下钻取分析中，还有一个可用的方法，问自己问题，然后作答，重复5遍
比如:
1. 查询多了 数据库性能就变差，why?
2. 由于内存换页，磁盘IO产生延时，why?
3. 数据库内存用量变得太大了，why?
4. 分配器消耗的内存比预计的多，why?
5. 分配器存在内存碎片问题，why?


---

### 2.5.13 延时分析

检查完成一项操作所用的时间，然后把时间再分为小的时间段，接着对有着最大延时的时间段再次进行划分，最后定位并量化问题的根本原因。

例子，mysql的请求延时分析可能涉及如下的问题
1. 存在请求延时问题吗?
   是的
2. 查询时间主要是 on-CPU 还是 off-CPU?
   off-CPU
3. 没花在CPU上的 时间在等待什么?
   文件系统IO
4. 文件系统的IO时间 花在磁盘IO还是 锁竞争上?
   磁盘IO
5. 磁盘IO时间主要用于 排队还是服务IO?
   服务
6. 磁盘服务时间主要是 IO初始化 还是 数据传输?
   数据传输


每一个问题将 延时划分为 2部分。 继续分析 延时较大的那部分


### 2.5.14 R方法

针对oracle数据库开发的性能分析方法， 意在找到 延时的根源， 基于oracle的 跟踪事件。

着重识别和量化 查询过程中所消耗的时间。


### 2.5.15 事件跟踪

系统的操作 就是处理 离散的事件，包括 CPU指令，磁盘IO，磁盘命令，网络包，系统调用，函数库调用，应用程序事务，数据库查询 等。

性能分析通常会研究 这些事件的 汇总数据，如 每秒的操作数，每秒的字节数，延时的均值。

网络排查可能有用的工具有 tcpdump
`tcpdump -ni eth4 -ttt`

块设备层，存储设备IO可以用 biosnoop来跟踪(基于 BCC/BPF)

系统调用层，视同 strace，perf 来跟踪

执行事件跟踪时，要找到下列信息
- 输入， 事件请求的所有属性，即类型，方向，size 等
- 时间， 起始时间，终止时间，延时
- 结果， 错误状态，事件结果

事件的时间戳有利于延时分析。

研究之前发生的事件 也能提供信息。  一个延时 比其他高很多的事件，通常叫做 延时离群值，它可能是由之前的事件造成的。 例如 队列尾部 事件的延迟可能很高。


### 2.5.16 基础线统计

监测，记录服务器性能指标，并可视化(x轴是时间)。 检查线的变化就可以看到 有什么不同。

有时添加额外的线条，如 历史平均数， 一周前的值

比线图更早的是 基础线统计数据，收集系统所有的指标，保存到文本 或数据库中。 软件可以是一个 shell 脚本，用它来运行观测工具并采集其他信息来源( /proc文件的 cat)。


### 2.5.17 静态性能调优

着重处理的是 架构配置的问题。

动态性能 是 着重处理 负载施加后的性能。

对系统的所有组件确认下列问题
1. 该组件是否合理? (过时，功率不足等)
2. 配置是针对预期负载设定的吗?
3. 组件的自动配置对于预期负载是 最优的吗?
4. 有组件出现错误吗? 是在降级状态码?

一些静态性能调优中可能发现的问题:
- 网络接口协商: 选择1Gb/s 而不是 10Gb/s
- 建立RAID池失败
- 使用的 OS，app，固件 是老版本
- 文件系统几乎满了
- 文件系统记录的尺寸和 工作负载IO的尺寸不一致
- app在运行时 意外开启了 高成本的 调试模式
- 服务器被意外 配置了网络路由 (开启了IP转发)
- 服务器使用的资源，诸如 认证，使用远端的数据中心，而不是本地的


### 2.5.18 ==缓存调优==

app 和 OS 会部署多层的 缓存来提高IO性能，详见3.2.11

这里介绍 各级缓存的 通用调优策略
1. 尽量将缓存放在 栈的顶端，靠近工作开展的地方，以降低缓存命中的操作开销
2. 确认缓存开启并确实在起效
3. 确认缓存的 命中/失效 比例 和 失效率
4. 如果缓存的大小是动态的，确认它的当前大小
5. 针对工作负载调整缓存，这依赖 缓存的可调参数
6. 针对缓存调整工作负载。 包括 减少对 缓存不必要的消耗， 这样可以释放更多的空间来给 目标工作负载使用

要小心二次缓存，比如，消耗主存的2个不同缓存块，或把 相同的数据缓存了2次

还要考虑 每层缓存调优 的 整体性能收益


### 2.5.19 微基准测试

简单的人造工作负载。

宏观基准测试(或 行业基准测试) 是 真实的工作负载

常用的微基准测试时 iperf， 执行 TCP吞吐量测试。

微基准测试的例子
- 系统调用时间: fork，execve，open，read，close
- 文件系统读取: 从缓存过的文件读取，读取的数据大小 从1b 变化到 1mb
- 网络吞吐量: 针对不同的socket缓冲区的尺寸 测试TCP 端到端 数据传输



### 2.5.20 性能箴言

如何提高性能的调优方法，从最有效到 最无效
1. 不要做
   消除不必要的工作
2. 做吧，但不要再做
   缓存
3. 做少点
   刷新，轮询，更新的 频率调低
4. 稍候再做
   回写缓存
5. 在不注意的时候做
   安排工作在空闲时进行
6. 同时做
   多线程
7. 做的更便宜
   更快的硬件




## 2.6 建模

建立 系统的分析模型， 有很多用途，特别是 对于 可扩展性分析: 研究当负载或资源扩展时性能会如何变化。 这里的资源 可以是硬件(CPU) 也可以是软件(进程，线程)

除了对 生产系统的观测(测量) 和实验性测试(仿真) 之外， 分析建模 被认为是 第三类性能评估方法。

对现有系统进行分析，可以从 ==测量==开始: 归纳工作负载特征 和 观测性能。
如果系统没有生产环境负载 或要测试的工作负载 在生产环境不可见，则可以用工作负载==仿真==做测试。
==分析建模== 可以基于 测量和仿真的 结果，对性能进行预测。

可扩展性分析可以揭示性能由于资源限制 停止 线性增长的点，即拐点


---

2.6.1 企业与云

建模 可以对 大型的企业系统进行 仿真，但是 大型环境的性能常常是复杂且难以精确建模的。

利用云技术，任意规模的环境都可以 短租。

---

2.6.2 可视化识别

通过实验收集到足够多的数据结果后，就可以 绘制 性能随规模变化的曲线。

曲线的 x轴是 扩展的维度， y轴是 相应的性能 (吞吐量，每秒事务数)

曲线的类型:
- 线性扩展，性能随着资源的扩展成比例地增加， 可能是其他曲线的 早期阶段
- 竞争， 架构的某些组件是共享的，而且只能串行执行， 对这些共享资源的竞争会减少扩展的效益
- 一致性， 要维护数据的一致性， 数据一致性的代价 会超过 扩展带来的好处 (。节点越多，一致性越慢)
- 拐点， 某个因素碰到了扩展的制约点，从而改变了扩展曲线
- 扩展上限， 达到某个硬件的极限。 该极限可能是 设备瓶颈，如 总线，互联器件 达到最大吞吐量， 或者是 一个软件设置的限制(系统资源控制)

![55fad643f06ba4dbb4cb8d4f85ab6554.png](../_resources/55fad643f06ba4dbb4cb8d4f85ab6554.png)


下面会介绍 Amdahl定律，通用扩展定律，排队理论

### 2.6.3 Amdahl定律

该定律对系统的扩展性进行了建模，考虑的是 ==串行构成==的 不能并行执行的工作负载。
该定律可以用于 CPU，线程，工作负载

Amdahl扩展定律 认为 早期的扩展特征是 竞争， 主要是对 串行资源，或工作负载的竞争。 可以描述为:
`SpeedUp(n) = 1 / ((1-a) + a/n)`
。书上是错的，这里上面是对的。 书上的少一个 括号， 即 N/(1+s(N-1)) . 1/(1/n + sn/n - s/n) . 1/((1-s)/n + s) . s=1-a . 1/((1-(1-a))/n + (1-a)) . 1/(a/n + 1-a)
。。a 代表 async。 有些公式用s，是sync。  a+s=1
。如果a 是1，表示 全部可以并行，那么上面就变成 n 了。 即可以n 倍加速。
。如果a 是0，代表 整个是串行的，那么上面是 1/1， 无论n是多少， 没有任何加速。
。还要注意 上面说到的 串行构成， 即 任务之间是 有先后的。 所以 才能 阿姆达尔定律， 任务可以并发的话， 没有上限的，是另外一个定律，忘记了。古斯塔夫森定律


### 2.6.4 通用扩展定律

Universal Scalability Law, USL。

用于描述 一致性扩展的曲线

`C(N) = N / (1 + s*((N-1) + b*N*(N-1)))`
。 原文: `C(N) = N / (1 + s*(N-1) + b*N*(N-1))`，网上看了几个公式， 正确的应该是上面的。
b是一致性系数，为0 时，就是 Amdahl


### 2.6.5 排队理论

用数学方法研究 带有队列的系统， 提供了 对 队列长度，等待时间，使用率 的 分析方法。

计算机中 很多 硬件，软件 都可以建模为 队列系统， 多条队列系统的模型被称为 队列网络。

排队理论是建立在: 概率分布，随机过程，Erlang的C公式，Little's 定律 上的。

Little's 定律: `L = aW`。 系统请求的平均数目L 是由 平均到达速率 a 乘以 平均服务时间W 得到的。
这个公式可以应用于 队列，比如 L 是 队列中的 请求个数，W是 该队列的 平均等待时间。

。。
。https://zhuanlan.zhihu.com/p/65687548
。请求以 a个/s 的速度到达， 每个请求需要 10s 处理，那么 系统中的 请求总数是 10 * a 个。
。请求 a个/s 到达， 队列等待时间是 10s， 那么 队列中一共有 10a 个请求。

利用排队系统，可以回答下面的问题:
- 如果负载增加一倍，平均响应时间会怎么变化?
- 增加一个处理器会对平均响应时间有什么影响?
- 当负载增加一倍时，系统的90% 响应时间 能在 100ms 以下吗?

。。? 怎么回答。。

除了响应时间，排队理论还研究: 使用率，队列长度，系统内的任务数目

一个简单的排队系统模型:
![e4559a26e9b0fd466158674cc009f6c5.png](../_resources/e4559a26e9b0fd466158674cc009f6c5.png)

图中一个单点的服务中心在处理队列里的任务。排队系统可以由多个服务中心 并行地工作。
在排队理论中，服务中心通常被称为 服务器。

排队系统能用以下3个要素进行归纳
- 到达过程: 描述的是 请求到达排队系统的 时间间隔，这个间隔可能是 随机的，固定的，泊松分布的
- 服务时间分布: 描述的是 服务中心的服务时间，可以是 确定性分布，指数型分布 等
- 服务中心数目: 一个或多个

这些要素可以用 Kendall 标记法: `A/S/m`， 到达过程A，服务时间分布S，服务中心数目m。 还有扩展格式 来囊括更多的要素: 系统中的缓冲数目，任务数目上限，服务规则

通常研究的排队系统如下:
- `M/M/1`，马尔可夫到达(指数分布到达)，马尔可夫服务时间(指数分布)，1个服务中心
- `M/M/c`，同上，但 多个服务中心
- `M/G/1`，马尔可夫到达，服务时间是 一般分布，1个服务中心
- `M/D/1`，马尔可夫到达，确定性的服务时间(固定时间)，1个服务中心

M/G/1 通常用于研究 旋转式硬盘性能

---

M/D/1 和 60%使用率

排队理论的一个简单例子: 假设 磁盘响应工作负载的时间是固定的(这是一种简化)。响应的模型是M/D/1。
现在的问题是: 随着使用率的增加，磁盘的响应时间如何变化?

根据排队理论，M/D/1的响应时间计算如下:
`r = s(2-p)/2(1-p)`    。。不知道有没有少 () 。少了!
响应时间r， 服务时间s，使用率p。


对于1ms 的服务时间，使用率0-100%，响应时间 和使用率的关系:

![a402465c1f72fd6508451af6d9a7c037.png](../_resources/a402465c1f72fd6508451af6d9a7c037.png)

。。L = rW。 W是固定的:s
。。L = rs。 r 就是马尔可夫到达， 但是 r 是多少? 看上面的式子，反推是 (2-p)/2(1-p)
。所以上面是 固定的服务时间s， 马尔可夫到达 (2-p)/2(1-p)， 相乘就是 每个请求的服务时间?
。。? 但是还有 等待队列呢。 而且 使用率p 和 马尔可夫到达有什么关系?

。。看了图， 应该是 `r = s*(2-p)/(2*(1-p))`
..当p接近100%，即1时， 2*(1-p) 是0， 所以响应时间 是 无穷大。
。p是0的时候，r=s。
。p是使用率， 1-p 是获得服务的几率? 。不使用的时候，你才能被服务。
。。上下都除以2， 分子就是 1-0.5p。  0.5感觉是 等待的队列的时间。
。 p是使用率，所以 p也是无法获得服务的几率，要进入等待队列。
。。等待队列的耗时 = 
。。。不知道了
。0.8->3， 1.2/(2*0.2)  还真是3
。`1/(1-p) - 0.5p/(1-p)`
s*(2-p)/(2*(1-p)) - s   响应时间 - 服务时间 ，计算 等待时间
2s-sp-2s+2sp   sp/(2-2p) 这个是等待的时间， 0.5sp/(1-p) . 不会正推 等待时间。
..
..不知道公式怎么来的。


---

2.7 容量规划

容量规划: 资源极限，因素分析。

扩展的解决方案: 负载均衡器，分片。

---

资源极限方法可以寻找 负载之下成为 瓶颈的资源，步骤:
1. 测量服务器请求的频率，并监视请求频率随时间的变化
2. 测量硬件和软件资源的使用，监视使用率随时间的变化
3. 用资源的使用来表示服务器的请求情况
4. 根据每个资源来推断服务器请求的极限(或用实验确定)

要先 识别 服务器种类， 服务的请求的类型。 如 web服务器的 http请求， nfs服务器的 nfs协议请求， 数据库服务器的 查询请求。

然后 判断 请求会消耗哪些系统资源。 查看系统资源的使用率 和 请求率 的关系， 推断哪个资源先到 100%，并查看 那时 的请求率。

要监视的资源:
- 硬件: cpu使用率，内存使用，磁盘IOPS，磁盘吞吐量，磁盘容量，网络吞吐量
- 软件: 虚拟内存使用情况，进程/线程，文件描述符

要多测几个点，来判断是 资源使用率和请求 的关系是 线性 还是 非线性。 并且算出 线性的方程。

---

确定什么样的 资源组合 是 最佳性价比 的方法
1. 测试 所有资源因素最大时的性能
2. 逐一改变因素，测试性能 ( 预期 每个因素的改变 都会引起 性能下降 。。。就是 向下改变，直到性能 降低?)
3. 基于测量的结果，对每个因素的变化 引起性能下降的 百分比 以及 所节省的成本 做统计
4. 将最高的性能(和成本) 作为起始点，选择能节省成本的资源因素，同时 保证 组合后的性能依然能满足需要
5. 重新测试 改变后的配置，确认所交付的性能


---

要更高的性能，常常 意味着 更大的系统， 这种策略 是 垂直扩展。

把负载分给 许许多多的系统，在这些系统前面 放置 负载均衡器，这种 是 水平扩展。

云计算 把水平扩展 更推进了一步。

有些技术 可以根据性能指标 自动进行 云扩展。 AWS在这方面的技术称为 自动扩展组(ASG)。 Netflix通常使用 ASG，目标是 CPU使用率为60%

容器编排系统 也支持 自动扩展。如，k8s 提供了 水平 pod自动伸缩器 (HPA)， 可以根据 CPU使用率 或 其他自定义指标 来调整 pod 的数量。

对于数据库来说，一个常见的扩展策略 叫做 ==分片sharding==， 把数据切分成一个个逻辑组件，每个组件由自己的数据库(或冗余的数据库) 来管理。


---

2.8 统计

平均值，标准方差，百分位数

2.8.1 量化性能收益

要比较问题 和 对问题排优先级，需要对 问题和问题修复后所带来的性能的潜在提升 做量化。 这种一般通过 观测或 实验来做。

---

基于观测
1. 选择可靠的指标
2. 估计解决问题带来的性能收益

比如
- 观测到: app请求需要10ms处理
- 观测到: 其中9ms是 磁盘IO
- 建议: app将IO缓存到内存中，预期DRAM的延时将是10us
- 估计性能收益: 10ms -> 1.01ms，大约9倍收益

延时 是一个 很适合做量化的指标。

---

基于实验
1. 实施修复
2. 用可靠的指标量化做前后对比

比如
- 观测到: app事务平均延时10ms
- 实验: 增加app线程数，来允许更多的并发，减少排队
- 观测到: app事务平均延时2ms
- 性能增益: 10ms -> 2ms， 5倍


---

2.8.2 平均值

用单个数据代表一组数据。

最常见的是 算术平均值。  还有 几何平均值，调和平均值

几何平均值: n个数值的 乘积的 n次方根。 比如: 内核网络栈 每层都 提升一定的性能，那么平均的性能提升了多少? 由于 网络的各层 是共同作用于 同一个包的，所以 提升有 相乘的效果， 所以使用 几何平均值。    。。? 直接乘就可以了吧? 为什么还要 n 次方根? 而且 还要看 占比啊， 某层的 耗时占了 99%，那么这层 就是 完全主导的。

调和平均值: 个数 除以 所有数值的倒数之和。 更适用于 利用速率求均值。 如: 计算传输800mb数据的平均速度，第一个100mb以50mb/s传输，其余700mb是10mb/s， 使用调和平均值: 800/(100/50 + 700/10) = 11.1mb/s  ....就是，一部分按某个速度，另一部分按另一个速度， 求均速， 就是 总量/(时间+时间+..+时间).

---

随着时间变化的平均值

CPU 1分钟100%， 4分钟0%， 那么 看5分钟平均值，只能得出 20%的结论， 不符合实际情况。

---

平均值 隐藏了一些细节， 为了更好地理解数据，需要使用 标准方差，百分位数，中位数


2.8.3 标准方差，百分位数，中位数

标准方差 是 数据的 离散程度
tp99
tp50 就是 中位数

。标准方差: sqrt( ((每项与平均值的差的平方)的和) / 个数 )。  没sqrt 就是 方差。


2.8.4 变异系数

。。学名: 离差系数 或 变差系数

标准方差 是 对于平均值而言的， 所以 单独的标准方差 没有太大意义。 加上平均值才有意义。

变异系数 (CoV 或 CV): 标准方差 相对于 平均值的比例。

50的标准方差，200平均值的 CV 是 25%。

CV越小，数据的差异越小。

另一种表达变异的 单一指标是 z值，即 一个数值与平均值 相差多少个 标准方差。
。。统计学  z-score， z分数



## 2.8.5 多重模态分布

平均值，标准方差，百分位数，都是 针对 正态分布 或 单模式分布 而言的。

系统性能通常是 双模态的， 对于快速的代码路径 是 低延迟的， 对于 缓慢的代码路径是 高延迟的， 或者 缓存命中时 低延迟，缓存未命中时 高延迟。

![b0c66f36148ddc1d8817b39b39650824.png](../_resources/b0c66f36148ddc1d8817b39b39650824.png)
左侧 是 磁盘缓存命中的情况， 右侧是 磁盘缓存失效的情况(随机读)。


2.8.6 异常值

统计的另一个问题是 异常值， 有非常少量的 极高值 或 极低值。

异常值 很可能让 平均值 偏移， 但是 对 中位数 没有影响。

用标准方差，tp99 能更好地识别 异常值。


2.9 监测

系统性能监测记录 一段时间内 的性能统计数据， 可以对 过去的记录 和现在的 做比较，这样能找出基于 时间的 使用规律。 这对 容量规划，量化增长，显示峰值 都很有用。 还可以为 理解性能指标的当前值(。是好，是坏) 提供 上下文背景


2.9.1 基于时间的规律

![98f87696331d6e82ad359c742c0a41af.png](../_resources/98f87696331d6e82ad359c742c0a41af.png)


这些曲线，反映的日常规律是: 每早8点 缓慢上升，下午稍微下降，晚上逐渐下降。
长时间跨度的曲线显示: 周末读操作 较少。 在30天的曲线上 可以看到一些 尖峰。

负载的非规律性增长 可能源于其他原因，如，在网上发布的新内容，以及销售(黑色星期五)。
非规律性下降，可能是外部活动，如 大范围的停电 ，停网， 体育决赛( 都去看比赛了，没人使用你的产品)


2.9.2 监测产品

有很多 第三方性能监测产品。 典型的功能包括 数据存档，数据图表，通过网页交互显示， 可配置的 警报系统。

一部分这样的操作 是通过在 系统上 运行 代理软件 收集统计数据实现的。 这些代理软件运行 OS的监测工具( 如 iostat, sar) 并解析 输出文本(。这样做被认为是低效的，甚至会引起性能问题)， 或直接 链接到 OS库 和接口 来读取。

随着系统越来越分布式，你越发需要监测大量的系统。 集中式 的检测产品就变得尤其有用，它可以 一个界面 监测整个环境


2.9.3 自启动以来的信息统计

如果没有执行监测， 至少还可以检查 系统自带的 自启动以来的 统计信息。 用这些值 来和 当前值对比。


## 2.10 可视化

2.10.1 线图

通常用于 检查一段时间的性能指标， X轴是 时间， Y轴是 性能值

可以画多条线，在同一个 X轴上 显示相关数据， 每块磁盘画一条线，来查看 彼此的性能是否相似。

也可以将 统计值 画上去，比如 中位数，标准方差，tp99。

![6ebd14166ef77513cb4a0ac17cb02391.png](../_resources/6ebd14166ef77513cb4a0ac17cb02391.png)


2.10.2 散点图

每次磁盘IO都按照 时间X，延迟Y 打标记。

![0e0020ba3ca3999bc99b7c1a47db4921.png](../_resources/0e0020ba3ca3999bc99b7c1a47db4921.png)

散点图显示了所有的数据，揭示了这些异常值的存在。

问题: 点和点重叠在一起，难以分辨，数据越多越糟糕。  需要收集每个点的信息。


### 2.10.3 热图(列量化)

将 x轴，y轴 范围 量化为 称为桶 的组来解决 散点图的伸缩性问题。

这些 桶 通过 x，y 范围内的 事件数量 进行着色。

![0cfdc3a1566ab3812c15a9d6bfd2c298.png](../_resources/0cfdc3a1566ab3812c15a9d6bfd2c298.png)

。。应该用 冷暖颜色 来 两极化。用 深和浅的话，也要注意范围，因为  浅色的离散点 不容易被注意到


2.10.4 时间线图(瀑布图)

时间线上的 条形显示一组活动。
通常用于 前端性能分析(网络浏览器)

![4558dce2a06214cb4646914726fa0a85.png](../_resources/4558dce2a06214cb4646914726fa0a85.png)

对于后端性能分析(服务器)，类似的图被用来显示 线程 或 CPU 的时间线。 比如 KernelShark，Trace Comprass。


2.10.5 表面图

三维的表示，呈现一个三维的平面。

。。图看不清，根据描述也想不出。。


2.10.6 可视化工具

Unix 主要是 基于文本的工具

可视化工具: Grafana


# ch03 操作系统


3.1 术语

![6d218f267d7080f834734949642e1322.png](../_resources/6d218f267d7080f834734949642e1322.png)
硬件中断: 由物理设备发给内核的信号，通常是请求IO服务。中断是自陷的一种类型。


3.2 背景

3.2.1 内核

内核是操作系统的核心软件。它做什么取决于内核模型，包括Linux和BSD在内的类UNIX操作系统拥有一个 单内核 ，管理着CPU调度、内存、文件系统、网络协议和系统设备（磁盘、网络接口等）

还存在其他内核模式: 微内核(小的内核，其功能被转移到用户态的程序中)，宏内核(内核和app的代码作为一个单一的程序编译在一起)，混合内核( Windows NT的内核，同时使用了 单内核和微内核的方法)。

。。? baidu了，linux 是 宏内核 。。。 ‌单内核（Monolithic kernel）‌，也称为宏内核
。主要区别是，宏内核中有 设备驱动，文件系统，网络协议等， 微内核没有。   一些基本的，如 内存管理，进程调度，消息传递， 2种内核都有。
。。所以 宏内核 不是 和 app编译在一起的，  是和 驱动 等 编译到一起。 确实，听到过 linux 的 硬件驱动的 消息。
。宏内核的优势: 基础设施都在 内核态中，所以不需要切换到 用户态
。劣势: 都在内核中，所以 某个崩了，可能导致 内核崩溃。 而且不安全。

Linux最近改变了它的模式，允许一种新的软件类型: eBPF，这让 安全的内核态的应用程序 与它自己的内核API一起使用成为可能。
通过 BPF帮助器，可以用BPF重写一些app和系统功能，以提供更高水平的安全性和性能。

3.4.4节讲 eBPF

---

内核的执行

内核是一个大程序，通常有几百万行的代码。当用户级程序进行系统调用，或设备发出中断时，内核首先要按需执行。有一些异步运行的内核线程进行内务处理，其中可能包括内核时钟例程和内存管理任务，但这些线程会尽量做到轻量级，消耗的CPU资源非常少。

频繁IO的工作负载，如网络服务器，主要在 内核上下文中运行。
计算密集型的工作负载，通常在 用户态运行。 内核会影响 计算密集型工作负载的 性能，比如 CPU调度，内核调度器需要决定哪些运行，哪些等待， 内核 还会选择 哪个CPU来运行线程，并且可以选择 具有更好 预热硬件缓存的 CPU，或 为 进程提供更好的 内存局部性。


## 3.2.2 内核态和用户态


内核 是运行在 特殊CPU模式下的 程序， 这一特殊的CPU模式叫做 内核态，在这个状态下，设备的一切访问及特权指令的执行都是被允许的。
由内核来控制设备的访问，用以支持多任务处理，除非明确允许，否则 进程之间，用户之间 的数据 无法彼此访问。

用户程序 运行在 用户态下， 对于 内核特权操作(如IO) 的请求是 通过 系统调用 传递的

用户态，内核态，是 在处理器上 使用 特权环(或保护环) 实现的。
x86 支持4个特权环，编号为0到3，通常只使用2或3个: 用户态，内核态，管理程序(如果存在)
访问设备的特权指令只允许在内核态下执行; 在用户态下执行 这些指令 会触发异常，然后由 内核处理。

传统内核中，系统调换会做 上下文切换，从用户态到内核态，然后执行 系统调用的代码。

用户态和内核态之间的切换是 模式转换。

所有的 syscall 都会进行 模式转换。 对于某些 syscall 还会进行 上下文切换: 那些阻塞的系统调用，如，磁盘，网络IO， 会进行上下文切换，以便在第一个线程 被阻塞时， 运行另一个线程。

### ==避免切换到内核态的开销==

p90，最下面

模式转换，和上下文切换 都会增加一小部分的时间开销。
有多种方式来避免开销:
- 用户态的syscall: 单独在用户态库中 实现一些系统调用。 linux内核通过 导出一个 映射到 进程地址空间里的 虚拟动态共享对象(vDSO) 来实现，该对象包含 如 gettimeofday,getcpu 的系统调用。
- 内存映射: 用于按需换页(7.2.3)，内核映射也可以用于数据存储和其他IO，可以避免系统调用的开销。
- 内核旁路(kernel bypass): 这类技术允许用户态的程序直接访问设备，绕过系统调用 和 典型的内核代码路径。例如，==DPDK==
- 内核态的app: 这些包括在内核中实现的 TUX网络服务器，以及(一些)eBPF技术



## 3.2.3 系统调用

p146/91

系统调用 请求 内核 执行 特权的系统例程。
可用的 syscall有 几百个，但需要努力确保这一数目 尽可能小，以保持内核简单。

更复杂的接口 应该作为 系统库 构建在用户空间中。

OS通常包含 C语言的标准库。

下面是关键的syscall

|syscall|desc|
|--|--|
|read(2)|读取字节|
|write(2)|写入字节|
|open(2)|打开文件|
|close(2)|关闭文件|
|fork(2)|创建新进程|
|clone(2)|克隆新进程 或线程|
|exec(2)|执行新程序|
|connect(2)|连接到网络主机|
|accept(2)|接受网络请求|
|stat(2)|获取文件统计信息|
|ioctl(2)|设置IO属性，或做其他事情|
|mmap(2)|把文件映射到内存地址空间|
|brk(2)|扩展堆指针|
|futex(2)|快速用户空间互斥锁|

man 手册

这些 syscall的目的都和明显。

下面是一些常见 但不太明显的用法
- ioctl， 通常用于向内核请求各种操作，特别适用于系统管理工具，在这类用途中其他的（更明显的）系统调用是不适合的。具体请看下面的例子。
- mmap， 通常用于 将 可执行文件 和 库文件 以及 内存映射文件 映射到 进程的地址空间。有时 会代替 基于 brk 和 malloc 对 进程的工作内存做分配，以减少syscall的频率，提升性能
- brk， 用于延伸堆的指针，该指针定义了进程工作内存的大小。 通常由 系统内存分配库 执行，当调用 malloc 不能满足堆内现有空间时 发生。
- futex， 用来处理 用户空间 锁的部分，可能阻塞的那个部分。

ioctl 学习起来 最困难， 因为它本身的用法太过多样， 如 perf工具， 执行特权指令 来协调 性能监测点。并非对每个行为都添加一个syscall，而只是添加一个syscall: perf_event_open()， 它会用 ioctl 返回一个 fd。 用不同的参数 调用 ioctl 会执行不同的行为。 例如 `ioctl(fd, PERF_EVENT_IOC_ENABLE)` 能开启监测点。 在这种情况下，开发人员可以很容易地对 参数 PERF_EVENT_IOC_ENABLE 做添加和修改。


## 3.2.4 中断

中断是 向处理器发出的信号， 即发生了一些需要处理的事件，要 中断 处理器 当前的执行 来实施处理。
如果处理器没有进入 内核模式的话，中断通常会使 处理器进入 内核模式，并保存当前线程状态，然后运行一个 中断服务例程(ISR) 来处理该事件。

有 由外部硬件产生的 异步中断 和 由软件指令产生的 同步中断。

![8b9d88c785f253b6054a1eb8877064ab.png](../_resources/8b9d88c785f253b6054a1eb8877064ab.png)


异步中断

硬件设备可以向 处理器发送 中断服务请求(IRQ)，这些请求以 异步方式 达到当前运行的软件。

硬件中断的例子有
- 磁盘设备 发出 磁盘IO完成的 信号
- 硬件显示有故障情况
- 网络接口 发出 数据包到达的信号
- 输入设备: 键盘和鼠标的输入


下面的图解释了 异步中断。
CPU0 上运行的 mysql 需要读取 磁盘数据，所以 调度器上下文 切换为 另一个线程(一个java应用)， 而mysql正在等待。 一段时间后，磁盘IO完成， 但 此时 mysql 已经不在 CPU0上了。 完成中断 已经异步地发生在 mysql 上， 图中用 虚线(。最右边的竖线)表示。

。。不过还是不太理解。 IO完成 这个信号，会在 CPU2 开始执行 mysql 时， 被CPU2 收到?
。mysql 从CPU0 被换出后， 哪个CPU 把 这个 完成信号 放到 哪里， 从而导致 CPU2 开始执行时 收到 IO完成?  。 不，还是说， IO完成后， 调度器 才可能把 mysql 发给某个 CPU， 应该是这样的。
。 对， IO完成， mysql 才能继续执行。 
。所以应该是: mysql 需要 磁盘IO， 调度器将 mysql 移出 CPU，(移出 可执行队列)， CPU收到 IO完成，将 mysql 加入 可执行队列 ( 等待调度器 调度)  ?

![3c19c8c3760f6cf68797d578ff1abcb9.png](../_resources/3c19c8c3760f6cf68797d578ff1abcb9.png)


---

同步中断

同步中断 由 软件指令产生。
下面用 自陷，异常，故障 等术语来描述不同类型的 软件中断，这些术语经常可以互换使用。

- 自陷， 故意调用内核，例如 通过 int指令。 系统调用的一种实现方式是 用 系统调用处理程序 调用 带向量的 int指令 ( 例如， linux 的 int 0x80 )。 int指令可以引发 软件中断
- 异常， 一个特殊的条件，例如 由指令执行除以0
- 故障， 一个通常用于 内存事件的术语， 例如 在没有MMU 映射的情况下 访问一个内存位置 所引发的 缺页故障。

对于这些中断， 相对应的软件 和指令 仍在 CPU上。

---

中断线程

中断服务例程ISR 被设计为 尽可能快地运行， 以减少 中断活动线程的影响。

如果一个 中断需要 执行更多的工作，尤其是 还可能被 锁 阻塞，那么最好用 中断 线程 来处理， 这个线程可以由内核来安排。

对于 linux 而言，设备驱动分为两半,上半部用于快速处理中断，针对下半部的调度工作在之后处理。
上半部分快速处理中断 是很重要的， 因为 上半部分 运行在 中断禁止模式(interrupt-disabled mode)， 会推迟 新中断的产生， 如果运行的时间太长，会造成 延时问题。 下半部分可以作为 tasklet 或 工作队列，之后由 内核做线程调度，如果需要也可以休眠。

例如， linux 网络驱动的 上半部分 处理 入栈数据包的 IRQ， 它调用 下半部分 将数据包 推上网络栈。 下半部分 被实现为 软中断(softirq)

---

中断屏蔽

内核中的某些代码路径 是不能被 安全中断的。 比如，内核代码 在 系统调用过程中 获得了一个 自旋锁，这个自旋锁 也可能被中断所需要。 在持有这个锁的情况下 进行中断 可能导致 死锁。
为了防止这种情况， 内核可以通过 设置 CPU 的 中断屏蔽寄存器 来==暂时屏蔽中断==。 屏蔽中断的 时间要尽可能短，因为它可能 干扰 被其他中断 唤醒的 应用程序的 及时运行。

中断禁用时间 也是一个 性能分析的目标。

一些 高优先级的事件 不应该被忽略， 因此 被实现为 ==不可屏蔽的中断==(NMI)
例如，linux 可以使用 智能平台管理接口(IPMI) 看门狗 定时器 来检查 内核 在一段时间内 是否在没有中断的情况下 被锁定。如果有，看门狗可以发出 NMI 中断 来重启系统。



3.2.5 时钟和空闲

早期UNIX 内核的一个核心组件是 clock()例程，由一个计时器中断执行。

现代内核 已经把 许多功能移出了 时钟例程，放到了 按需中断中。

linux的时钟例程是 scheduler_tick()， 在没有任何 CPU负载的情况下，linux 有办法 不调用时钟。

---

空闲线程

CPU没有工作时， 内核会安排一个等待工作的占位线程，称为 空闲线程。
在现代的linux中， 空闲任务可以调用 hlt 指令来关闭 CPU的电源，直到收到 下一个中断，从而节约电力。



3.2.6 进程

进程是用以执行用户级别程序的环境。
包括: 内存地址空间，fd，线程栈，寄存器。

pid，process id，标示 进程

一个进程 包含 >=1 个线程。
线程 在 进程的 地址空间内操作 并 共享 所有打开的fd。
线程是 一个 可执行的上下文，包括 栈，寄存器，==指令指针(也称为 程序计数器)==


---

进程的创建

UNIX中，通常使用 fork(2) 创建进程
Linux中， C语言库 通过包裹多功能的 clone(2) 来实现 fork 功能。

这些 syscall 创建 进程的 副本，该副本有自己的 pid， 然后调用 exec(2) ( 或 execve(2)) 来 开始执行一个不同的程序。

fork 和 clone 可以通过 写时拷贝 (copy-on-write, COW) 来提高性能。
。默认使用引用指向内存，直到 修改时，才复制 被修改的内容。


---

进程生命周期

![3a82a8846abd4e0e2b202b511afba743.png](../_resources/3a82a8846abd4e0e2b202b511afba743.png)


进程环境

![c575cfa0a062d0dc1bdbed4be804731d.png](../_resources/c575cfa0a062d0dc1bdbed4be804731d.png)



3.2.7 栈

是一个用于 存储 临时数据的 内存区域。

LIFO

函数被调用时，返回地址被保存到栈中。 如果调用后 需要一些寄存器的值，寄存器也可以被保存在栈中。当被调用的函数执行完成后，它将恢复所有需要的寄存器，并通过 从栈中获得的返回地址，将执行转移到调用函数。
栈也可以用于 向函数传递参数。 栈中 和函数的执行有关的数据集 被称为 栈帧。

---

用户栈 和 内核栈

执行syscall 时， 一个进程的线程 有2个栈，用户级别的栈 和 内核级别的栈。


![2d1c857cf34062bd51e4b47319ef42b7.png](../_resources/2d1c857cf34062bd51e4b47319ef42b7.png)


线程被阻塞时， 用户级别的栈 在 syscall 期间不会改变，因为当执行在内核上下文时，线程用的是一个单独的内核级别的栈。

linux中，有用于不同用途的多个内核的栈。


3.2.8 虚拟内存

是主存的抽象， 为进程和内核 提供 近乎无限的，私有的 主存视图

虚拟内存支持 多任务处理，允许 进程和 内核 在它们自己的私有地址空间执行 而不用担心任何竞争。
还支持 主存的超额使用，OS可以将 虚拟内存 在主存 和二级存储(磁盘) 之间进行映射

下图的 一级存储是RAM，二级存储是磁盘


![d66a8cc461181d4018bdb08b9e55e092.png](../_resources/d66a8cc461181d4018bdb08b9e55e092.png)


---

内存管理

当虚拟内存 用 二级存储作为 主存的扩展时，内核会尽力 保持最活跃的数据在主存中。 下面的2个内核例程做这件事
- 进程交换: 让整个进程在主存和二级存储之间移动
- 换页: 移动被称为 页 的内存单元

进程交换是原始的UNIX方法(linux不支持)，会引起严重的性能损耗。 换页是更高效的方法。


3.2.9 调度器

类UINX 的 OS 是 分时系统，通过划分执行时间，让多个进程同时运行。

调度器的基本作用是将 CPU时间 划分给 活跃的进程和线程， 而且 维护一套优先级机制，让 更重要的工作可以更快执行。

调度器 跟踪所有 处于 `ready-to-run` 状态的线程。

工作负载分为 CPU密集型，IO密集型

普遍使用的 调度策略可以追溯到 UNIX， 调度器可以识别 CPU密集型 的进程 并==降低==它们的优先级，让 IO密集型(需要低延时的响应) 更快运行。

现代内核支持 多类别调度 (linux中称为 调度策略)， 对优先级和可运行线程的管理 实施不同的算法。


3.2.10 文件系统

是文件和目录的数据组织。

有一个基于文件的接口用于访问 文件系统，该接口通常是基于 POSIX 标准的。

内核支持多种文件系统。

OS提供了全局的文件命名空间，其被组织为一个 以根目录("/") 为起点，自上而下的 拓扑结构。
通过 挂载 可以添加 文件系统的树。

顶层的目录包括:
- etc，放系统配置文件
- usr，放系统提供的用户级别的程序和库文件
- dev，设备文件
- var，包括系统日志在内的各种文件
- tmp，临时文件
- home，用户的主目录

多数文件系统类型 使用 存储设备(磁盘) 来存放内存。 有些文件系统类型是由内核动态生成的，如 /proc, /dev

内核提供多种方式( chroot) 将 进程隔离到文件命名空间的 一个部分中， linux有用于容器的 mount命名空间


---

VFS
virtual file system

对文件系统类型进行抽象的内核接口。 使得 内核添加新的文件系统时更加简单。

---

IO栈

基于存储设备的文件系统，从用户级软件 到 存储设备的路径被称为 IO栈。



3.2.11 缓存

常见的缓存

|缓存|实例|
|--|--|
|客户端缓存|网络浏览器缓存|
|app缓存|-|
|web服务器缓存|apache缓存|
|缓存服务器|memcached|
|数据库缓存|mysql缓冲区|
|目录缓存|DCache|
|文件元数据缓存|inode缓存|
|OS缓存区|-|
|文件系统主缓存|换页缓存，ZFS ARC|
|文件系统次缓存|ZFS L2ARC|
|设备缓存|ZFS vdev|
|块缓存|缓冲区高速缓存|
|磁盘控制器缓存|RAID卡缓存|
|存储阵列缓存|-|
|磁盘内置缓存|-|


3.2.12 网络

现代内核 内置网络协议栈。

网络协议不常变，有一个新的传输协议被越来越多的人采用: ==QUIC==。


3.2.13 设配驱动

设备驱动用于 设备管理 ，设配IO。

通常由 硬件厂商提供

- 字符设配，也称为 原始设配， 提供 无缓冲的设配顺序访问，访问可以是 任意IO尺寸， 这类设配包括 键盘和串口
- 块设配， IO以块为单位， 带缓存。 linux中 缓冲区缓存 是 页面缓存的一部分


3.2.14 多处理器

通常用 对称多处理器架构 (symmetric multiprocessing, SMP)， 对所有CPU 平等对待。 很难实现，因为 并行运行的线程间访问与共享内存和CPU会遇到不少问题。

非统一内存访问(NUMA) 允许主存 连接到不同的 CPU核心。


IPI
多处理器系统 会出现 CPU需要协调的情况，如 内存翻译条目的缓存一致性。 CPU可以通过处理器间中断(IPI) 去 请求其他 CPU 执行这类工作。


3.2.15 抢占

支持内核抢占 让高优先级的用户线程 可以中断内核并开始被执行。 这使得 实施系统成为可能。
支持抢占的 内核 被称为 完全可抢占的内核，虽然实际上还是有 少量的关键代码路径 是不能被中断的。

Linux支持的是 自愿内核抢占， 在内核代码中的 逻辑停止点 可以做检查 并执行抢占。
Linux通过 CONFIG_PREEMPT_VOLUNTARY 选项来配置 自愿内存抢占(CONFIG_PREEMPT)，禁用抢占(CONFIG_PREEMPT_NONE，较高的延时，吞吐量更高)。


3.2.16 资源管理

OS提供各种配置，来调整系统资源，如 CPU，内存，磁盘，网络。

linux 使用控制组 (control groups, cgroups) 来进行资源控制

3.2.17 可观测性

OS由 内核，库，程序组成， 程序(包括 观测系统活动 和性能分析的工具) 通常 安装在 /usr/bin, /usr/sbin 下。


## 3.3 内核

下面讨论 内核的 实现细节， 重点是 性能

先一些背景， 3.4 是 linux 内核

UNIX早期只有 20个syscall， linux现在有几百个  (。。6.9 有644个)


3.3.1 UNIX

进程调度优先级
磁盘IO以块为单位
每个设备有 置于内存中的 缓存区高速缓存
空闲的进程会被交换到存储器中
支持网络，多文件系统，换页

3.3.2 BSD

分页虚拟内存， 不是交换整个进程，而是 移动分页。
按需换页， 物理内存 到 虚拟内存的 映射 推迟到 第一次写入时
FFS， 伯克利快速文件系统， 将磁盘分配 归入 柱面组，大大减少碎片
TCP/IP网络栈， 第一个高性能的 TCP/IP 网络栈
套接字， socket
Jail， 轻量级的OS级别的虚拟化，允许 多个用户共享一个 内核。
内核TLS， 将大部分TLS处理转入 内核，提高性能


3.3.3 Solaris

VFS， 虚拟文件系统，是一个 抽象和接口，让多种 文件系统 可以共存
完全抢占内核
多处理器支持
slab分配器， 代替了 buddy分配器， slab可以让 每个CPU缓存 预分配的 缓存区能更快地被重用
DTrace， 一套 静态和动态的 跟踪框架和工具， 可以对 整个软件栈 进行 无限的观测。
Zone， 基于OS的虚拟化技术，用于创建 共享一个内核的 OS实例，类似于 FreeBSD 的 Jail。 OS虚拟化现在被 linux 容器 广泛使用。
ZFS， 企业级功能和性能的 文件系统。


## ==3.4 Linux==

开发的思路 来源于
- UNIX: OS层级，syscall，多任务处理，进程，进程属性，虚拟内存，全局文件系统，文件系统权限，设备文件，缓存区高速缓存
- BSD: 换页虚拟内存，按需换页，快速文件系统，TCP/IP网络栈，套接字
- Solaris: VFS，NFS，页缓存，统一页缓存，slab分配器
- Plan9: 资源forks (rforks), 为进程间和线程(任务)间的共享设置不同的级别。


3.4.1 linux 内核开发

和性能方面的:
- CPU调度类型: 
- IO调度类型: 
- TCP拥塞算法: 
- Overcommit: OOM killer
- Futex: fast user-space mutex 的缩写，提供高性能的 用户级别的 同步原语
- 巨型页: 由 内核 和 内存管理单元(MMU) 支持 大型内存的 预分配， 第七章
- OProfile， 研究CPU使用 和其他活动的 系统剖析工具，对内核 和 应用程序 都适用
- RCU: 内核提供的 只读更新同步机制， 支持 伴随更新 实现多个 读取的并发，提升了 读取频繁的数据的 性能和 扩展性
- epoll: 对多个打开的 fd，可以高效地 针对IO等待 进行 syscall
- 模块IO调度: 对 块设配IO 提供 可插拔的 调度算法
- DebugFS: 简单的非结构化结构，内核用该接口 将数据暴露给 用户，通常被 性能工具使用
- Cpusets: 进程独占的 CPU分组
- 自愿内核抢占
- inotify: 文件系统事件的监测框架
- blktrace: 跟踪块IO时间的框架和工具 (后 迁移到 tracepoints中)
- splice: 一个syscall， 将数据在 fd 和管道之间快速移动，不经过用户空间 ( sendfile 封装了 splice)
- 延时审计: 跟踪每个任务的 延时状态
- IO审计: 测量每个进程的 各种存储IO统计
- DynTicks: 动态tick，当不需要时(tickless)，内核定时中断不会触发，节省CPU和电力
- SLUB， slab的简化版
- CFS: 完全公平调度
- cgroups: 控制组，测量并限制 进程组的资源使用
- TCP LRO: TCP 大型接收卸载 (large receive offload)，允许 网络驱动和 硬件 在将 数据包发送到 网络栈 之前 聚合成较大的体积。 linux也支持 大型发送卸载(LSO)
- latencytop， 观测OS的延时来源的 工具
- tracepoints， 静态内核跟踪点 (也称 静态探针)，组织内核里的 逻辑执行点，用于跟踪工具
- perf， 一套性能观测工具。
- 没有BKL， 消除了 大内核锁(BKL) 的性能瓶颈
- 透明巨型页， 简化 巨型内存页面的使用
- KVM， 基于内核的虚拟机(kernel based virtual machine)，创建虚拟的OS实例，并运行虚拟机自己的内核
- BPF JIT， Berkeley packet filter 的 即时编译器(JIT)， 将BPF字节码编译为 本地指令 来提高 包过滤性能
- CFS带宽控制， CPU调度算法，支持CPU配置和节流
- TCP防缓冲器， 解决缓冲区膨胀， 包括 传输包数据的字节队列限制(BQL)，CoDel队列管理，TCP小队列，比例积分控制器增强(PIE)包调度程序
- uprobes: 动态跟踪用户级软件的 基础设置，被其他工具(perf,SystemTap)使用
- TCP早期重传， RFC 5827， 减少触发快速重传所需的重复确认

TFO，NUMA平衡，SO_REUSEPORT，SSD缓存设备，bcache，TCP TLP，NO_HZ_FULL，多队列块IO，
SCJED_DEADLINE，TCP autocorking，

![aa143a919cb4d40d14e7087700392640.png](../_resources/aa143a919cb4d40d14e7087700392640.png)


MSC锁和qspinlock，扩展BPF，Overlayfs，DCTCP，DAX，队列自旋锁，TCP无锁监听器，
cgroup v2，epoll可扩展性，KCM，TCP NV，XDP，TCP BBR，硬件延时跟踪器，

![e63b145fc297680e6a93ed8f7571a974.png](../_resources/e63b145fc297680e6a93ed8f7571a974.png)
![0e26dfc3b60b9d56a88086d77c602dcb.png](../_resources/0e26dfc3b60b9d56a88086d77c602dcb.png)



perf c2c, 英特尔CAT，多队列IO调度器，内核TLS，MSG_ZEROCOPY，PCID，PSI，TCP EDT，
多队列IO，UDP GRO，io_uring，MADV_COLD，MADV_PAGEOUT，MultiPath TCP，
启动时跟踪，热压力，perf火焰图

![0554d48c945445abe1404ba1fddbb79c.png](../_resources/0554d48c945445abe1404ba1fddbb79c.png)
![f7b3df044e93bfabe6ce9f3bc18c8ecf.png](../_resources/f7b3df044e93bfabe6ce9f3bc18c8ecf.png)


这里没有列出，对于 锁，驱动，VFS，文件系统，异步IO，内存分配器，NUMA，新处理器指令支持，GPU，性能工具，Ftrace 的 许多小的性能改进。


3.4.2 systemd

是常用的linux服务管理器， 作为 UNIX init系统的 替代品 开发的。

有各种功能，包括 依赖感知服务启动 和 服务时间统计。

在系统性能方面，偶尔遇到的任务是 优化系统的启动时间， systemd 的时间统计 可以显示 调整的方向。

`systemd-analyze` 命令 报告 总体启动时间。

`systemd-analyze critial-chain` 可以看到更多信息， 会显示 关键路径: 导致延时的 各步的序列。
。。就是 启动时间 肯定是 最慢的启动链， 这个就可以 显示 最慢的启动链。


---

3.4.3 KPTI(meltdown)

2018年 linux4.14 添加的， 内核页表隔离(KPTI) 是对 被称为meltdown 的 interl处理器漏洞的一种缓解。

旧的linux内核版本 有 用于类似目的的 KAISER 补丁， 但 需要额外的 CPU周期，上下文切换，系统调用时 额外的 TLB刷新， 会降低 处理器性能。

linux在 同一版本(4.14)中 添加了 对 进程上下文ID (PCID) 的支持， 只要处理器支持 pcid，就可以 避免一些 tlb 刷新

。。tlb: 转译后备缓冲器/页表缓存/转址旁路缓存， 是CPU的一种缓存，加速 虚拟地址 到 物理地址的 转译速度。

KPTI 对 netflix 云计算的性能影响，在 0.1% - 6% 之间， 取决于 工作负载的 系统调用率 ( 成正比)

额外的调优可以进一步减少成本: 巨型页 可以让刷新的 tlb更快地 热起来。


---

3.4.4 eBPF

BPF: berkeley packet filter， 是一项 不知名的技术，最早开发于1992年， 改善了 数据包捕获工具的性能。

2013年 重写， 2014年 进入linux内核。 成为了 一个 通用的执行引擎， 用于各种事情，包括 网络，可观测性，安全。

BPF是一项 灵活且高效的技术，由 指令集、存储对象（map）和helper函数组成。
鉴于BPF的虚拟指令集规范，它可以被认为是一个虚拟机。
BPF程序在内核态下运行（如图3.2所示）, 并被配置为运行在 socket event、tracepoint、USDT probe、kprobes、uprobes 和 perf_events等这些事件上。

BPF字节码必须 首先通过一个 检查安全的验证器，确保 BPF程序 不会崩溃或 破坏内核。
还可以使用一个 BPF类型格式(BTF)系统 来 理解 数据类型和结构。

BPF程序可以通过perf环形缓冲区输出数据，这是一种有效的发送每个事件的数据的方法，或者通过适合于统计的map来输出数据

因为BPF正在为新一代高效、安全和先进的跟踪工具提供动力,所以BPF对系统性能分析很重要。
它为现有的内核事件源: tracepoint、kprobes、uprobes和perf_events提供了可编程性



## 3.5 其他主题

PGO内核，unikernel, 微内核，混合内核，分布式OS

3.5.1 PGO内核

剖析引导的优化(PGO), 也称为 反馈引导的优化(FDO)， 使用 CPU剖析信息 来改善 编译器的决策。

这可以应用于内核构建，过程:
1. 在生产环境中，实施一次 CPU剖析
2. 基于该 CPU剖析 重新编译内核
3. 部署新的内核到 生产环境

这就为 特定的工作负载创建了一个 性能更好的内核。

一个相关的 编译优化是 链接时间优化 (LTO)，整个二进制文件被一次性编译，以实现整个程序的优化。

google有一个 AutoFDO工具， 从 perf 收集配置文件，然后转为 编译器使用的 参数。


---

3.5.2 unikernel

单一应用的机器镜像， 将 内核，库，应用软件 结合在一起， 通常可以在 硬件虚拟机 或 裸机的 单一地址空间中运行。
这会带来性能和安全方面潜在的好处: 更少的指令文本意味着更高的CPU高速缓存命中率和更少的安全漏洞。
这也产生了一个问题:可能没有SSH、shell或性能工具供你登录和调试系统，也没有办法添加它们


3.5.3 微内核和混合内核

本章 大部分内容是 类unix的内核，即 单内核， 所有 管理设备的代码 都作为一个 大的内核程序 一起运行。

微内核， 内核软件 被保持在 最小的程度，包括 内存管理，线程管理，进程间通信。  文件系统，网络栈，驱动程序 是 通过 用户态的 软件实现的。

微内核的一个缺点是， 执行 IO 和其他功能 会需要 额外的 IPC步骤， 降低了 性能。 一种解决方案是 混合内核， 混合内核 将 性能的关键服务 移回 内核空间 ( 用 直接函数调用 代替 IPC)


3.5.4 分布式OS

在一组独立的计算机节点 上运行 一个 OS实例，并将其连成网络。

没有广泛使用。



## 3.6 内核比较

linux的内核 超过其他内核，因为 linux 在 性能改进，应用，驱动支持 方面 做了大量工作。



# ch04 观测工具



4.1 工具范围

![4f58d21667489e2a1f0edd983d3173b5.png](../_resources/4f58d21667489e2a1f0edd983d3173b5.png)

这些工具大多 聚焦在 某个一定的资源(如 CPU，内存，磁盘)上


4.1.1 静态性能工具

检查的是 系统在静止状态下的 特性。

![6625f896dfc3c967c7a36dd9ac730a7d.png](../_resources/6625f896dfc3c967c7a36dd9ac730a7d.png)


## ==4.1.2 危机处理工具==

遇到生产环境的性能问题，需要 各种性能工具来调试时， 你可能发现 这些工具 都没有安装。
更糟的是， 由于服务器正遭受 性能问题，安装这些工具 可能比 平时 花费更多的时间。

下面的推荐的 危机处理工具 的安装包 或 源码库。 名字是 ubuntu/debian 中的名称

![251f020f18dcacdc4e73603dc81eb3a0.png](../_resources/251f020f18dcacdc4e73603dc81eb3a0.png)

![185b2c3e3ef421eb7005da08fd650fd0.png](../_resources/185b2c3e3ef421eb7005da08fd650fd0.png)

默认的linux 发行版 可能只安装了 procps， util-linux


只是 安装，通常是不够的， 可能 还需要 配置内核，用户空间软件 来支持 这些工具。
跟踪工具 通常需要 启用 某些 内核 CONFIG选项，如 CONFIG_FTRACE，CONFIG_BPF。

剖析工具 通常需要 将 软件配置为 支持 栈遍历，或 需要使用软件(包括系统库: libc,libpthread等) 的 帧指针编译版本， 或需要安装 debuginfo 包 来支持 dwarf栈遍历。


---

4.2 工具类型

性能观测工具 按照 系统级别 和 进程级别 来分类。
多数工具 要么基于 计数器，要么基于 事件。


![fe042daf0207a92b9d94ce713a18288b.png](../_resources/fe042daf0207a92b9d94ce713a18288b.png)


---

4.2.1 固定计数器

内核维护了 各种提供系统统计数据的 计数器。通常，计数器被实现为 无符号整型， 发生事件时 递增。

一个常见的内核方法是 维护 一对累积计数器: 一个对 事件计数， 一个记录事件的总时间。 通过 总时间/数量，就可以得到 事件的平均时间。

==计数器的使用 可以认为是 0开销的==， 因为它们 默认开启，且 始终由 内核维护。 唯一的额外开销是 从 用户空间读取 它们 (可以忽略不计)。


系统级别
下面的工具 利用 内核的计数器 在 系统 软硬件的 环境中 检查 系统级别的活动。 包括
- vmstat, 统计虚拟内存和物理内存，系统级别的
- mpstat, 检测每个CPU的使用情况
- iostat, 检测每个磁盘IO的使用情况，由 块设备接口报告
- nstat, tcp/ip 栈的统计
- sar, 各种各样的统计，能归档历史数据

这些工具有一个 使用惯例，即 可选时间间隔 和次数。
`vmstat 1 3` 1秒间隔，输出3次。


进程级别
下面的工具以进程为导向，使用的是 内核为每个进程维护的计数器
- ps，进程状态，显示进程的各种统计信息。
- top，
- pmap，将进程的 内存段 和 使用统计 一起列出

一般来说，上述工具是从 `/proc` 文件系统中 读取统计信息。


---

4.2.2 剖析

profiling 通过对目标收集采样 或快照 来归纳目标特征。

比如可以剖析: CPU使用率， CPU硬件缓存未命中， 总线活动。

剖析 和 跟踪 通常只在 需要时 才启用， 因为它们在收集时 会产生一些CPU开销， 存储时 也产生 存储开销。

基于定时器的 剖析起更安全，因为事件频率是已知的，所以 开销可以预测， 而且可以调整 事件频率，使得 开销可以被忽略。

系统级别
系统级别的linux剖析器( 这些也适用于 单个进程) 包括
- perf， 标准剖析器，包含 其他子命令
- profile， 来自BCC代码库的 基于 BPF的 CPU剖析器，在 内核上下文中 对 栈踪迹 进行频率统计
- Intel VTune Amplifier XE

进程级别
- grof， gnu的剖析工具，分析由 编译器添加的 剖析信息，例如 gcc -pg
- cachegrind, valgrind的一个工具，可以对 硬件缓存的使用情况 进行剖析， 使用 kcachegrind 可视化
- Java Flight Recorder(JFR)


---

4.2.3 跟踪

跟踪每次发生的 记录事件，并存储 事件的 细节信息，供以后分析或生成摘要。

和剖析不同， 跟踪 是 收集/检查 所有的事件， 而不是 某个样本。  跟踪的 开销更大， 可能对生产环境 产生 负面影响


系统级别
- tcpdump，网络包跟踪 (使用 libpcap)
- biosnoop，块IO跟踪 (使用BCC或 bpftrace)
- execsnoop，新的进程跟踪 (使用BCC或 bpftrace)
- perf，linux标准剖析器，也可以跟踪事件
- perf trace，特殊的 perf子命令，可以跟踪 系统级别的 系统调用
- Ftrace，linux内置的跟踪器
- BCC，基于BPF的跟踪库和工具集
- bpftrace，基于BPF的跟踪器和工具库


进程级别
- strace，系统调用 跟踪
- gdb，代码级别的调试器


---

4.2.4 监测

监测持续记录统计数据，以备日后需要


sar
监测单个OS主机的 传统工具是 system activity reporter。
sar是基于计数器的，在预定的时间 (通过cron) 执行以 记录系统级别 计数器的状态。

sar可以记录 几十个不同的 统计数据。


SNMP

189












































# end

