深入Linux内核架构

2.6.24  2008.1 太老了。

2024-03-26 17:37


名词
- RCU, 5.2.4，read copy update, 修改时，复制一份，在副本上修改，然后 修改指向原对象的 指针 ，让这个指针 指向 修改后的副本。 适合读多写少，可以 读写并发。
- slab，3.6，比页更小的内存，内存池。
- TLB，地址转换中出现最频繁的那些地址，保存到称为地址转换后备缓冲器（Translation Lookaside Buffer，TLB）的CPU高速缓存中。无需访问内存中的页表即可从高速缓存直接获得地址数据，因而大大加速了地址转换

- UMA计算机(一致内存访问，uniform memory access)
  将可用内存以连续方式组织起来（可能有小的缺口）。
  SMP系统中的每个处理器访问各个内存区都是同样快。
- NUMA计算机(非一致内存访问，non-uniform memory access)
  总是多处理器计算机。
  系统的各个CPU都有本地内存，可支持特别快速的访问。
  各个处理器之间通过总线连接起来，以支持对其他CPU的本地内存的访问，当然比访问本地内存慢些。



硬盘中 块 是4k
内存中 页 是4k


[[toc]]

---
---


# ch01 简介和概述

版本2.6.24，2008年1月末

。。 v6.9-rc1 。。


在纯技术层面上，==内核是硬件与软件之间的一个中间层==。
其作用是==将应用程序的请求传递给硬件==，并==充当底层驱动程序，对系统中的各种设备和组件进行寻址==

## 实现策略

OS方面，主要有2种主要的范型
- 微内核，最基本的功能由微内核实现。其他功能委托给独立的进程，它们可以和 微内核通信
- 宏内核，构建系统内核的传统方法。内核的全部代码，包括所有子系统（如内存管理、文件系统、设备驱动程序）都打包到一个文件中。内核中的每个函数都可以访问内核中所有其他部分

Linux 是宏内核。
宏内核的性能比 微内核好。 (因为不需要IPC，频繁的用户态/内核态切换)


## 内核的组成部分

![0b885d9332b554f5650b8dd4d4f8ef97.png](../_resources/0b885d9332b554f5650b8dd4d4f8ef97.png)

---

每个进程都在CPU的虚拟内存中分配地址空间。
各个进程的地址空间是完全独立的，因此==进程并不会意识到彼此的存在==。

- 进程切换
- 调度，确定哪个进程运行多长时间

Linux对进程采用了一种层次系统，每个进程都依赖于一个父进程。
内核启动init程序作为第一个进程，该进程负责进一步的系统初始化操作，并显示登录提示符或图形登录界面（现在使用比较广泛）。
因此init是进程树的根，所有进程都直接或间接起源自该进程

pstree 查看系统进程树，init是root

UNIX OS有2种创建新进程的方法
- fork，创建当前进程的一个副本，父进程和子进程 只有 PID(进程ID) 不同，linux使用 COW(写时复制，copy on write) 来让 fork 更高效
- exec，将一个新程序加载到当前进程的内存中并执行

Linux使用 clone 来创建==线程==。工作方式类似 fork， 但使用 精确的检查，以确认哪些资源 与父进程共享，哪些资源 为线程独立创建。


## 命名空间
命名空间，这使得不同的进程可以看到不同的系统视图。
启用命名空间之后，以前的全局资源现在具有不同分组。每个命名空间可以包含一个特定的PID集合，或可以提供文件系统的不同视图，在某个命名空间中挂载的卷不会传播到其他命名空间中。


## 虚拟地址
Linux将虚拟地址空间划分为两个部分，分别称为内核空间和用户空间

系统中每个用户进程都有自身的虚拟地址范围，从0到TASK_SIZE。
用户空间之上的区域（从TASK_SIZE到232或264）保留给内核专用，用户进程不能访问。
TASK_SIZE是一个特定于计算机体系结构的常数

由于地址空间虚拟化的结果，==每个用户进程都认为自身有3 GiB内存==。
各个系统进程的==用户空间是完全彼此分离==的。
而虚拟地址空间顶部的==内核空间总是同样的==，无论当前执行的是哪个进程

## 特权
尽管英特尔处理器区分4种特权级别，但Linux只使用两种不同的状态：核心态和用户状态。

用户进程不能操作或读取内核空间中的数据，也==无法执行内核空间中的代码==。

从用户状态到核心态的==切换==通过==系统调用==的特定转换手段完成

除了代表用户程序执行代码之外，内核还可以由异步硬件中断激活，然后在中断上下文中运行。
与在进程上下文中运行的主要区别是，在中断上下文中运行不能访问虚拟地址空间中的用户空间部分。


---

在核心态和用户状态执行。
CPU大多数时间都在执行用户空间中的代码。
当应用程序执行系统调用时，则切换到核心态，内核将完成其请求。
在==此期间，内核可以访问虚拟地址空间的用户部分==。
在系统调用完成之后，CPU切换回用户状态。
==硬件中断==也会使CPU切换到核心态，这种情况下==内核不能访问用户空间==


`ps fax`
在多处理器系统上，==许多线程启动时指定了CPU，并限制只能在某个特定的CPU上运行==。
从内核线程名称之后的斜线和CPU编号可以看到这一点。



## 页表
==页表==来为物理地址分配虚拟地址。虚拟地址关系到进程的用户空间和内核空间，而物理地址则用来寻址实际可用的内存

物理内存页经常称作页帧。相比之下，页则专指虚拟地址空间中的页。


由于内核负责将虚拟地址空间映射到物理地址空间，因此可以决定哪些内存区域==在进程之间共享，哪些不共享==。


用来将虚拟地址空间映射到物理地址空间的数据结构称为==页表==

因为虚拟地址空间的大部分区域都没有使用，因而也没有关联到页帧，那么就可以使用功能相同但内存用量少得多的模型：==多级分页==。

为减少页表的大小并容许忽略不需要的区域，计算机体系结构的设计会将虚拟地址划分为多个部分，如图1-7所示。（具体在地址字的哪些位区域进行划分，可能依不同的体系结构而异，但这与现在我们讨论的内容不相关）。
在例子中，我将虚拟地址划分为4部分，这样就需要一个三级的页表。大多数体系结构都是这样的做法。
但有一些采用了四级的页表，而Linux也采用了四级页表。为简化场景，我在这里会一直用三级页表阐述

![3e15cb6a188b6665027861c10b0f644b.png](../_resources/3e15cb6a188b6665027861c10b0f644b.png)


### 全局页目录，中间页目录，页表/页目录，偏移量

虚拟地址的第一部分称为全局页目录（Page Global Directory，PGD）。
PGD用于索引进程中的一个数组（每个进程有且仅有一个），该数组是所谓的全局页目录或PGD。
PGD的数组项指向另一些数组的起始地址，这些数组称为中间页目录（Page Middle Directory，PMD）。

虚拟地址中的第二个部分称为PMD，在通过PGD中的数组项找到对应的PMD之后，则使用PMD来索引PMD。
PMD的数组项也是指针，指向下一级数组，称为页表或页目录。

虚拟地址的第三个部分称为PTE（Page Table Entry，页表数组），用作页表的索引。
虚拟内存页和页帧之间的映射就此完成，因为页表的数组项是指向页帧的。

虚拟地址最后的一部分称为偏移量。
它指定了页内部的一个字节位置。归根结底，每个地址都指向地址空间中唯一定义的某个字节

页表的一个特色在于，对虚拟地址空间中不需要的区域，不必创建中间页目录或页表。与前述使用单个数组的方法相比，多级页表节省了大量内存。

该方法也有一个缺点。每次访问内存时，必须逐级访问多个数组才能将虚拟地址转换为物理地址。
CPU试图用下面两种方法加速该过程。
1. CPU中有一个专门的部分称为MMU（Memory Management Unit，内存管理单元），该单元优化了内存访问操作。
2. 地址转换中出现最频繁的那些地址，保存到称为地址转换后备缓冲器（Translation Lookaside Buffer，TLB）的CPU高速缓存中。无需访问内存中的页表即可从高速缓存直接获得地址数据，因而大大加速了地址转换

内核与体系结构无关的部分总是假定使用四级页表。



## 内存映射
映射方法可以将==任意来源的数据传输到进程的虚拟地址空间中==。
作为映射目标的地址空间区域，可以像普通内存那样用通常的方法访问。但任何修改都会自动传输到原数据源。
这样就可以使用相同的函数来处理完全不同的目标对象。
例如，文件的内容可以映射到内存中。==处理只需读取相应的内存即可访问文件内容，或向内存写入数据来修改文件的内容==。内核将保证任何修改都会自动同步到文件中。
内核在==实现设备驱动程序时直接使用了内存映射==。==外设的输入/输出可以映射到虚拟地址空间的区域中==。==对相关内存区域的读写会由系统重定向到设备==，因而大大简化了驱动程序的实现。

。。和读取有什么区别？ 加载文件到内存， 是不是就是这个 内存映射？


## 伙伴系统
在内核分配内存时，必须记录页帧的已分配或空闲状态，以免两个进程使用同样的内存区域。
。。但是 2个进程可以使用 相同的 物理内存区域的吧。 不过 切换的 时候 需要 移动，可能要 移到磁盘上。

内存分配和释放非常频繁，内核还必须保证相关操作尽快完成。内核可以只分配完整的页帧。
==将内存划分为更小的部分==的工作，则委托给==用户空间==中的==标准库==。标准库将来源于内核的页帧拆分为小的区域，并为进程分配内存

。。嗯

内核中很多时候要求分配连续页。为快速检测内存中的连续区域，内核采用了一种古老而历经检验的技术：伙伴系统。

系统中的空闲内存块总是两两分组，每组中的两个内存块称作伙伴。伙伴的分配可以是彼此独立的。
但如果两个伙伴都是空闲的，内核会将其合并为一个更大的内存块，作为下一层次上某个内存块的伙伴。

内核对所有大小相同的伙伴（1、2、4、8、16或其他数目的页），都放置到同一个列表中管理。各有8页的一对伙伴也在相应的列表中。

。。就是 merge，split。 当你需要 8个page时， 从 8page 的列表中找，找不到就 从 16page 的列表中找，没有就继续往上找， 找到后，就 split 成2份，放到 下一级中。
。。释放的时候，反向，看下能不能 合并成 上一级。
。。不过应该会保留一些吧。不然 感觉 每次都要 从上级找。

在应用程序释放内存时，内核可以直接检查地址，来判断是否能够创建一组伙伴，并合并为一个更大的内存块放回到伙伴列表中，这刚好是内存块分裂的逆过程。这提高了较大内存块可用的可能性。

==碎片==
频繁的分配和释放页帧可能导致一种情况：系统中有若干页帧是空闲的，但却散布在物理地址空间的各处
系统中缺乏连续页帧组成的较大的内存块，

### slab缓存
内核本身经常需要比完整页帧小得多的内存块。由于==内核无法使用标准库的函数==，因而必须==在伙伴系统基础上自行定义额外的内存管理层==，将伙伴系统提供的页==划分为更小的部分==。
该方法不仅可以分配内存，还为频繁使用的小对象实现了一个一般性的缓存——slab缓存。

它可以用两种方法分配内存
- 对==频繁使用==的对象，内核定义了==只包含了所需类型对象实例的缓存==。每次需要某种对象时，可以从对应的缓存快速分配（使用后释放到缓存）。slab缓存自动维护与伙伴系统的交互，在缓存用尽时会请求新的页帧
- 对通常情况下小内存块的分配，内核针对==不同大小==的对象定义了一组slab缓存，可以像用户空间编程一样，用相同的函数访问这些缓存。不同之处是这些函数都增加了前缀k，表明是与内核相关联的：==kmalloc和kfree==。

。。第一个是 object pool ？ 第二个是 根据 size 建立 pool ？


## 页面交换和页面回收
页面交换通过利用磁盘空间作为扩展内存，从而增大了可用的内存

通过==缺页异常机制==，这种切换操作对应用程序是透明的(。。无感的)。
在进程试图访问此类页帧时，CPU则启动一个可以==被内核截取的缺页异常==。此时内核可以将硬盘上的数据切换到内存中。接下来用户进程可以恢复运行。

页面回收用于将内存映射==被修改的内容与底层的块设备同步==，为此有时==也简称为数据回写==。
数据刷出后，==内核即可将页帧用于其他用途==（类似于页面交换）。内核的数据结构==包含了与此相关的所有信息==，当==再次需要该数据==时，可根据相关信息==从硬盘找到相应的数据==并加载。

。。数据回写，页帧回收，但是 进程不知道这个 页被回收了， 它还会访问这个页， 此时会 根据 内核中的信息，从 磁盘读取。
。。 但是，你回收后 用来干嘛？ 如果还是这个 进程使用的话， 下次 为什么会访问这个页？  除非 这个 页 不是这个进程使用， 但是 进程的 地址空间是独立的， 没有其他人会用啊，  内核不太可能 把 它自己的数据 加载到 用户空间 吧。

。。
页面回收的基本思想就是将磁盘和内存看做一体,只是访问速度有差异,将经常访问的数据放到速度比较快的内存中,不经常访问的数据放到磁盘上.
。。
。。但是这是 页面交换啊。
。。主要是 书上说， 用作其他用途。  除非 这段内存 不是 进程申请的。 不然 不可能 页面回收，用作其他用途， 但是 进程空间，还有谁能 修改呢？


计时
jiffies，一秒递增 100 - 1000 次
jiffies_64


## 1.3.7 系统调用

系统调用是用户进程与内核交互的经典方法。

POSIX标准定义了许多系统调用，以及这些系统调用在所有遵从POSIX的系统包括Linux上的语义。传统的系统调用==按不同类别分组==，如下所示。
- 进程管理：创建新进程，查询信息，调试。
- 信号：发送信号，定时器以及相关处理机制。
- 文件：创建、打开和关闭文件，从文件读取和向文件写入，查询信息和状态。
- 目录和文件系统：创建、删除和重命名目录，查询信息，链接，变更目录。
- 保护机制：读取和变更UID/GID，命名空间的处理。
- 定时器函数：定时器函数和统计信息。


所有这些函数都对内核提出了要求。
这些函数不能以普通的用户库形式实现，因为==需要特别的保护机制来保证系统稳定性或安全==不受危及。
此外==许多调用依赖内核内部的结构或函数==来得到所需的数据或结果，这也导致了无法在用户空间实现。
在发出系统调用时，==处理器==必须==改变特权级别==，从用户状态切换到核心态。


## 设备驱动程序，块设备，字符设备

设备驱动程序用于与系统连接的输入/输出装置通信，如硬盘、软驱、各种接口、声卡等

按照经典的UNIX箴言“万物皆文件”（everything is a file），==对外设的访问可利用/dev目录下的设备文件来完成==，程序对设备的处理==完全类似于常规的文件==

设备驱动程序的任务在于支持应用程序经由设备文件与设备通信。换言之，使得能够按适当的方式在设备上读取/写入数据

外设可分为以下两类
- 字符设备
  提供连续的数据流，应用程序可以顺序读取，通常不支持随机存取。
  调制解调器是典型的字符设备
- 块设备
  应用程序可以随机访问设备数据，程序可自行确定读取数据的位置。
  硬盘是典型的块设备


## 网络
网卡也可以通过设备驱动程序控制，但在内核中属于特殊状况，因为网卡==不能利用设备文件访问==
原因在于在网络通信期间，数据打包到了各种协议层中。

在接收到数据时，内核必须针对各协议层的处理，对数据进行拆包与分析，然后才能将有效数据传递给应用程序。
在发送数据时，内核必须首先根据各个协议层的要求打包数据，然后才能发送。

Linux使用了源于BSD的 ==套 接 字== 抽象
套接字可以看作应用程序、文件接口、内核的网络实现之间的代理



## 文件系统

存储使用了层次式文件系统。文件系统使用目录结构组织存储的数据，并将其他元信息（例如所有者、访问权限等）与实际数据关联起来。

内核必须提供一个额外的软件层，将各种底层文件系统的具体特性与应用层（和内核自身）隔离开来。
该软件层称为VFS（Virtual Filesystem或Virtual Filesystem Switch，虚拟文件系统或虚拟文件系统交换器）。
VFS既是向下的接口（==所有文件系统都必须实现该接口==），同时也是向上的接口（用户进程==通过系统调用最终能够访问文件系统功能==）。如图1-10所示

![6bd29a29a663feb22fe316503fcee162.png](../_resources/6bd29a29a663feb22fe316503fcee162.png)





## 模块与热插拔

模块在本质上不过是普通的程序，只是在内核空间而不是用户空间执行而已。模块必须提供某些代码段在模块初始化（和终止）时执行，以便向==内核注册和注销模块==。

模块代码与普通内核代码的权利（和义务）都是相同的，可以像编译到内核中的代码一样，访问内核中所有的函数和数据


## 缓存

内核使用缓存来改进系统性能。
从==低速的块==设备读取的数据会暂时保持在内存中，即使数据在当时已经不再需要了。
在应用程序下一次访问该数据时，它可以从访问速度较快的内存中读取，因而绕过了低速的块设备。
由于内核是通过基于页的内存映射来实现访问块设备的，因此==缓存也按页组织==，也就是说整页都缓存起来，故称为==页缓存==（page cache）。

块缓存用于缓存没有组织成页的数据，其重要性差得多。在传统的UNIX系统上，块缓存用作系统的主缓存，而Linux很久以前也是这样。到如今，==块缓存已经被页缓存取代了==。



## 链表处理

C程序中重复出现的一项任务是对双链表的处理。内核也需要处理这样的链表。

类型为list_head，其中包含了正向和反向指针

因为链表的实现不是类型安全的，所以查询时需要显式指定类型。



## 对象管理和引用计数

一般性的内核对象机制可用于执行下列对象操作
- 引用计数；
- 管理对象链表（集合）；
- 集合加锁；
- 将对象属性导出到用户空间（通过sysfs文件系统）

```C
// kobject.h
struct kobject {
    const char *k_name; // 对象的文本名，可利用sysfs导出到用户空间
    struct kref kref;   // 用于简化 引用计数的管理
    struct list_head entry; // 用于将若干kobject放置到一个链表中（在这种情况下称为集合）。
    struct kobject *parent; // 指向父对象的指针，建立层次结构
    struct kset *kset;      // 将对象与其他对象放置到一个集合时，则需要kset。
    struct kobj_type *ktype;// 包含kobject的数据结构的更多详细信息，最重要的是用于释放该数据结构资源的析构器函数。
    struct sysfs_dirent *sd;// 用于支持内核对象与sysfs之间的关联
};
```

kobject 会直接嵌入到 内核的许多数据结构中， 而不是通过 指针访问。

kobject的标准方法
- kobject_get, kobject_put： 引用计数器 +1，-1
- kobject_(un)register： 从层次结构中注册或删除
- kobject_init， 初始化kobject
- kobject_add，初始化一个内核对象，并显示在 sysfs 中
- kobject_cleanup，在不需要 kobject 时，释放分配的资源


```C
// kref.h
struct kref {
    atomic_t refcount;
};
```


。。最新的。6.9
```C
struct kobject {
	const char		*name;
	struct list_head	entry;
	struct kobject		*parent;
	struct kset		*kset;
	const struct kobj_type	*ktype;
	struct kernfs_node	*sd; /* sysfs directory entry */
	struct kref		kref;

	unsigned int state_initialized:1;
	unsigned int state_in_sysfs:1;
	unsigned int state_add_uevent_sent:1;
	unsigned int state_remove_uevent_sent:1;
	unsigned int uevent_suppress:1;

#ifdef CONFIG_DEBUG_KOBJECT_RELEASE
	struct delayed_work	release;
#endif
};
```
。。。



## 数据类型

内核使用 typedef 来定义各种数据类型，以避免依赖于体系结构相关的特性，比如，各个处理器上标准数据类型的位长不一致。

定义的类型名称如sector_t（用于指定块设备上的扇区编号）、pid_t（表示进程ID）等，这些都是由内核在特定于体系结构的代码中定义的，以确保相关类型的==值落在适当的范围==内。

如果某个变量的类型是typedef而来的，则不能直接访问，而需要通过辅助函数

### 字节序

![d0c84a4632ff8e666f8e2377ebb4ecdb.png](../_resources/d0c84a4632ff8e666f8e2377ebb4ecdb.png)


内核提供了各种函数和宏，可以在CPU使用的格式与特定的表示法之间转换

cpu_to_le64将64位数据类型转换为小端序格式，而le64_to_cpu所做的刚好相反

如果体系结构采用的字节序是小端序格式，这两个例程当然是空操作，否则必须相应地交换字节位置



per-cpu 变量

普通的用户空间程序设计不会涉及

通过DEFINE_PER_CPU(name, type)声明，其中name是变量名，而type是其数据类型（例如int[3]、struct hash等）。
在单处理器系统上，这与常规的变量声明没有不同。
在有若干CPU的SMP系统上，会为每个CPU分别创建变量的一个实例。
用于某个特定CPU的实例可以通过get_cpu(name, cpu)获得，其中smp_processor_id()可以返回当前活动处理器的ID，用作前述的cpu参数。



---

源代码中的多处指针都标记为__user，该标识符对用户空间程序设计是未知的。
内核使用该记号来==标识指向用户地址空间中区域的指针==，在没有进一步预防措施的情况下，不能轻易访问这些指针指向的区域。







# ch02 进程管理和调度

多进程，进程间隔离

内核必须能够
- 除非明确地要求，否则应用程序不能彼此干扰
- CPU时间必须在各种应用程序之间尽可能公平地共享
- 决定为各个进程分配多长时间，何时切换到下一个进程，哪个进程是下一个
- 在内核从进程A切换到进程B时，必须确保进程B的执行环境与上一次撤销其处理器资源时完全相同



## 2.1 进程优先级

进程可以分为==实时==进程和==非实时==进程。

硬实时进程有严格的时间限制，某些任务必须在指定的时限内完成。
如果飞机的==飞行控制命令==通过计算机处理，则必须尽快处理发送

Linux不支持硬实时处理，至少在主流的内核中不支持。但有一些修改版本如RTLinux、Xenomai、RATI提供了该特性。

软实时进程是硬实时进程的一种弱化形式。尽管仍然需要快速得到结果，但稍微晚一点不会造成世界末日。

大多数进程是没有特定时间约束的普通进程，但仍然可以根据重要性来分配优先级。


抢占式多任务处理
各个进程都分配到一定的时间段可以执行。
时间段到期后，内核会从进程收回控制权，让一个不同的进程运行，而不考虑前一进程所执行的上一个任务。



## 进程生命周期

进程可能有以下几种状态。
- 运行：该进程此刻==正在执行==。
- 等待：进程==能够运行==，但==没有得到许可==，因为CPU分配给另一个进程。调度器可以在下一次任务切换时选择该进程。
- 睡眠：进程正在睡眠==无法运行==，因为它在等待一个外部事件。调度器无法在下一次任务切换时选择该进程。
- 终止


![6800fdd248272126ae561e7577efa291.png](../_resources/6800fdd248272126ae561e7577efa291.png)



上文没有列出的一个特殊的进程状态是所谓的“僵尸”状态。顾名思义，这样的进程已经死亡，但仍然以某种方式活着。
实际上，说这些进程死了，是因为其资源（内存、与外设的连接，等等）==已经释放==，因此它们无法也决不会再次运行。
说它们仍然活着，是因为==进程表中仍然有==对应的表项。

僵尸是如何产生的？
其原因在于UNIX操作系统下==进程创建和销毁的方式==。
在两种事件发生时，程序将终止运行。
第一，程序必须由另一个进程或一个用户杀死（通常是通过发送SIGTERM或SIGKILL信号来完成，这等价于正常地终止进程）；
进程的父进程在子进程终止时必须调用或已经调用wait4（读做wait for）系统调用。 
这相当于==向内核证实父进程已经确认子进程的终结==。
该系统调用使得内核可以释放为子进程保留的资源。

只有在第一个条件发生（程序终止）而第二个条件不成立的情况下（wait4），才会出现“僵尸”状态。
在进程终止之后，其数据尚未从进程表删除之前，进程总是暂时处于“僵尸”状态。有时候（例如，如果父进程编程极其糟糕，没有发出wait调用），僵尸进程可能稳定地寄身于进程表中，直至下一次系统重启。
从进程工具（如ps或top）的输出，可以看到僵尸进程。因为残余的数据在内核中占据的空间极少，所以这几乎不是问题



抢占式多任务处理

用户状态和核心态。这反映了所有现代CPU都有（至少）两种不同执行状态的事实，其中一种具有无限的权利，而另一种则受到各种限制。

进程通常都处于用户状态，只能访问自身的数据，无法干扰系统中的其他应用程序，甚至也不会注意到自身之外其他程序的存在。


如果进程想要访问系统数据或功能（后者管理着所有进程之间共享的资源，例如文件系统空间），则必须切换到核心态。
显然这只能在受控情况下完成，否则所有建立的保护机制都是多余的，而且这种访问必须经由明确定义的路径。
第1章简要提到“==系统调用==”是在状态之间切换的一种方法。第13章深入讨论了系统调用的实现。

从用户状态切换到核心态的第二种方法是通过==中断==，此时切换是自动触发的。
系统调用是由用户应用程序有意调用的，中断则不同，其发生或多或少是==不可预测==的。
处理中断的操作，==通常与中断发生时执行的进程无关==。



内核的抢占调度模型建立了一个层次结构，用于判断哪些进程状态可以由其他状态抢占。
- 普通进程总是可能被抢占，甚至是由其他进程抢占
- 如果系统处于核心态并正在处理系统调用，那么系统中的其他进程是无法夺取其CPU时间的
- 中断可以暂停处于用户状态和核心态的进程。中断具有最高优先级

在内核2.5开发期间，一个称之为内核抢占（kernel preemption）的选项添加到内核。 
该选项支持在紧急情况下切换到另一个进程，甚至当前是处于核心态执行系统调用（中断处理期间是不行的）。


## 进程表示
Linux内核涉及进程和程序的所有算法都围绕一个名为task_struct的数据结构建立，该结构定义在include/sched.h中

。书上简化后，写了 2页。。。

。。现在在 include/linux/sched.h , 800行。。


该结构的内容可以分解为各个部分，每个部分表示进程的一个特定方面。
- 状态和执行信息，如待决信号，使用的二进制格式，进程ID，到父进程和其他有关进程的指针，优先级，程序执行有关的时间信息
- 有关已经分配的虚拟内存的信息。
- 进程身份凭据，如用户ID，组ID，权限。
- 使用的文件包含程序代码的二进制文件，以及进程所处理的所有文件的文件系统信息。
- 线程信息记录该进程特定于CPU的运行时间数据
- 在与其他应用程序协作时所需的进程间通信有关的信息。
- 该进程所用的信号处理程序，用于响应到来的信号。



### 资源限制
对进程使用系统资源施加某些限制
使用了task_struct中的rlim数组，数组项类型为struct rlimit
```C
// resource.h
struct rlimit {
    unsigned long rlim_cur; // 进程当前的资源限制，软限制
    unsigned long rlim_max; // 该限制的最大容许值，硬限制
};
```
。。6.9 task_struct 中没有，在 sched.h 中搜 lim 都没有搜到有用的信息。

使用 setrlimit 来增减 当前限制。

特定于进程的资源限制
- RLIMIT_CPU    按毫秒计算的最大CPU时间
- RLIMIT_FSIZE  允许的最大文件长度
- RLIMIT_DATA   数据段的最大长度
- RLIMIT_STACK （用户状态）栈的最大长度
- RLIMIT_CORE   内存转储文件的最大长度
- RLIMIT_RSS    常驻内存的最大尺寸。换句话说，进程使用页帧的最大数目。目前未使用
- RLIMIT_NPROC  与进程真正UID关联的用户可以拥有的进程的最大数目
- RLIMIT_NOFILE 打开文件的最大数目
- RLIMIT_MEMLOCK 不可换出页的最大数目
- RLIMIT_AS     进程占用的虚拟地址空间的最大尺寸
- RLIMIT_LOCKS  文件锁的最大数目
- RLIMIT_SIGPENDING 待决信号的最大数目
- RLIMIT_MSGQUEUE 信息队列的最大数目
- RLIMIT_NICE   非实时进程的优先级（nice level）
- RLIMIT_RTPRIO 最大的实时优先


`cat /proc/self/limits`
查看当前rlimit值


### 进程类型
fork
exec

clone用于实现 线程。


### 命名空间
提供了虚拟化的一种轻量级形式



### 进程ID，pid


## 2.4 进程管理相关的系统调用

### 进程复制
linux 有3个
- fork，重量级调用，建立了父进程的一个完整副本，然后作为子进程执行。使用 COW技术
- vfork，类似于fork，但不创建 父进程数据的副本。 父子进程 共享数据。 一个进程操作共享数据，另一个进程会注意到。 。 。由于fork使用了写时复制技术，vfork速度方面不再有优势，因此应该==避免使用==它。
- clone，产生 ==线程==，可以对 父子进程之间的 共享，复制 进行精确控制


fork,vfork,clone 系统调用的 入口点分别是 是sys_fork、sys_vfork和sys_clone函数

上述函数的任务是从处理器==寄存器==中==提取由用户空间提供的信息==，调用体系结构无关的==do_fork==函数，后者负责进程复制。



父子进程的task_struct实例==只有一个成员不同==：新进程分配了一个==新的核心态栈==，即task_struct->stack。
通常栈和thread_info一同保存在一个联合中，thread_info保存了线程所需的所有特定于处理器的底层信息。

```C
// sched.h
union thread_union { 
    struct thread_info thread_info; 
    unsigned long stack[THREAD_SIZE/sizeof(long)];      // THREAD_SIZE / sizeof(long) !!!
};
```

在大多数体系结构上，使用一两个内存页来保存一个thread_union的实例。
在IA-32上，两个内存页是默认设置，==因此可用的内核栈长度略小于8 KiB==，其中一部分被thread_info实例占据。
不过要注意，配置选项4KSTACKS会将栈长度降低到4 KiB，即一个页面。





### 内核线程
直接由内核本身启动的进程。内核线程实际上是将内核函数委托给独立的进程
内核线程经常称之为（内核）守护进程
它们用于执行下列任务。
- 周期性地将修改的内存页与页来源块设备同步（例如，使用mmap的文件映射）。
- 如果内存页很少使用，则写入交换区。
- 管理延时动作（deferred action）。
- 实现文件系统的事务日志。


有两种类型的内核线程。
- 类型1：线程启动后一直等待，直至内核请求线程执行某一特定操作。
- 类型2：线程启动后按周期性间隔运行，检测特定资源的使用，在用量超出或低于预置的限制值时采取行动。内核使用这类线程用于连续监测任务

大多数计算机上系统的==全部虚拟地址空间分成两个部分==：==底部可以由用户层程序访问==，==上部则专供内核使用==。



惰性TLB处理（lazy TLB handling）


---

使用kthread_create_cpu代替kthread_create创建内核线程，使之==绑定到特定的CPU==。
内核线程会出现在系统进程列表中，但在ps的输出中由方括号包围，以便与普通进程区分。
如果内核线程绑定到特定的CPU，CPU的编号在斜线后给出


---

execve



## 2.5 调度器的实现

内存中保存了对每个进程的唯一描述，并通过若干结构与其他进程连接起来。
调度器面对的情形就是这样，其任务是在程序之间共享CPU时间，创造并行执行的错觉。
任务分为两个不同部分：一个涉及==调度策略==，另一个涉及==上下文切换==


schedule函数是理解调度操作的起点。该函数定义在kernel/sched.c中，是内核代码中最常调用的函数之一。调度器的实现受若干因素的影响而稍显模糊。
- 在多处理器系统上，必须要注意几个细节（有一些非常微妙），以避免调度器自相干扰。
- 不仅实现了优先调度，还实现了Posix标准需要的其他两种软实时策略。
- 使用goto以生成最优的汇编语言代码。这些语句在C代码中来回地跳转，与结构化程序设计的所有原理背道而驰。但如果小心翼翼地使用它，该特性就可以发挥作用（调度器就是一个例子）。


。。没有，没有这个文件，在其他地方有好几个 sched.c， 随便打开了一个 `arch/powerpc/platforms/cell/spufs/sched.c`，里面使用了 schedule()，但是没有定义
。。看了一遍，根据 sched.c 搜到的所有文件中 都没有 schedule()的定义


Linux调度器的一个杰出特性是，它==不需要时间片概念==，至少不需要传统的时间片。

==经典的调度器==对系统中的进程分别计算时间片，使进程运行直至时间片用尽。在==所有进程的所有时间片都已经用尽==时，则需要重新计算。

linux的完全公平调度器 只考虑进程的等待时间，即进程在就绪队列（run-queue）中已经等待了多长时间。对CPU时间需求最严格的进程被调度执行。

调度器的一般原理是，按所能分配的计算能力，向系统中的每个进程提供最大的公正性。



如果通过轮流运行各个进程来模拟多任务，那么当前运行的进程，其待遇显然好于哪些等待调度器选择的进程，即等待的进程受到了不公平的对待。不公平的程度正比于等待时间。
每次调用调度器时，它会挑选具有最高等待时间的进程，把CPU提供给该进程。如果经常发生这种情况，那么进程的不公平待遇不会累积，不公平会均匀分布到系统中的所有进程
。。无法理解
。。第一段的意思是：进程执行完，然后执行下一个进程？
。。第二段的意思是：每隔一段时间，就 挑选 最高等待时间的进程，让它执行？ 就是约等于时间片？


图2-12说明了调度器如何记录哪个进程已经等待了多长时间。由于可运行进程是排队的，该结构称之为 ==就绪队列==
![39aafa4d62f05bb8f8343d03f30ffab9.png](../_resources/39aafa4d62f05bb8f8343d03f30ffab9.png)


所有的可运行进程都按时间在一个红黑树中排序，所谓时间即其等待时间。等待CPU时间最长的进程是最左侧的项，调度器下一次会考虑该进程。等待时间稍短的进程在该树上从左至右排序。

。。
图中 就绪队列的 哪行是不是有问题？  怎么从 红黑转为 数组？ 
为什么不用 heap
红黑树 和 数组 中元素个数 是一样的？
。。

除了红黑树外，就绪队列还装备了虚拟时钟。
该时钟的时间流逝速度慢于实际的时钟，精确的速度依赖于当前等待调度器挑选的进程的数目。
假定该队列上==有4个进程==，那么虚拟时钟将以==实际时钟四分之一==的速度运行。

假定就绪队列的虚拟时间由fair_clock给出，而进程的等待时间保存在wait_runtime。为排序红黑树上的进程，内核使用差值fair_clock - wait_runtime。fair_clock是完全公平调度的情况下进程将会得到的CPU时间的度量，而wait_runtime直接度量了实际系统的不足造成的不公平。

。。wtf？
。。好像理解一些，主要是 没有一个 例子。
。。==要用控制变量法看==
1. 如果 fair_clock 相同， 有2个进程A，B， A等待了10s (wait_runtime)， B等待了20s， 这样的话， A的 差值就是 fair_clock - 10, B的是 fair_clock-20, 插入 红黑树后， B 在 A之前， 即 B的下次调用 会在 A的下次调用之前 被调用。 这样就 弥补了一定的 不公平 ( 因为 B 之前 等待的时间(20s) > A之前等待的(10s))
。。而且 某个进程 等待的时候 够长的话， 可能下一次 就是 还是这个进程。

2. 如果 2个进程，等待时间相同， 那么 先调用的，下一次还是 先调用。 这个就是 完全公平的情况下的， 就类似 1,2,3,4,1,2,3,4,1,2,3,4 这种 完全公平的 顺序。

。。主要就是 不知道 真实情况下 fair_clock 是保存的什么， wait_runtime 保存的是什么。  还有 大约值是多少， 估计wait_runtime 1ms都不满？ 。百度了下， 一个时间片 约 10ms。不同系统 不同

fair_clock，知道，是 内置的一个时钟。
wait_runtime，应该是 开始执行前 才能计算出来的。  就是 应该保存了 上次结束(或开始？) 的时间点， 等待 队列 轮到这个进程后， 用 now - 上次结束时间， 就知道了 它等待了 多久，就是 wait_runtime。
。。

在进程允许运行时，将从wait_runtime减去它已经运行的时间。这样，在按时间排序的树中它会向右移动到某一点，另一个进程将成为最左边，下一次会被调度器选择。


该策略受若干现实问题的影响，已经变得复杂了
- 进程的不同优先级（即，nice值）
- 进程不能切换得太频繁，因为上下文切换，即从一个进程改变到另一个，是有一定开销的。

理解调度决策的一个好方法是，在==编译时激活调度器统计==。
这会在运行时生成文件/proc/sched_debug，其中包含了调度器当前状态所有方面的信息。


### 数据结构

调度器使用一系列数据结构，来排序和管理系统中的进程。调度器的工作方式与这些结构的设计密切相关。几个组件在许多方面彼此交互，图2-13概述了这些组件的关联。

![1939df13d51fd01b47a9939ed84d84e4.png](../_resources/1939df13d51fd01b47a9939ed84d84e4.png)


==2种方式激活调度==
一种是直接的，比如进程打算睡眠或出于其他原因放弃CPU；
另一种是通过周期性机制，以固定的频率运行，不时检测是否有必要进行进程切换。

在下文中我将这两个组件称为==通用调度器==（generic scheduler）或==核心调度器==（core scheduler）。
本质上，通用调度器是一个分配器，与其他两个组件交互


==调度类==用于判断接下来运行哪个进程。
==内核支持不同的调度策略（完全公平调度、实时调度、在无事可做时调度空闲进程==），调度类使得能够以模块化方法实现这些策略，即一个类的代码不需要与其他类的代码交互。

在调度器被调用时，它会查询调度器类，得知接下来运行哪个进程。

在选中将要运行的进程之后，必须执行底层==任务切换==。这需要与CPU的紧密交互。

每个进程都刚好属于某一调度类，各个调度类负责管理所属的进程。通用调度器自身完全不涉及进程管理，其工作都委托给调度器类。
。。多个调度器类 怎么合作？ 哪个调度器类的 首进程 先运行？

进程的 task_struct 中有几个成员和 调度有关
```C
int prio, static_prio, normal_prio; // 静态优先级在进程启动时设置，normal_prio是 静态优先级 + 调度策略 计算出的优先级。 prio 是 调度器使用的 优先级，因为有时需要 提高 优先级。
unsigned int rt_priority;   // 实时优先级 0-99，含义和nice 相反
struct list_head run_list; // 见 最后一个 time_slice
const struct sched_class *sched_class;  // 调度器类
struct sched_entity se;   // 用于组调度
unsigned int policy;  // -- 调度策略 --
cpumask_t cpus_allowed; // -- 位域，限制进程在哪些CPU上运行 --
unsigned int time_slice;  // 循环实时调度所需要的，但不用于完全公平调度器
```

==调度策略==，linux支持5个值
- SCHED_NORMAL，用于普通进程。通过完全公平调度器来处理。
- SCHED_BATCH,SCHED_IDLE，通过完全公平调度器处理，不过可用于 次要的进程。  
  - SCHED_BATCH 用于 ==非交互，CPU使用密集的 批处理进程==。调度决策对此类进程给予“冷处理”：它们决不会抢占CF调度器处理的另一个进程，因此不会干扰交互式进程。如果不打算用nice降低进程的静态优先级，同时又不希望该进程影响系统的交互性，此时最适合使用该调度类。
  - SCHED_IDLE 这个类型的进程 ==重要性也较低==，其相对权重总是最小的。 不负责调度空闲进程，空闲进程由内核提供单独的机制来处理。
- SCHED_RR,SCHED_FIFO，用于实现 软实时进程。 不是由完全公平调度器类处理，而是由 实时调度器类 处理。
  - SCHED_RR 实现了一种 循环方法
  - SCHED_FIFO 使用先进先出



---

调度器类

sched.h
struct sched_class {};

类提供了通用调度器和各个调度方法之间的关联。
调度器类由特定数据结构中汇集的几个函数指针表示。
全局调度器请求的各个操作都可以由一个指针表示。
这使得无需了解不同调度器类的内部工作原理，即可创建通用调度器。

调度类 之间的 结构层次：
==实时进程最重要，在完全公平进程之前处理；而完全公平进程则优先于空闲进程==；空闲进程只有CPU无事可做时才处于活动状态。

各个调度器可以提供的操作。
- enqueue_task
  向就绪队列添加一个新进程。在进程从睡眠状态==变为可运行==状态时，即==发生该==操作。
- dequeue_task
  将一个进程从就绪队列去除。在进程从可运行状态==切换到不可运行==状态时，就会发生该操作。内核有可能因为其他理由将进程从就绪队列去除(如，进程的优先级需要改变)
- sched_yield系统调用，进程==自愿放弃==对处理器的控制权。这导致 内核调用 yield_task
- check_preempt_curr，用一个新唤醒的进程来抢占当前进程。 使用 wake_up_new_task 唤醒新进程时，会使用这个函数
- pick_next_task，选择下个要执行的进程
- put_prev_task，在用另一个进程 代替当前运行的进程 之前使用。 这2个方法不等价于 出队，入队。 它们向进程 提供 或撤销CPU
- set_curr_task，进程的调度策略发生变化时需要调用
- task_tick，每次激活周期性调度器时，由周期性调度器调用
- new_task，建立 fork 系统调用 与 调度器之间的关联。 每次新进程建立后，使用 new_task 通知调度器



---

就绪队列

核心调度器用于管理活动进程的主要数据结构称之为就绪队列。各个==CPU都有自身的就绪队列==，各个活动进程只出现在一个就绪队列中。

就绪队列是全局调度器许多操作的起点
要注意，进程并不是由就绪队列的成员直接管理的！这是各个调度器类的职责，因此在各个就绪队列中嵌入了特定于调度器类的子就绪队列。

就绪队列是使用下列数据结构实现的。
```C
// kernel/sched.c
struct rq { 
  unsigned long nr_running;   // 队列上可运行进程的数目，不考虑其优先级，调度类
  #define CPU_LOAD_IDX_MAX 5 
  unsigned long cpu_load[CPU_LOAD_IDX_MAX]; 
... 
  struct load_weight load;  // 就绪队列当前负荷的度量
  struct cfs_rq cfs; // 和rt，都是 嵌入的 子就绪队列，分别用于 完全公平调度器 和 实时调度器
  struct rt_rq rt; 
  struct task_struct *curr, *idle; // curr指向当前运行的进程的task_struct实例。 idle指向idle进程的task_struct实例，在无其他可执行进程时执行
  u64 clock; // 用于实现 就绪队列自身的时钟。每次调用 周期性调度器时，都会更新clock的值。
... 
}; 
```



---
调度实体

调度器可以操作比进程更一般的实体，因此需要一个适当的数据结构来描述此类实体。

```C
// sched.h
struct sched_entity { 
  struct load_weight load; /* 用于负载均衡 */  // 指定了权重。
  struct rb_node run_node;  // 标准的树节点，使得实体可以在红黑树上排序
  unsigned int on_rq;   // 该实体 当前是否在 就绪队列上 接受调度。
  u64 exec_start;   // 调用时 更新到当前时间
  u64 sum_exec_runtime; // 记录进程执行消耗的CPU时间，用于完全公平调度器。
  u64 vruntime; // 记录 进程执行 消耗的 虚拟时钟上的流逝的时间
  u64 prev_sum_exec_runtime; // 进程被撤销CPU时，sum_exec_runtime被保存到这里
... 
} 
```

每次调用时，会计算当前时间和exec_start之间的差值，exec_start则更新到当前时间。差值则被加到sum_exec_runtime
。？ 那么 exec_start 是启动的时间， 当前 - 上次启动 计算出来的 值 无意义啊。 而且 肯定不是 消耗的时间啊。

每个task_struct都嵌入了sched_entity的一个实例，所以进程是可调度实体。



### 处理优先级
处理优先级相当复杂

---
优先级的内核表示

在用户空间可以通过nice命令设置进程的静态优先级，这在内部会调用==nice系统调用==。
进程的nice值在 -20和+19之间（包含）。值越低，表明优先级越高

内核使用一个简单些的数值范围，从0到139（包含），用来表示内部优先级。同样是值越低，优先级越高。

从==0到99==的范围专供==实时进程==使用。nice值[-20, +19]映射到范围==100到139==


---
计算优先级

回想一下，可知只考虑进程的静态优先级是不够的，还必须考虑下面3个优先级。
- 动态优先级（task_struct->prio）
- 普通优先级（task_struct->normal_prio）
- 静态优先级（task_struct->static_prio）

static_prio是计算的起点。假定它已经设置好，而内核现在想要计算其他优先级。一行代码即可：
`p->prio = effective_prio(p); `

```C
// kernel/sched.c
static int effective_prio(struct task_struct *p) 
{
  p->normal_prio = normal_prio(p);
  /* 
  * 如果是实时进程或已经提高到实时优先级，则保持优先级不变。否则，返回普通优先级：
  */ 
  if (!rt_prio(p->prio))
    return p->normal_prio; 
  return p->prio; 
}
```
这里首先计算了普通优先级，并保存在normal_priority。这个副效应使得能够用一个函数调用设置两个优先级（prio和normal_prio）。
另一个辅助函数rt_prio，会检测普通优先级是否在实时范围中，即是否小于RT_RT_PRIO。
请注意，该检测与调度类无关，它只涉及优先级的数值。


假定我们在处理==普通进程==，不涉及实时调度
在这种情况下，normal_prio只是返回静态优先级。结果很简单：所有3个优先级都是同一个值，即静态优先级


==实时进程==的情况有所不同。注意普通优先级的计算方法：

```C
kernel/sched.c 
static inline int normal_prio(struct task_struct *p) 
{ 
  int prio; 
  if (task_has_rt_policy(p)) 
    prio = MAX_RT_PRIO - 1 - p->rt_priority;  // .
  else 
    prio = __normal_prio(p); 
  return prio; 
} 
```

__normal_prio的计算==只适用于普通==进程。只是返回 静态优先级

而实时进程的普通优先级计算，则需要根据其rt_priority设置。
由于更高的rt_priority值表示更高的实时优先级，内核内部优先级的表示刚好相反，==越低==的值表示的==优先级越高==。


---
计算负荷权重

进程的重要性不仅是由优先级指定的，而且还需要考虑保存在task_struct->se.load的负荷权重。
set_load_weight负责根据进程类型及其静态优先级计算负荷权重

内核不仅维护了负荷权重自身，而且还有另一个数值，用于计算被负荷权重除的结果

一般概念是这样，进程每降低一个nice值，则多获得10%的CPU时间，每升高一个nice值，则放弃10%的CPU时间。为执行该策略，内核将优先级转换为权重值

转换表
```C
kernel/sched.c 
static const int prio_to_weight[40] = { 
/* -20 */ 88761, 71755, 56483, 46273, 36291, 
/* -15 */ 29154, 23254, 18705, 14949, 11916, 
/* -10 */ 9548, 7620, 6100, 4904, 3906, 
/* -5 */ 3121, 2501, 1991, 1586, 1277, 
/* 0 */ 1024, 820, 655, 526, 423, 
/* 5 */ 335, 272, 215, 172, 137, 
/* 10 */ 110, 87, 70, 56, 45, 
/* 15 */ 36, 29, 23, 18, 15, 
}; 
```

对内核使用的范围[0, 39]中的每个nice级别，该数组中都有一个对应项。各数组之间的乘数因子是1.25。

要知道为何使用该因子，可考虑下列例子。
两个进程A和B在nice级别0运行，因此两个进程的CPU份额相同，即都是50%。nice级别为0的进程，其权重查表可知为1024。每个进程的份额是1024/（1024+1024）=0.5，即50%。
如果进程B的优先级加1，那么其CPU份额应该减少10%。换句话说，这意味着进程A得到总的CPU时间的55%，而进程B得到45%。优先级增加1导致权重减少，即1024/1.25≈820。因此进程A现在将得到的CPU份额是1024/(1024+820)≈0.55，而进程B的份额则是820/(1024+820)≈0.45，这样就产生了10%的差值。

执行转换的代码也需要考虑实时进程。实时进程的权重是普通进程的两倍


### 核心调度器

调度器的实现基于两个函数：周期性调度器函数和主调度器函数。
这些函数根据现有进程的优先级分配CPU时间。

本节将论述优先调度的实现方式

---
#### 周期性调度器
周期性调度器在 scheduler_tick 中实现
如果系统正在活动中，内核会按照频率HZ自动调用该函数。
如果没有进程在等待调度，那么在==计算机电力供应不足==的情况下，也可以关闭该调度器以减少电能消耗。例如，笔记本电脑或小型嵌入式系统

该函数有下面两个主要任务。
1. 管理内核中 与整个系统 和 各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1
2. 激活负责当前进程的调度类的周期性调度方法



#### 主调度器
在内核中的许多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数（schedule）。




#### 与fork的交互
每当使用fork系统调用或其变体之一建立新进程时，调度器有机会用sched_fork函数挂钩到该进程。


#### 上下文切换

内核选择新进程之后，必须处理与多任务相关的技术细节。这些细节总称为上下文切换（context switching）。
辅助函数context_switch是个分配器，它会调用所需的特定于体系结构的方法

上下文切换本身通过调用两个特定于处理器的函数完成。
1. switch_mm更换通过task_struct->mm描述的内存管理上下文。该工作的细节取决于处理器，主要包括==加载页表、刷出地址转换后备缓冲器（部分或全部）、向内存管理单元（MMU）提供新的信息==。由于这些操作深入到CPU的细节中，我不打算在此讨论其实现。
2. switch_to切换==处理器寄存器内容和内核栈==（虚拟地址空间的用户部分在第一步已经变更，其中也包括了用户状态下的栈，因此用户栈就不需要显式变更了）。此项工作在不同的体系结构下可能差别很大，代码通常都使用汇编语言编写。


switch_to的复杂之处
调度过程可能选择了一个新进程，而清理则是针对此前的活动进程。
请注意，这不是发起上下文切换的那个进程，而是系统中随机的某个其他进程！
内核必须想办法使得该进程能够与context_switch例程通信，这可以通过switch_to宏实现



惰性FPU模式
由于上下文切换的速度对系统性能的影响举足轻重，所以内核使用了一种技巧来减少所需的CPU时间。
浮点寄存器（及其他内核未使用的扩充寄存器，例如IA-32平台上的SSE2寄存器）除非有应用程序实际使用，否则不会保存。
此外，除非有应用程序需要，否则这些寄存器也不会恢复。这称之为惰性FPU技术。

如果不考虑平台，浮点寄存器的内容不是保存在进程栈上，而是保存在线程数据结构中。


## 2.6 完全公平调度类

核心调度器必须知道的有关完全公平调度器的所有信息，都包含在fair_sched_class中

```C
kernel/sched_fair.c 
static const struct sched_class fair_sched_class = { 
  .next = &idle_sched_class, 
  .enqueue_task = enqueue_task_fair, 
  .dequeue_task = dequeue_task_fair, 
  .yield_task = yield_task_fair, 
  .check_preempt_curr = check_preempt_wakeup, 
  .pick_next_task = pick_next_task_fair, 
  .put_prev_task = put_prev_task_fair, 
... 
  .set_curr_task = set_curr_task_fair, 
  .task_tick = task_tick_fair, 
  .task_new = task_new_fair, 
}; 
```
。。？C 还有这个？

### 数据结构
CFS的就绪队列
知主调度器的每个就绪队列中都嵌入了一个该结构的实例

。。c是什么？ fs 应该是 fair schedule。 难道c是 complete？ 还真是。。

```C
// kernel/sched.c 
struct cfs_rq { 
  struct load_weight load; 
  unsigned long nr_running;   // 队列上可运行进程的数目
  u64 min_vruntime;   // 队列上所有进程的最小虚拟运行时间。
  struct rb_root tasks_timeline; // 用于在 按时间排序的 红黑树中管理所有进程
  struct rb_node *rb_leftmost; 
  struct sched_entity *curr; // 指向当前执行进程的可调度实体
}
```


### CFS操作

1. 虚拟时钟
所有与虚拟时钟有关的计算都在update_curr中执行，功能包括：
- 更新进程的物理运行时间和虚拟运行时间
- 对CFS队列更新min_vruntime
- 设置rq->exec_start

该函数在系统中各个不同地方调用，包括周期性调度器之内

对于运行在nice级别0的进程来说，根据定义虚拟时间和物理时间是相等的。
在使用不同的优先级时，必须根据进程的负荷权重重新衡定时间

`delta_exec_weighted = delta_exec * (NICE_0_LOAD / Curr->load.weight)`


完全公平调度器的==真正关键点==是，红黑树的排序过程是根据下列键进行的：
```C
// kernel/sched_fair.c 
static inline s64 entity_key(struct cfs_rq *cfs_rq, struct sched_entity *se) 
{ 
  return se->vruntime - cfs_rq->min_vruntime; 
} 
```

键值较小的结点，排序位置就更靠左，因此会被更快地调度。
用这种方法，内核实现了下面两种对立的机制。
(1) 在进程运行时，其vruntime稳定地增加，它在红黑树中总是向右移动的。因为越重要的进程vruntime增加越慢，因此它们向右移动的速度也越慢，这样其被调度的机会要大于次要进程，这刚好是我们需要的。
(2) 如果进程进入睡眠，则其vruntime保持不变。因为每个队列min_vruntime同时会增加（回想一下，它是单调的！），那么睡眠进程醒来后，在红黑树中的位置会更靠左，因为其键值变得更小了。

。。那前面的  fair_clock - wait_runtime  是什么东西？



---
延迟跟踪

内核有一个固有的概念，称之为良好的调度延迟，即保证每个可运行的进程都应该至少运行一次的某个时间间隔。
。。这里有注解说，这与时间片无关，旧的调度器才使用时间片！
。。但是百度下，linux 和时间片 还是有关联的。
。。估计是 时间片是固定的， 时间间隔是不固定的

它在sysctl_sched_latency给出，可通过/proc/sys/kernel/sched_latency_ns控制，默认值为20 000 000纳秒或20毫秒。
。。20ms，0.02s， 好像确实 没有感知。

第二个控制参数sched_nr_latency，控制在一个延迟周期中处理的最大活动进程数目
如果活动进程的数目超出该上限，则延迟周期也成比例地线性扩展。



### 队列操作
增删就绪队列的成员：enqueue_task_fair和dequeue_task_fair。

除了指向所述的就绪队列和task_struct的指针外，该函数还有另一个参数wakeup。
这用于指定入队的进程是最近才被唤醒并转换为运行状态（在这种情况下wakeup为1），还是此前就是可运行的（那么wakeup是0）

![8e614c5d8583a517140ebafadd5df752.png](../_resources/8e614c5d8583a517140ebafadd5df752.png)


如果进程此前在睡眠，那么在place_entity中首先会调整进程的虚拟运行时间


由于内核已经承诺在当前的延迟周期内使所有活动进程都至少运行一次，队列的min_vruntime用作基准虚拟时间，通过减去sysctl_sched_latency，则可以确保新唤醒的进程只有在当前延迟周期结束后才能运行。


### 选择下一个进程
选择下一个将要运行的进程由pick_next_task_fair执行

![0294f4b1bac211b1d63b133f3a6104cc.png](../_resources/0294f4b1bac211b1d63b133f3a6104cc.png)



### 处理周期性调度器

在处理周期调度时前述的差值很重要。形式上由函数task_tick_fair负责，但实际工作由entity_tick完成。

![d6cc67e5d138cbeb5a0809bd63a345b8.png](../_resources/d6cc67e5d138cbeb5a0809bd63a345b8.png)


### 唤醒抢占

当在try_to_wake_up和wake_up_new_task中唤醒进程时，内核使用check_preempt_curr看看是否新进程可以抢占当前运行的进程。
请注意该过程不涉及核心调度器！
对完全公平调度器处理的进程，则由check_preempt_wakeup函数执行该检测。



## 2.7 实时调度类
按照POSIX标准的强制要求，除了“普通”进程之外，Linux还支持两种实时调度类。
调度器结构使得实时进程可以平滑地集成到内核中，而无需修改核心调度器，这显然是调度类带来的好处

实时进程的特点在于其优先级比普通进程高，对应地，其static_prio值总是比普通进程低


实时进程与普通进程有一个==根本的不同之处==：如果系统中有一个实时进程且可运行，那么调度器总是会选中它运行，除非有另一个优先级更高的实时进程。


现有的两种实时类，不同之处如下所示。
- 循环进程（SCHED_RR）
  ==有时间片==，其值在进程运行时会减少，就像是普通进程。
  在==所有的时间段==都到期后，则该值重置为初始值，而进程则置于队列的末尾。
  这确保了在有几个优先级相同的SCHED_RR进程的情况下，它们总是依次执行。
- 先进先出进程（SCHED_FIFO）
  ==没有时间片==，在被调度器选择执行后，可以运行任意长时间。
  很明显，如果实时进程编写得比较差，==系统可能变得无法使用==。
  只要写一个无限循环，循环体内不进入睡眠即可。
  在编写实时应用程序时，应该多加小心



### 数据结构


### 调度器操作



## 2.8 调度器增强

### SMP调度

多处理器系统上，内核必须考虑几个额外的问题，以确保良好的调度

- CPU负荷必须尽可能公平地在所有的处理器上共享。
  如果一个处理器负责3个并发的应用程序，而另一个只能处理空闲进程，那是没有意义的。
- 进程与系统中某些处理器的亲合性（affinity）必须是可设置的。
  例如在4个CPU系统中，可以==将计算密集型应用程序绑定到前3个CPU==，而剩余的（交互式）进程则在第4个CPU上运行。
- 内核必须能够将进程==从一个CPU迁移到另一个==。
  但该选项必须谨慎使用，因为它==会严重危害性能==。
  在小型SMP系统上CPU高速缓存是最大的问题。
  对于真正大型系统，CPU与迁移进程此前使用的物理内存距离可能有若干米，因此对该进程内存的访问代价高昂。



### 调度域和控制组

调度器并不直接与进程交互，而是处理可调度实体。
这使得可以实现组调度：进程置于不同的组中，调度器首先在这些组之间保证公平，然后在组中的所有进程之间保证公平

举例来说，这使得可以向每个 用户 授予相同的CPU时间份额。

把进程按用户分组不是唯一可能的做法。内核还提供了控制组（control group），该特性使得通过特殊文件系统cgroups可以创建任意的进程集合，甚至可以分为多个层次。



### 内核抢占和低延迟相关工作

在编译内核时启用对内核抢占的支持，则可以解决这些问题。
如果高优先级进程有事情需要完成，那么在启用内核抢占的情况下，不仅用户空间应用程序可以被中断，内核也可以被中断。
切记，内核抢占和用户层进程被其他进程抢占是两个不同的概念！

内核不能在任意点上被中断。
幸运的是，大多数不能中断的点已经被SMP实现标识出来了，并且在实现内核抢占时可以重用这些信息。
内核的某些易于出现问题的部分每次只能由一个处理器访问，这些部分使用所谓的自旋锁保护：到达危险区域（亦称之为临界区）的第一个处理器会获得锁，在离开该区域时释放该锁。
另一个想要访问该区域的处理器在此期间必须等待，直到第一个处理器释放锁为止。
只有此时它才能获得锁并进入临界区。







# ch03 内存管理 107

内存管理的实现涵盖了许多领域：
- 内存中的物理内存页的管理；
- 分配大块内存的伙伴系统；
- 分配较小块内存的slab、slub和slob分配器；
- 分配非连续内存块的vmalloc机制；
- 进程的地址空间。

Linux内核一般将处理器的虚拟地址空间划分为两个部分。底部比较大的部分用于用户进程，顶部则专用于内核。
虽然（在两个用户进程之间的）上下文切换期间会改变下半部分，但虚拟地址空间的内核部分总是保持不变。

如果物理内存比可以映射到内核地址空间中的数量要多，那么内核必须借助于高端内存（highmem）方法来管理“多余的”内存。
。。32位 会有这个问题， 64不太可能。

在内核使用高端内存页之前，必须使用下文讨论的kmap和kunmap函数将其映射到内核虚拟地址空间中

有两种类型计算机，分别以不同的方法管理物理内存。
- UMA计算机(一致内存访问，uniform memory access)
  将可用内存以连续方式组织起来（可能有小的缺口）。
  SMP系统中的每个处理器访问各个内存区都是同样快。
- NUMA计算机(非一致内存访问，non-uniform memory access)
  总是多处理器计算机。
  系统的各个CPU都有本地内存，可支持特别快速的访问。
  各个处理器之间通过总线连接起来，以支持对其他CPU的本地内存的访问，当然比访问本地内存慢些。

![b6e102d3d5d2b34055da6564a2c4972b.png](../_resources/b6e102d3d5d2b34055da6564a2c4972b.png)


实际上内核会区分3种配置选项：FLATMEM、DISCONTIGMEM和SPARSEMEM。SPARSEMEM和DISCONTIGMEM实际上作用相同，但从开发者的角度看来，对应代码的质量有所不同。SPARSEMEM更激进，有性能优化，但不够稳定

。。平坦，不连续，稀疏
。。flatmen 是 UMA
。。discontigmen sparsemen 是 NUMA

在以后几节里，我们的讨论主要限于FLATMEM。
在大多数配置中都使用该内存组织类型，通常它也是内核的默认值。
由于所有内存模型实际上都使用同样的数据结构，因此不讨论其他选项也没多大损失。

NUMA计算机过于昂贵。


。。
uca，nuca，non-uniform cache access, 就是 L1,L2,L3 cache 是否是共享的，目前都是 CPU 有各自的 L1,2,3 cache


`https://zhuanlan.zhihu.com/p/387117470`

。。CPU性能=IPC(CPU每一时钟周期内所执行的指令多少)×频率(MHz时钟速度)
。。作者是 128核 服务器。。


开启NUMA会优先就近使用内存，在本NUMA上的内存不够的时候可以选择回收本地的PageCache还是到其它NUMA 上分配内存，这是可以通过Linux参数 zone_reclaim_mode 来配置的，默认是到其它NUMA上分配内存，也就是跟关闭NUMA是一样的。

这个架构距离是物理上就存在的不是你在BIOS里关闭了NUMA差异就消除了，我更愿意认为在BIOS里关掉NUMA只是掩耳盗铃。

以上理论告诉我们：也就是在开启NUMA和 zone_reclaim_mode 默认在内存不够的如果去其它NUMA上分配内存，比关闭NUMA要快很多而没有任何害处。

即使Intel在只有两个NUMA的情况下跨性能差异也有2倍，可见正确的绑核方法收益巨大，尤其是在刷榜的情况下， NUMA更多性能差异应该会更大。

实际在不开NUMA的同样CPU上，进行以上绑核测试测试，测试结果也完全一样。 测试数据说明了前面的理论分析是正确的。


事实上Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的Local内存上分配空间。如果local内存不足，优先淘汰local内存中无用的Page（Inactive，Unmapped）。然后才到其它NUMA上分配内存。
我查了2.6.32以及4.19.91内核的机器 zone_reclaim_mode 都是默认0，也就是kernel会：优先使用本NUMA上的内存，如果本NUMA不够了不要优先回收PageCache而是优先使用其它NUMA上的内存。这也是我们想要的。


总结
- 放弃对NUMA的偏见吧，优先回收 PageCache 这个Bug早已修复了
- 没必要自欺欺人关掉NUMA了，不管是X86还是ARM关闭NUMA不会带来性能提升
- 按NUMA绑定core收益巨大，即使只有两个NUMA的intel芯片，也有一倍以上的性能提升，在飞腾等其他芯片上收益更大
- MySQL这样独占物理机的服务可以做到按NUMA来绑定core，收益可观
- 云上的VM售卖如果能够精确地按NUMA绑核的话性能，超卖比能高很多
- 在刷TPCC数据的时候更应该开NUMA和正确绑核

补充下，从评论来看，最容易误解的就是：如果关闭NUMA，就变成UMA了。这里大家首先要理解NUMA的引入是CPU核越来越多，内存条数也越来越多（总内存是由很多条一起组合起来的），这样带来的问题就是一部分内存插在一部分core上，另外一些内存插在剩下的core上，从而导致了core访问不同的内存物理距离不一样，所以RT也不一样，这个物理距离是设计CPU的时候就带来了的，无法改变！

那么BIOS层面关闭NUMA的意思是什么呢？关闭后OS无法感知CPU的物理架构，也就是没有办法就近分配内存，带来的问题就是没法让性能最优，或者用户能感知到RT上的抖动（如果是2个NUMA节点的话，平均会有50%的RT偏高）
。。


。。
CPU - MMU - cache - memory
。。


本书内容集中在UMA系统，不考虑CONFIG_NUMA。



在下文的讨论中，我们会经常遇到术语 分配阶（allocation order）。
它表示内存区中页的数目取以2为底的对数。
阶0的分配由一个页面组成，阶1的分配包括2^1=2个页，阶2的分配包括2^2=4个页，依次类推


## 3.2 (N)UMA 模型中的内存组织

内核对一致和非一致内存访问系统使用相同的数据结构，因此针对各种不同形式的内存布局，各个算法几乎没有什么差别。

在UMA系统上，只使用一个NUMA结点来管理整个系统内存。而内存管理的其他部分则相信它们是在处理一个伪NUMA系统

首先，==内存划分为结点==。==每个结点关联到系统中的一个处理器==，在内核中表示为pg_data_t的实例（稍后定义该数据结构）。
各个==结点又划分为内存域==，是内存的进一步细分。

只有前16 MiB适用，还有一个高端内存区域无法直接映射。在二者之间是通用的“普通”内存区。因此一个结点最多由3个内存域组成。


各个==内存域都关联了一个数组==，用来组织属于该内存域的==物理内存页==（内核中称之为页帧）。
对每个页帧，都分配了一个struct page实例以及所需的管理数据。
各个内存结点保存在一个单链表中，供内核遍历

出于性能考虑，在为进程分配内存时，内核总是试图在当前运行的CPU相关联的NUMA结点上进行。
但这并不总是可行的，例如，该结点的内存可能已经用尽。对此类情况，每个结点都提供了一个备用列表（借助于struct zonelist）。
该列表包含了其他结点（和相关的内存域），可用于代替当前结点分配内存。
列表项的位置越靠后，就越不适合分配


---
### 数据结构

#### 结点管理
pg_data_t是用于表示结点的基本元素

```C
// <mmzone.h> 
typedef struct pglist_data { 
  struct zone node_zones[MAX_NR_ZONES]; // 包含了结点中 各内存域的数据结构
  struct zonelist node_zonelists[MAX_ZONELISTS]; // 备用结点及其 内存域的列表。当前结点没有可用空间时，从这里分配内存
  int nr_zones; // 不同内存域的数目
  struct page *node_mem_map; // 指向 page 实例数组的指针，用于描述结点的 所 有 物理内存页。
  struct bootmem_data *bdata;
  unsigned long node_start_pfn; // 该NUMA结点 第一个页帧的逻辑编号。在UMA中总是0，因为只有一个结点，所以第一个页帧总是0。
  unsigned long node_present_pages; /* 物理内存页的总数 */  // 结点中页帧的数目
  unsigned long node_spanned_pages; /* 物理内存页的总长度，包含洞在内 */ // 该结点以页帧为单位计算的长度， 不一定等于 node_present_pages，因为 结点中 可能有一些空洞，并不对应真正的页帧。
  int node_id; // 全局结点ID，NUMA结点从0开始编号
  struct pglist_data *pgdat_next; // 下一个内存结点，系统中所有结点都通过 单链表连接
  wait_queue_head_t kswapd_wait; // 交换守护进程 (swap daemon) 的等待队列，将页帧换出结点时会用到
  struct task_struct *kswapd; // 指向负责该结点的交换守护进程的 task_struct
  int kswapd_max_order; // 用于 页交换子系统的实现，用来定义需要释放的区域的长度。
} pg_data_t; 
```

结点的内存域保存在`node_zones[MAX_NR_ZONES]`。该数组总是有3个项，即使结点没有那么多内存域，也是如此。如果不足3个，则其余的数组项用0填充。


---
结点状态管理
如果系统中结点多于一个，内核会维护一个位图，用以提供各个结点的状态信息。状态是用位掩码指定的，可使用下列值
```C
// nodemask.h
enum node_states {
  N_POSSIBLE, /* 结点在某个时候可能变为联机 */ 
  N_ONLINE, /* 结点是联机的 */ 
  N_NORMAL_MEMORY, /* 结点有普通内存域 */ 

#ifdef CONFIG_HIGHMEM 
  N_HIGH_MEMORY, /* 结点有普通或高端内存域 */ 
#else 
  N_HIGH_MEMORY = N_NORMAL_MEMORY, 
#endif 

  N_CPU, /* 结点有一个或多个CPU */ 
  NR_NODE_STATES 
};
```
N_POSSIBLE、N_ONLINE和N_CPU用于CPU和内存的热插拔

对内存管理有必要的标志是N_HIGH_MEMORY和N_NORMAL_MEMORY。
如果结点有普通或高端内存则使用N_HIGH_MEMORY，仅当结点没有高端内存才设置N_NORMAL_MEMORY。



---
#### 内存域
内核使用zone结构来描述内存域

`mmzone.h`
`struct zone {};`

该结构比较特殊的方面是它由ZONE_PADDING分隔为几个部分。这是因为对zone结构的访问非常频繁
如果数据保存在CPU高速缓存中，那么会处理得更快速。
高速缓存分为行，每一行负责不同的内存区。
内核使用ZONE_PADDING宏生成“填充”字段添加到结构中，以确保每个自旋锁都处于自身的缓存行中。
还使用了编译器关键字__cacheline_maxaligned_in_smp，用以实现最优的高速缓存对齐方式


---
内存域水印的计算

---
#### 冷热页

struct zone的 pageset 成员用于 实现 冷热分配器

内核说页是热的，意味着页已经加载到CPU高速缓存


#### 页帧
页帧代表系统内存的最小单位，对内存中的每个页都会创建struct page的一个实例。
内核程序员需要注意保持该结构尽可能小，因为即使在中等程度的内存配置下，系统的内存同样会分解为大量的页。

页的广泛使用，增加了保持结构长度的难度：内存管理的许多部分都使用页，用于各种不同的用途。

C语言的联合很适合于该问题，尽管它未能增加struct page的清晰程度。

考虑一个例子：一个物理内存页能够通过多个地方的不同页表映射到虚拟地址空间，内核想要跟踪有多少地方映射了该页。
为此，struct page中有一个==计数器用于计算映射的数目==。
如果一页用于slub分配器（将整页细分为更小部分的一种方法，请参见3.6.1节），那么可以确保==只有内核会使用该页==，而不会有其他地方使用，
因此映射计数信息就是==多余==的。
因此内核可以重新解释该字段，用来表示该页被细分为多少个小的内存对象使用。
在数据结构定义中，这种双重解释如下所示

```C
// <mm_types.h> 
struct page { 
... 
  union { 
    atomic_t _mapcount;  /* 内存管理子系统中映射的页表项计数，
                          * 用于表示页是否已经映射，还用于限制逆向映射搜索。
                          */ 
    unsigned int inuse; /* 用于SLUB分配器：对象的数目 */ 
  }; 
... 
} 
```

`struct list_head lru; /* 换出页列表，例如由zone->lru_lock保护的active_list! */ `
。。lru


## 3.3 页表

层次化的页表用于支持对大地址空间的快速、高效的管理

内核内存管理总是假定使用四级页表，而不管底层处理器是否如此。

`page.h, pgtable.h`


### 数据结构

内存管理更喜欢使用unsigned long类型的变量，而不是void指针，因为前者更易于处理和操作。技术上，它们都是有效的。

根据四级页表结构的需要，虚拟内存地址分为5部分（4个表项用于选择页，1个索引表示页内位置）。

各个体系结构不仅 ==地址字长度不同==，而且地址字==拆分的方式也不同==。因此内核定义了宏，用于将地址分解为各个分量。

BITS_PER_LONG定义用于unsigned long变量的比特位数目，因而也适用于指向虚拟地址空间的通用指针。

![e2217164b3ff382b581de36a86cc6d2d.png](../_resources/e2217164b3ff382b581de36a86cc6d2d.png)


内核提供了4个数据结构（定义在page.h中）来表示页表项的结构。
- pgd_t用于全局页目录项。
- pud_t用于上层页目录项。
- pmd_t用于中间页目录项。
- pte_t用于直接页表项。

PAGE_ALIGN是另一个每种体系结构都必须定义的标准宏（通常在page.h中）
它需要一个地址作为参数，并将该地址“舍入”到下一页的起始处
如果页大小是4 096，该宏总是返回其倍数。PAGE_ALIGN(6000)=8192 = 2× 4096, PAGE_ALIGN(0x84590860)=0x84591000 = 542097 × 4096

尽管使用了C结构来表示页表项，但大多数页表项都只有一个成员，通常是unsigned long类型，以AMD64体系结构为例:
```C
// include/asm-x86_64/page.h
typedef struct { unsigned long pte; } pte_t; 
typedef struct { unsigned long pmd; } pmd_t; 
typedef struct { unsigned long pud; } pud_t; 
typedef struct { unsigned long pgd; } pgd_t 
```

特定于PTE的信息
最后一级页表中的项不仅包含了指向页的内存位置的指针，还在上述的多余比特位包含了与页有关的附加信息。


## 3.4 初始化内存管理

在内存管理的上下文中，初始化（initialization）可以有多种含义。
在许多CPU上，必须显式设置适于Linux内核的内存模型。


---

对相关数据结构的初始化是从全局启动例程start_kernel中开始的，该例程在加载内核并激活各个子系统之后执行。



#### 内核在内存中的布局
p136

物理内存最低几兆字节的布局
![513e264e437c4dae77639d2d50b94079.png](../_resources/513e264e437c4dae77639d2d50b94079.png)

0x9e800 = 649216 = 634k

该图给出了物理内存的前几兆字节，==具体的长度依赖于内核二进制文件的长度==。
前4 KiB是第一个页帧，一般会忽略，因为通常保留==给BIOS使用==。
接下来的640 KiB原则上是可用的，但也==不用==于内核加载。其==原因==是，该区域之后紧邻的区域由系统保留，用于映射各种ROM（通常是系统BIOS和显卡ROM）。不可能向映射ROM的区域写入数据。但内核总是会装载到一个连续的内存区中，如果要从4 KiB处作为起始位置来装载内核映像，则要求内核必须小于640 KiB。

。。但是 图中 0x9e800 是 634k，在 640k之前啊。 
。。根据 文字，ROM的开始应该是 644k (4+640)，是 0xA1000。 我感觉这个 数字好一点。 应该是 图错了。
。。百度了下，没有具体的， 只有 linux 0.11 的时候 是 0-640k

IA-32内核使用==0x100000==作为起始地址。这对应于内存中==第二兆==字节的开始处。从此处开始，有足够的连续内存区，可容纳整个内核。

内核占据的内存分为几个段，其边界保存在变量中。
- _text和_etext是==代码段==的起始和结束地址，包含了编译后的内核代码。
- ==数据段==位于_etext和_edata之间，保存了大部分内核变量。
- ==初始化数据==在内核==启动过程结束后不再需要==（例如，包含初始化为0的所有静态全局变量的BSS段）保存在最后一段，从_edata到_end。在内核初始化完成后，其中的大部分数据都可以从内存删除，给应用程序留出更多空间。这一段内存区划分为更小的子区间，以控制哪些可以删除，哪些不能删除，但这对于我们现在的讨论没多大意义


准确的数值依内核配置而异，因为每种配置的代码段和数据段长度都不相同，这取决于启用和禁用了内核的哪些部分。只有起始地址（_text）总是相同的

每次编译内核时，都生成一个文件System.map并保存在源代码目录下。除了所有其他（全局）变量、内核定义的函数和例程的地址，该文件还包括图3-11给出的常数的值

```shell
wolfgang@meitner> cat /proc/iomem 
00000000-0009e7ff : System RAM 
0009e800-0009ffff : reserved 
000a0000-000bffff : Video RAM area 
000c0000-000c7fff : Video ROM 
000f0000-000fffff : System ROM 
00100000-17ceffff : System RAM 
00100000-00381ecc : Kernel code 
00381ecd-004704df : Kernel data
```
。。0xa000 == 640k


在AMD64系统上也可以获得类似的信息。这里内核在第一个页帧之后2 MiB开始，物理内存映射到虚拟地址空间中从0xffffffff80000000开始。
```shell
wolfgang@meitner> cat System.map 
ffffffff80200000 A _text 
... 
ffffffff8041fc6f A _etext 
... 
ffffffff8056c060 A _edata 
... 
ffffffff8077548c A _end 
```

在运行时，也可以从/proc/iomem获得内核的相关信息：
```shell
root@meitner # cat/proc/iomem 
... 
00100000-cff7ffff : System RAM 
00200000-0041fc6e : Kernel code 
0041fc6f-0056c05f : Kernel data 
006b6000-0077548b : Kernel bss 
... 
```



![14365b26615f401dcdb1a341e5e92864.png](../_resources/14365b26615f401dcdb1a341e5e92864.png)


---

![a7d42982befa7219e7ac4a7cd70fef9d.png](../_resources/a7d42982befa7219e7ac4a7cd70fef9d.png)


---

在IA-32系统上内核通常将总的4 GiB可用虚拟地址空间按3 : 1的比例划分。==低端==3 GiB用于用户状态应用程序，而==高端==的1GiB则专用于内核。


。。百度了下， 高端内存是指 大于等于 0xc0000000 的内存， 用于 内核， 但是 这是 逻辑地址， 映射到 物理地址的时候 要 减去 0c0000000， 所以 实际上 物理地址从 0开始
。 这1g 的内存，分为 3块，ZONE_DMA 开始的16mb，ZONE_NORMAL 是16-896mb，ZONE_HIGHMEM (高端内存) 是 896mb - 结束
。高端内存 (指 896mb之后的，而不是 高端的1g ) 是用来 映射到其他内存的， 以扩展 内核的 1g 的容量 ( 可以访问所有的物理内存)。

。。不知道 用户空间的 怎么 映射到 物理内存。 
。。而且好多都是 32位的， 天天4g = 3g + 1g 。


这些划分主要的动机如下所示
- 在用户应用程序的执行切换到核心态时（这总是会发生，例如在使用系统调用或发生周期性的时钟中断时），内核必须装载在一个可靠的环境中。因此有必要将地址空间的一部分分配给内核专用。
- 物理内存页则映射到内核地址空间的==起始处==，以便内核直接访问，而无需复杂的页表操作。

---

按3∶1的比例划分地址空间，只是约略反映了内核中的情况，内核地址空间自身又分为各个段。图3-15对此给出了图示
![46f2d9f6890e709b1912c86cec61d420.png](../_resources/46f2d9f6890e709b1912c86cec61d420.png)


。。IA-32（Intel Architecture 32-bit，英特尔32位体系架构），属于X86体系结构的32位版本

该图给出了用来管理虚拟地址空间的第四吉字节的页表项的结构。它标明了虚拟地址空间的各个区域的用途，这与物理内存的分配无关。
。。第4个g的，就是 内核的 内存。  吉==g 。。吉字节 还真有，真是 gb

地址空间的第一段用于将系统的所有物理内存页映射到内核的虚拟地址空间中。
由于==内核地址==空间==从偏移量0xC0000000==开始，即经常提到的3 GiB，每个==虚拟地址x==都对应于==物理地址x—0xC0000000==，因此这是一个简单的线性平移。

如果物理内存超过896 MiB，则内核无法直接映射全部物理内存。
内核必须保留地址空间最后的128 MiB用于其他目的

将这128 MiB加上直接映射的896 MiB内存，则得到内核虚拟地址空间的总数为1 024 MiB
内核使用两个经常使用的缩写normal和highmem，来区分==是否可以直接映射的页帧==。

内核移植的每个体系结构都必须提供两个宏，用于一致映射的内核虚拟内存部分，进行物理和虚拟地址之间的转换（最终这是一个平台相关的任务）。
- __pa(vaddr)返回与虚拟地址vaddr相关的物理地址。
- __va(paddr)则计算出对应于物理地址paddr的虚拟地址。

IA-32将页帧映射到从PAGE_OFFSET开始的虚拟地址空间
```C
// include/asm-x86/page_32.h 
#define __pa(x) ((unsigned long)(x)-PAGE_OFFSET) 
#define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET)) 
```

。。
在6.9-rc1中 /include/asm-generic/page.h 中
这2个宏 和 书上的不一样。 但是 有 virt_to_pfn 这2个方法，不知道用来干什么。不知道 IA32 有没有。

PAGE_SHIFT 是 16

而且 
`#define PAGE_OFFSET		(0)`  书上的 PAGE_OFFSET 在 6.9中是 0 了。。

```C
#define __va(x) ((void *)((unsigned long) (x)))
#define __pa(x) ((unsigned long) (x))

static inline unsigned long virt_to_pfn(const void *kaddr)
{
	return __pa(kaddr) >> PAGE_SHIFT;
}
#define virt_to_pfn virt_to_pfn
static inline void *pfn_to_virt(unsigned long pfn)
{
	return __va(pfn) << PAGE_SHIFT;
}
```
。。


内核地址空间的最后128 MiB用于何种用途呢？如图3-15所示，该部分有3个用途。
- 虚拟内存中连续、但物理内存中不连续的内存区，可以在==vmalloc区==域分配。该机制通常用于用户过程，内核自身会试图尽力避免非连续的物理地址。内核通常会成功，因为大部分大的内存块都在启动时分配给内核，那时内存的碎片尚不严重。但在已经运行了很长时间的系统上，在内核需要物理内存时，就可能出现可用空间不连续的情况。此类情况，主要出现在动态加载模块时。
- ==持久映射==用于将高端内存域中的非持久页映射到内核中。3.5.8节将仔细讨论该主题
- ==固定映射==是与物理地址空间中的固定页关联的虚拟地址空间项，但具体关联的页帧可以自由选择。它与通过固定公式与物理内存关联的直接映射页相反，虚拟固定映射地址与物理内存位置之间的关联可以自行定义，关联建立后内核总是会注意到的

在这里有两个预处理器符号很重要：__VMALLOC_RESERVE 设置了 vmalloc区域的长度，而 MAXMEM 则表示内核可以直接寻址的物理内存的最大可能数量。

max_low_pfn 指定了物理内存数量小于896 MiB的系统上内存页的数目。该值的上界受限于896 MiB可容纳的最大页数（具体的计算在find_max_low_pfn给出）。如果启用了高端内存支持，则high_memory 表示两个内存区之间的边界，总是896 MiB。

如果VMALLOC_OFFSET取最小值，那么在直接映射的所有内存页和用于非连续分配的区域之间，会出现一个缺口。

这个缺口可用作针对任何内核故障的==保护措施==。如果访问越界地址（即无意地访问物理上不存在的内存区），则访问失败并生成一个异常，报告该错误。

在某些场合可能最好将地址空间对称划分，2 GiB用于用户地址空间，2 GiB用于内核地址空间。
那么 `__PAGE_OFFSET` 必须设置为0x80000000，而不是通常的默认值0xC0000000。
如果系统执行的任务需要将==大量内存用于内核==，而几乎==没有多少内存用于用户进程==（这样的任务很少见），对称划分可能比较有用。


- 对超大内存页的支持。
- 如有可能，内核页会设置另一个属性（__PAGE_GLOBAL），这也是__PAGE_KERNEL和__PAGE_KERNEL_EXEC变量中__PAGE_GLOBAL比特位已经置位的原因。

在上下文切换期间，设置了__PAGE_GLOBAL位的页，对应的TLB缓存项不从TLB刷出。


> 冷热缓存的初始化

上述代码计算得到的batch，大约相当于内存域中页数的0.25‰

对热页来说，下限为0，上限为`6*batch`，缓存中页的平均数量大约是`4*batch`，因为内核不会让缓存水平降到太低。
`batch * 4`相当于内存域中页数的千分之一（这也是zone_batchsize试图将批量大小优化到总页数0.25‰的原因）。

根据经验，缓存大小是主内存的千分之一。
考虑到当前系统每个CPU配备的物理内存大约在1 GiB～2 GiB，该规则是有意义的。
这样，计算出的批量大小使得冷热缓存中的页有可能==放置到CPU的L2缓存==中。

---

> AMD64地址空间的设置

AMD64系统地址空间的设置在某些方面比IA-32容易，但在另一些方面要困难。
虽然64位地址空间避免了古怪的高端内存域，但有另一个因素使情况复杂化。
64位地址空间的跨度太大，当前没有什么应用程序需要这个。
因此，当前只实现了一个比较小的物理地址空间，==地址字宽度为48位==。
这在不失灵活性的前提下，简化并加速了地址转换。
48位宽的地址字可以寻址256 TiB的地址空间，或256×1024 GiB，即使对Firefox也足够了！

尽管物理地址字位宽被限制在48位，但在寻址虚拟地址空间时仍然使用了64位指针，因而虚拟地址空间在形式上仍然会跨越264字节。
但这引起了一个==问题==：由于物理地址字实际上只有48位宽，虚拟地址空间的某些部分无法寻址。

---

![d35dd217e8fd8f3030091531da3da0a4.png](../_resources/d35dd217e8fd8f3030091531da3da0a4.png)


可访问的地址空间的整个下半部用作用户空间，而整个上半部专用于内核。
由于两个空间都极大，无须调整划分比例之类的参数。


### 3.4.3 启动过程期间的内存管理

在启动过程期间，尽管内存管理尚未初始化，但内核仍然需要分配内存以创建各种数据结构。

bootmem分配器用于在启动阶段早期分配内存。

显然，对该分配器的需求集中于简单性方面，而不是性能和通用性。因此内核开发者决定实现一个==最先适配（first-fit）分配器==用于在启动阶段管理内存，这是可能想到的最简单方式。
在需要分配内存时，分配器逐位扫描位图，直至找到一个能提供足够连续页的位置，即所谓的最先最佳（first-best）或最先适配位置。

该过程不是很高效，因为每次分配都必须从头扫描比特链。
因此在内核完全初始化之后，不能将该分配器用于内存管理。
伙伴系统（连同slab、slub或slob分配器）是一个好得多的备选方案，将在3.5.5节讨论。


---

内核（为系统中的每个结点都）提供了一个`bootmem_data`结构的实例，用于该用途

bootmem分配器的初始化是一个特定于体系结构的过程，此外还取决于所述计算机的内存布局。


![098739742df57caf0c9b515523036b8f.png](../_resources/098739742df57caf0c9b515523036b8f.png)


---

![8b96cd15f7d9aea3f882259a251396dc.png](../_resources/8b96cd15f7d9aea3f882259a251396dc.png)



## 3.5 物理内存的管理

在内核初始化完成后，内存管理的责任由==伙伴系统==承担。
伙伴系统基于一种相对简单然而令人吃惊的强大算法，已经伴随我们几乎40年。它结合了==优秀内存分配器的两个关键特征：速度和效率==。

。。自定义内存池 岂不是也可以这样？
。。或者 直接 使用 kernel 的 方法？


### 3.5.1 伙伴系统的结构

系统内存中的每个物理内存页（页帧），都对应于一个struct page实例。
每个内存域都关联了一个struct zone的实例，其中保存了用于管理伙伴数据的主要数组。

```C
// <mmzone.h> 
struct zone { 
... 
  /* 
  * 不同长度的空闲区域
  */ 
  struct free_area free_area[MAX_ORDER]; 
... 
}; 

struct free_area { 
  struct list_head free_list[MIGRATE_TYPES]; // 空闲页 的链表
  unsigned long nr_free;  // 当前内存区的空闲页块的数目
};
```

#### 阶

阶 描述了内存分配的数量单位。 内存块的长度是 2^阶。

```C
// <mmzone.h> 
#ifndef CONFIG_FORCE_MAX_ZONEORDER 
#define MAX_ORDER 11 
#else 
#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER 
#endif 
#define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER -1)) 
```

该常数 通常 设置为11，这意味着一次分配可以请求的页数最大是 `2^11=2048` 。
值可以手工改变

。。我翻了几个 mmzone.h ， 没有找到 这个 MAX_ORDER 。。。

free_area[]数组中各个元素的索引也解释为阶，用于指定对应链表中的连续内存区包含多少个页帧。
第0个链表包含的内存区为单页，第1个链表管理的内存区为两页，第3个管理的内存区为4页，依次类推。

内存区是如何连接的？
内存区中第1页内的链表元素，可用于将内存区维持在链表中。
因此，也不必引入新的数据结构来管理物理上连续的页，否则这些页不可能在同一内存区中。
图3-22对此给出了图示。

![733c0f9d4e5781cb0db8e9368597023b.png](../_resources/733c0f9d4e5781cb0db8e9368597023b.png)


伙伴==不必是彼此连接==的。
如果一个内存区在分配其间分解为两半，内核会自动将未用的一半加入到对应的链表中。
如果在未来的某个时刻，由于内存释放的缘故，两个内存区都处于空闲状态，可==通过其地址判断其是否为伙伴==。
管理工作较少，是伙伴系统的一个主要优点。

。。这里分裂后，是 从当前链表删除，然后放入 下一级阶 的链表中，还是，当前结点置为 已分裂，然后放入 下一阶中？
。。主要是 都空闲的时候，合并后的状态的问题，
。。如果是 第一种，那么 可能导致 上一阶 中 链表 不是 两两成对的。 比如，原先有 1,1,2,2,3,3, 有6个小的内存请求，导致分裂了2,3,3, 所以现在 剩下 1,1,2， 分裂后的 一个3的 内存回收了， 就变成了 1,1,2,3 这种 就 不是成对的了。 回收 就必须 遍历整个链表了。

基于伙伴系统的内存管理专注于某个结点的某个内存域，例如，DMA或高端内存域。
但所有内存域和结点的伙伴系统都通过备用分配列表连接起来。

![abca9f9e2267bcc1f50982521459a586.png](../_resources/abca9f9e2267bcc1f50982521459a586.png)

在首选的内存域或节点无法满足内存分配请求时，首先尝试同一结点的另一个内存域，接下来再尝试另一个结点，直至满足请求。


### 避免碎片

大多数现代CPU都提供了使用==巨型页==的可能性，比普通页大得多。
这对内存使用密集的应用程序有好处。
在使用更大的页时，地址转换后备缓冲器只需处理较少的项，降低了TLB缓存失效的可能性。但分配巨型页需要连续的空闲物理内存！


文件系统也有碎片，该领域的碎片问题主要通过碎片合并工具解决。
它们分析文件系统，重新排序已分配存储块，从而建立较大的连续存储区。
理论上，该方法对物理内存也是可能的，但由于==许多物理内存页不能移动到任意位置==，阻碍了该方法的实施。

内核的方法是反碎片（anti-fragmentation），即==试图从最初开始尽可能防止碎片==。

反碎片的工作原理如何？为理解该方法，我们必须知道内核将已分配页划分为下面3种不同类型。
- 不可移动页
  在内存中有固定位置，不能移动到其他地方
  核心内核分配的大多数内存属于该类别
- 可回收页
  不能直接移动，但可以删除，其内容可以从某些源==重新生成==。
  例如，映射自文件的数据属于该类别。
  kswapd守护进程会根据可回收页访问的频繁程度，==周期性释放此类内存==。
- 可移动页
  可以随意地移动
  属于用户空间应用程序的页属于该类别。
  它们是通过页表映射的。
  如果它们复制到新位置，页表项可以相应地更新，应用程序不会注意到任何事。


nr_free 统计了所有列表上空闲页的数目，而每种迁移类型都对应于一个空闲列表。
宏for_each_migratetype_order(order, type)可用于迭代指定迁移类型的所有分配阶。


> 全局变量和辅助函数

尽管页可移动性分组特性总是编译到内核中，但只有在系统中有足够内存可以分配到多个迁移类型对应的链表时，才是有意义的。
由于每个迁移链表都应该有适当数量的内存，内核需要定义“适当”的概念。
这是通过两个全局变量pageblock_order和pageblock_nr_pages提供的。
第一个表示内核认为是“大”的一个分配阶，pageblock_nr_pages则表示该分配阶对应的页数。

如果体系结构提供了巨型页机制，则pageblock_order通常定义为巨型页对应的分配阶：
```C
// <pageblock-flags.h>
#define pageblock_order HUGETLB_PAGE_ORDER
```
。。这个有 /include/linux/pageblock-flags.h


内核如何知道给定的分配内存属于何种迁移类型？读者在3.5.4节会看到，有关各个内存分配的细节都通过==分配掩码==指定。
内核提供了两个标志，分别用于表示分配的内存是可移动的（__GFP_MOVABLE）或可回收的（__GFP_RECLAIMABLE）。


---

在内存子系统初始化期间，memmap_init_zone负责处理内存域的page实例。
该函数完成了一些不怎么有趣的标准初始化工作，但其中有一件是实质性的，即==所有==的页==最初==都标记为==可移动==的


在分配内存时，如果必须“盗取”不同于预定迁移类型的内存区，内核在策略上倾向于==“盗取”更大的==内存区。
由于所有页最初都是可移动的，那么在内核==分配不可移动==的内存区时，则必须“盗取”
。。更大是 向上取整，2倍，4倍？


---

虚拟可移动内存域

依据可移动性组织页是防止物理内存碎片的一种可能方法，内核还提供了==另一种阻止==该问题的手段：虚拟内存域ZONE_MOVABLE。

与可移动性分组相反，ZONE_MOVABLE特性必须由管理员显式激活

基本思想很简单：可用的物理内存划分为两个内存域，一个==用于可移动分配==，一个==用于不可移动分配==。这会自动防止不可移动页向可移动内存域引入碎片。

另一个问题：内核如何在两个==竞争的内存域==之间分配可用的内存？这显然对内核要求太高，因此系统管理员必须作出决定。



### 初始化内存域和结点数据结构

体系结构相关代码需要在启动期间建立以下信息：
- 系统中各个内存域的页帧边界，保存在max_zone_pfn数组；
- 各结点页帧的分配情况，保存在全局变量early_node_map中。


### 分配器API

就伙伴系统的接口而言，NUMA或UMA体系结构是没有差别的，二者的调用语法都是相同的。
==所有函数==的一个==共同点==是：只能分配==2的整数幂个页==。
因此，接口中不像C标准库的malloc函数或bootmem分配器那样指定了所需内存大小作为参数。
相反，必须指定的是==分配阶==，伙伴系统将在内存中分配2^order页。
内核中==细粒度的分配==只能借助于==slab分配器==（或者slub、slob分配器），后者基于伙伴系统（更多细节在3.6节给出）。


在空闲内存无法满足请求以至于分配失败的情况下，所有上述函数都返回空指针（alloc_pages和alloc_page）或者0（get_zeroed_page、__get_free_pages和__get_free_page）。


内核除了伙伴系统函数之外，还提供了其他内存管理函数。它们以伙伴系统为基础，但并不属于伙伴分配器自身。
这些函数包括vmalloc和vmalloc_32，使用页表将==不连续的内存==映射到==内核地址空间==中，==使之看上去是连续的==。
还有一组kmalloc类型的函数，用于分配==小于一整页==的内存区。


内核提供了所谓的内存域修饰符（zone modifier）（在掩码的最低4个比特位定义），来指定从哪个内存域分配所需的页。


### 分配页

所有API函数都追溯到alloc_pages_node，从某种意义上说，该函数是伙伴系统主要实现的“发射台”。

gfp.h
```C
static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,  unsigned int order) 
{ 
  if (unlikely(order >= MAX_ORDER)) 
    return NULL; 
  /* 未知结点即当前结点 */
  if(nid< 0) 
    nid = numa_node_id(); 
  return __alloc_pages(gfp_mask, order, NODE_DATA(nid)->node_zonelists + gfp_zone(gfp_mask)); 
} 
```


只执行了一个简单的检查，避免分配过大的内存块。
如果指定负的结点ID（不存在），内核自动地使用当前执行CPU对应的结点ID。
接下来的工作委托给__alloc_pages，只需传递一组适当的参数。
请注意，gfp_zone用于选择分配内存的内存域。这是个容易遗漏的重要细节！

内核源代码将__alloc_pages称之为“伙伴系统的心脏”，因为它处理的是实质性的内存分配。

> 选择页


默认情况下（即没有因其他因素带来的压力而需要更多的内存），只有内存域包含页的数目至少为zone->pages_high时，才能分配页。这对应于ALLOC_WMARK_HIGH标志。
如果要使用较低（zone->pages_low）或最低（zone->pages_min）设置，则必须相应地设置ALLOC_WMARK_MIN或ALLOC_WMARK_LOW。
ALLOC_HARDER通知伙伴系统在==急需内存时放宽分配规则==。在分配高端内存域的内存时，ALLOC_HIGH进一步放宽限制。
最后，ALLOC_CPUSET告知内核，内存只能从当前进程允许运行的CPU相关联的内存结点分配，当然该选项只对NUMA系统有意义。

。。ALLOC_HIGH，ALLOC_HARDER，就是 将 预期 /2, /4， 所以最大 /8，就是 只分配 1/8 原定目标的 内存。


> 移除选择的页

如果内核找到适当的内存域，具有足够的空闲页可供分配，那么还有两件事情需要完成。
首先它必须检查这些页是否是连续的（到目前为止，只知道有许多空闲页）。
其次，必须按伙伴系统的方式从free_lists移除这些页，这可能需要分解并重排内存区。

如果只分配一页，内核会进行优化，即分配==阶为0==的情形，2^0 = 1。
该页不是从伙伴系统直接取得，而是==取自per-CPU的页缓存==（回想一下，可知该缓存提供了CPU本地的热页和冷页的列表，所需数据结构在3.2.2节讲过）


内核会遍历per-CPU缓存中的所有页，检查是否有指定迁移类型的页可用。
如果前一次调用中，用不同迁移类型的页重新填充了缓存，就可能找不到。
如果无法找到适当的页，则向缓存添加一些符合当前要求迁移类型的页，然后从per-CPU列表移除一页，接下来进一步处理。

如果需要分配多页（由else分支处理），内核调用__rmqueue会从内存域的伙伴列表中选择适当的内存块。
如有必要，该函数会自动分解大块内存，将未用的部分放回列表中（具体过程将在下文讲解）。
切记，可能有这样的情况：内存域中有足够空闲页满足分配请求，但==页不是连续==的。在这种情况下，__rmqueue==失败并返回NULL==指针。

在返回指针之前，prep_new_page需要做一些准备工作，以便内核能够处理这些页
prep_new_page对页进行几项检查，确保分配之后分配器处于理想状态。
特别地，这意味着在现存的映射中不能使用该页，也没有设置不正确的标志（如PG_locked或PG_buddy），因为这说明页处于使用中，不应该放置在空闲列表上。
但通常情况下，不应该发生错误，否则就意味着内核在其他地方出现了错误


如果设置了__GFP_COMP并请求了多个页，内核必须将这些页组成复合页（compound page）。第一个页称作首页（head page），而所有其余各页称作尾页（tail page）。

内核可能合并多个相邻的物理内存页，形成所谓的巨型TLB页。
在用户层应用程序处理大块数据时，许多处理器允许使用巨型TLB页，将数据保存在内存中。
由于巨型TLB页比普通页大，这降低了保存在地址转换后备缓冲器（TLB）中的信息的数量，因而又降低了TLB缓存失效的概率，从而==加速了内存访问==


> __rmqueue辅助函数

内核使用了__rmqueue函数（前面讲过），该函数充当进入伙伴系统核心的看门人




### 3.5.6 释放页

__free_pages是一个基础函数，用于实现内核API中所有涉及内存释放的函数

首先判断所需释放的内存是单页还是较大的内存块？
如果释放==单页==，则不还给伙伴系统，而是==置于per-CPU缓存==中，对很可能出现在CPU高速缓存的页，则放置到热页的列表中。
出于该目的，内核提供了free_hot_page辅助函数，该函数只是作一下参数转换，接下来调用free_hot_ cold_page

如果free_hot_cold_page判断per-CPU缓存中页的数目超出了pcp->count，则将数量为pcp->batch的一批内存页还给伙伴系统。
该策略称之为惰性合并（lazy coalescing）。
如果单页直接返回给伙伴系统，那么会发生合并，而为了满足后来的分配请求又需要进行拆分。
因而==惰性合并策略阻止了大量可能白费时间的合并操作==。free_pages_bulk用于将页还给伙伴系统。




### 内核中不连续页的分配

我们知道物理上连续的映射对内核是最好的，但并不总能成功地使用。

在分配一大块内存时，可能竭尽全力也无法找到连续的内存块。在用户空间中这不是问题，因为普通进程设计为使用处理器的分页机制，当然这会降低速度并占用TLB。

在内核中也可以使用同样的技术。在3.4.2节讨论过，内核分配了其虚拟地址空间的一部分，用于建立连续映射

在IA-32系统中，紧随直接映射的前892 MiB物理内存，在插入的8 MiB安全隙之后，是一个用于管理不连续内存的区域。这一段具有线性地址空间的所有性质。
==分配到其中的页可能位于物理内存中的任何地方==。通过修改负责该区域的内核页表，即可做到这一点

![dffd98c7a17e947b23adc571abac028b.png](../_resources/dffd98c7a17e947b23adc571abac028b.png)


每个vmalloc分配的子区域都是自包含的，与其他vmalloc子区域通过一个内存页分隔。
类似于直接映射和vmalloc区域之间的边界，不同vmalloc子区域之间的分隔也是为防止不正确的内存访问操作。
这种情况只会因为内核故障而出现，应该通过系统错误信息报告，而不是允许内核其他部分的数据被暗中修改。
因为分隔是在虚拟地址空间中建立的，不会浪费宝贵的物理内存页。

vmalloc是一个接口函数，内核代码使用它来分配在==虚拟内存中连续==但在==物理内存中不一定连续的内存==
```C
// <vmalloc.h> 
void *vmalloc(unsigned long size);  // 字节数，不是页数
```

使用vmalloc的最著名的实例是内核对模块的实现。
因为模块可能在任何时候加载，如果模块数据比较多，那么无法保证有足够的连续内存可用，特别是在系统已经运行了比较长时间的情况下。
如果能够用小块内存拼接出足够的内存，那么使用vmalloc可以规避该问题


因为用于vmalloc的内存页总是必须映射在内核地址空间中，因此使用ZONE_HIGHMEM内存域的页要优于其他内存域。这使得内核可以节省更宝贵的较低端内存域


---

> 创建vm_area

在创建一个新的虚拟内存区之前，必须找到一个适当的位置。
vm_area实例组成的一个链表，管理着vmalloc区域中已经建立的各个子区域。
定义在mm/vmalloc的全局变量vmlist是表头。

由于各个vmalloc子区域之间需要插入1页（==警戒页）作为安全隙==，内核首先适当提高需要分配的内存长度。

内核调用map_vm_area将分散的物理内存页连续映射到虚拟的vmalloc区域

---

除了vmalloc之外，还有其他方法可以创建虚拟连续映射。
- vmalloc_32，和vmalloc一样，但保证所使用的物理内存总是可以用 32位指针寻址的。
- vmap，使用一个page数组作为起点，来创建虚拟连续内存区。该函数所用的物理内存位置不是隐式分配的，而需要先行分配好，作为参数传递
- ioremap，特定于处理器的函数，必须在所有体系结构上实现。它可以将取自物理地址空间、由系统总线用于I/O操作的一个内存块，映射到内核的地址空间中


#### 释放内存

vfree 用于释放 放vmalloc和vmalloc_32分配的区域
vunmap用于释放由vmap或ioremap创建的映射



### 3.5.8 内核映射

尽管vmalloc函数族可用于从高端内存域向内核映射页帧（这些在内核空间中通常是无法直接看到的），但这并不是这些函数的实际用途。
重要的是强调以下事实：内核提供了其他函数用于将ZONE_HIGHMEM页帧显式映射到内核空间，这些函数与vmalloc机制无关。因此，这就造成了混乱。

---

如果需要将高端页帧长期映射（作为持久映射）到内核地址空间中，==必须使用kmap==函数

如果没有启用高端支持，该函数的任务就比较简单。
在这种情况下，所有页都可以直接访问，因此只需要返回页的地址，无需显式创建一个映射。

如果确实存在高端页，情况会比较复杂。类似于vmalloc，内核首先必须建立高端页和所映射到的地址之间的关联。
还必须在虚拟地址空间中分配一个区域以映射页帧，最后，内核必须记录该虚拟区域的哪些部分在使用中，哪些仍然是空闲的

用kmap映射的页，如果不再需要，必须用kunmap解除映射


许多体系结构==不支持高端内存==，因为不需要该特性，==64位体系结构就是如此==。
。。艹



## 3.6 slab分配器

内核也必须经常分配内存，但是无法借助于 标准库的函数(如 malloc)。

上面的伙伴系统 支持按页分配内存，但是 这个单位太大了。

为了最小化 新的内存管理机制 对 系统性能的负担，它的实现必须紧凑，以便不会对 CPU的高速缓存和 TLB 带来显著影响


提供小内存块不是slab分配器的唯一任务。由于结构上的特点，它也用作一个缓存，主要针对经常分配并释放的对象。通过建立slab缓存，内核能够储备一些对象，供后续使用，即使在初始化状态，也是如此。

举例来说，为管理与进程关联的文件系统数据，内核必须经常生成struct fs_struct的新实例（参见第8章）。
此类型实例占据的内存块同样需要经常回收（在进程结束时）。
换句话说，内核趋向于非常有规律地分配并释放大小为sizeof{fs_struct}的内存块。
slab分配器将释放的内存块保存在一个内部列表中，并不马上返回给伙伴系统。
在请求为==该类对象==分配一个新实例时，会==使用最近释放的内存块==。
这有两个优点。
首先，由于内核不必使用伙伴系统算法，处理时间会变短。
其次，由于该内存块仍然是“新”的，因此其仍然驻留在CPU高速缓存的概率较高

slab分配器还有两个更进一步的好处。
- 减少调用 伙伴系统 这个较重的功能。伙伴系统 还会污染cache。伙伴系统的高速缓存 和 TLB占用 过大，这是负面效应，会导致 不重要的数据驻留在CPU的高速缓存中
- 伙伴系统 的地址 总是2的 幂次， 这对 CPU高速缓存有 负面影响，会导致 某些缓存行 过度使用。 多处理器系统 会加剧 这种不利情况，因为不同的内存地址 可能在不同的总线上传输，会导致 某些总线拥塞，其他总线几乎没有使用。

slab分配器由何得名？各个缓存管理的对象，会合并为较大的组，覆盖一个或多个连续页帧。这种组称作slab，每个缓存由几个这种slab组成。

### 备选分配器

slab分配器对许多可能的工作负荷都工作良好，但它无法提供最优性能

些计算机处于当前硬件尺度的边界上，在此类计算机上使用slab分配会出现一些问题：微小的嵌入式系统，配备有大量物理内存的大规模并行系统。
对嵌入式系统来说，slab分配器代码量和复杂性都太高。
在大型系统上，slab分配器所需的大量元数据可能成为一个问题：开发者称，仅slab的数据结构就需要很多吉字节内存。

slab分配器的2个替代品
- slob分配器进行了特别优化，减少代码量。它围绕一个简单的内存块链表展开
- slub分配器通过将页帧打包为组，并通过struct page中未使用的字段来管理这些组，试图最小化所需的内存开销。在大型计算机上slub比slab提供了更好的性能

每个分配器都必须实现一组特定的函数，用于内存分配和缓存。
- kmalloc、__kmalloc和kmalloc_node是一般的（特定于结点）内存分配函数。
- kmem_cache_alloc、kmem_cache_alloc_node提供（特定于结点）特定类型的内核缓存。


kmalloc(size, flags)分配长度为size字节的一个内存区，并返回指向该内存区起始处的一个void指针。
如果没有足够内存（在内核中这种情形不大可能，但却始终要考虑到），则结果为NULL指针。
flags参数使用3.5.4节讨论的GFP_常数，来指定分配内存的具体内存域，例如GFP_DMA指定分配适合于DMA的内存区。

`kfree(*ptr)`释放`*ptr`指向的内存区

与用户空间程序设计相比，内核还包括percpu_alloc和percpu_free函数，用于为各个系统CPU分配和释放所需内存区（不是明确地用于当前活动CPU）

`info = (struct cdrom_info *) kmalloc (sizeof (struct cdrom_info), GFP_KERNEL);`


建立和使用==缓存==的任务不是特别困难
先用kmem_cache_create建立一个适当的缓存，接下来即可使用kmem_cache_alloc和kmem_cache_free分配和释放其中包含的对象。
slab分配器负责完成与伙伴系统的交互，来分配所需的页。

所有活动缓存的列表保存在`/proc/slabinfo`中
`cat /proc/slabinfo`



### slab分配的原理

slab==分配器==由一个紧密地交织的数据和内存结构的网络组成

slab==缓存==由图3-44所示的两部分组成：保存管理性数据的缓存对象和保存被管理对象的各个slab。

![2d2721027e480ec2a5a36386345598f3.png](../_resources/2d2721027e480ec2a5a36386345598f3.png)


每个缓存只负责一种对象类型（例如struct unix_sock实例），或提供一般性的缓冲区。
系统中所有的缓存都保存在一个双链表中。这使得内核有机会依次遍历所有的缓存。例如在即将发生内存不足时，内核可能需要缩减分配给缓存的内存数量。


除了管理性数据（如已用和空闲对象或标志寄存器的数目），缓存结构包括两个特别重要的成员
- 指向一个数组的指针，其中保存了各个CPU最后释放的对象。
- 每个内存结点都对应3个表头，用于组织slab的链表。第1个链表包含完全用尽的slab，第2个是部分空闲的slab，第3个是空闲的slab。


![69a30563147cd8a01df15ebeb9d98cad.png](../_resources/69a30563147cd8a01df15ebeb9d98cad.png)


缓存结构指向一个数组，其中包含了与系统CPU数目相同的数组项。
每个元素都是一个指针，指向一个进一步的结构称之为数组缓存（array cache），其中包含了对应于特定系统CPU的管理数据（就总体来看，不是用于缓存）。
管理性数据之后的内存区包含了一个==指针数组==，各个数组项==指向slab中未使用的对象==
。。指针数组就是 图上 `指向slab中对象的指针构成的数组`，它和 它上面的 array_cache 是一起的。

为最好地利用CPU高速缓存，这些per-CPU指针是很重要的。在分配和释放对象时，采用后进先出原理（LIFO，last in first out）。内核假定刚释放的对象仍然处于CPU高速缓存中。仅当per-CPU缓存为空时，才会用slab中的空闲对象重新填充它们。

对象分配的体系就形成了一个三级的层次结构，分配成本和操作对CPU高速缓存和TLB的负面影响逐级升高。
。。应该就是 per-CPU，slab中空闲，伙伴系统  ？。。。下面

- 仍然处于CPU高速缓存中的per-CPU对象。
- 现存slab中未使用的对象。
- 刚使用伙伴系统分配的新slab中未使用的对象。


对象在slab中并非连续排列，而是按照一个相当复杂的方案分布
个对象的长度并不反映其确切的大小。相反，长度已经进行了舍入，以满足某些对齐方式的要求。
- slab创建时使用标志SLAB_HWCACHE_ALIGN，要求对象按==硬件缓存行==对齐
- 如果不要求按硬件缓存行对齐，那么内核保证对象按BYTES_PER_WORD对齐，该值是表示void指针所需字节的数目

填充字节可以加速对slab中对象的访问。如果使用对齐的地址，那么在几乎所有的体系结构上，内存访问都会更快

缓存的各个slab成员会指定不同的偏移量，以便将数据定位到不同的缓存行

乘法在这些计算机上要快得多，因此内核使用所谓的Newton-Raphson方法，这只需要乘法和移位，内核可以不计算C = A/B，而是采用C = reciprocal_divide(A, reciprocal_value(B))

`kmem_cache_init`，初始化slab 分配器。它在内核初始化阶段（start_kernel）、伙伴系统启用之后调用。
`kmem_cache_create`，创建新的slab 缓存

`calculate_slab_order`，实现迭代过程，找到理想的slab长度

`kmem_cache_alloc`，从特定的缓存获取对象

`cache_grow`，缓存增长

`kmem_cache_free`，释放对象

`kmem_cache_destroy`，销毁缓存，必须只包含 未使用的对象


### 通用缓存

如果不涉及对象缓存，而是传统意义上的分配/释放内存，则必须调用kmalloc和kfree函数。
kmalloc和kfree实现为slab分配器的前端，其语义尽可能地模仿malloc/free



## 3.7 处理器高速缓存和 TLB 控制

高速缓存对系统总体性能十分关键，这也是内核尽可能提高其利用效率的原因。这主要是通过在内存中巧妙地对齐内核数据。审慎地混合使用普通函数、内联定义、宏，也有助于从处理器汲取更高的性能。附录C讨论的编译器优化也相当有作用。

内核仍然提供了一些命令，可以直接作用于处理器的高速缓存和TLB。但这些命令并非用于提高系统的效率，而是用于维护缓存内容的一致性，确保不出现不正确和过时的缓存项。




# ch04 进程虚拟内存

用户层进程的虚拟地址空间是Linux的一个重要抽象：它向每个运行进程提供了同样的系统视图，这使得多个进程可以同时运行，而不会干扰到其他进程内存中的内容。

它容许使用各种高级的程序设计技术，如内存映射

这同样需要考察可用物理内存中的页帧与所有的进程虚拟地址空间中的页之间的关联：逆向映射（reverse mapping）技术有助于从虚拟内存页跟踪到对应的物理内存页，而缺页处理（page fault handling）则允许从块设备按需读取数据填充虚拟地址空间。


## 4.2 进程虚拟地址空间

各个进程的虚拟地址空间起始于地址0，延伸到TASK_SIZE - 1，其上是内核地址空间

无论当前哪个用户进程处于活动状态，虚拟地址空间==内核==部分的内容==总是同样的==。

虚拟地址空间由许多不同长度的段组成，用于不同的目的，必须分别处理。

### 进程地址空间的布局

虚拟地址空间中包含了若干区域。其分布方式是特定于体系结构的，但所有方法都有下列共同成分。
- 当前==运行代码==的二进制代码。该代码通常称之为text，所处的虚拟内存区域称之为text段。
- 程序使用的==动态库==的代码。
- 存储==全局变量==和动态产生的数据的==堆==。
- 用于保存==局部变量==和实现函数/过程调用的==栈==。
- ==环境变量和命令行参数==的段。
- 将==文件==内容映射到虚拟地址空间中的==内存映射==

系统中的各个进程都具有一个struct mm_struct的实例，可以通过task_struct访问
这个实例保存了进程的内存管理信息

```C
// <mm_types.h> 
struct mm_struct { 
... 
  unsigned long (*get_unmapped_area) (struct file *filp, 
      unsigned long addr, unsigned long len, 
      unsigned long pgoff, unsigned long flags); 
... 
  unsigned long mmap_base; /* mmap区域的基地址 */ // 虚拟地址空间中 用于内存映射的起始地址；调用get_unmapped_area在mmap区域中为新映射找到适当的位置
  unsigned long task_size; /* 进程虚拟内存空间的长度 */ // 进程的地址空间长度。通常是 TASK_SIZE
... 
  unsigned long start_code, end_code, start_data, end_data; // 代码(text)段的起止，数据(data)段的起止
  unsigned long start_brk, brk, start_stack; // 堆的起止，brk的值会变；栈的起(是指栈顶吧？)
  unsigned long arg_start, arg_end, env_start, env_end; // 参数列表和环境变量的位置，这2个区域处于栈中最高的区域
... 
}
```

通常，栈自顶向下增长
。。就是 从 内核区域 开始 ？  内核是 高区

进程标志PF_RANDOMIZE。如果设置了该标志，则内核不会为栈和内存映射的起点选择固定位置，而是在每次新进程启动时随机改变这些值的设置。
使得攻击因缓冲区溢出导致的安全漏洞更加困难

text段如何映射到虚拟地址空间中由ELF标准确定

每个==体系结构==都指定了一个==特定的起始地址==：IA-32系统起始于0x08048000，在text段的起始地址与最低的可用地址之间有大约128 MiB的间距，==用于捕获NULL指针==。
其他体系结构也有类似的缺口：UltraSparc计算机使用0x100000000作为text段的起始点，而AMD64使用0x0000000000400000。
==堆紧接着text段开始，向上增长。==

栈起始于STACK_TOP，如果设置了PF_RANDOMIZE，则起始点会减少一个小的随机量。
每个体系结构都必须定义STACK_TOP，大多数都设置为TASK_SIZE，即用户地址空间中==最高的可用地址==。

进程的==参数列表和环境变量==都是==栈的初始数据==。

用于==内存映射==的区域起始于mm_struct->mmap_base，通常设置为TASK_UNMAPPED_BASE，每个体系结构都需要定义。
几乎所有的情况下，其值都是==TASK_SIZE/3==。

![8da1f12bf60d70380061609b0b1da685.png](../_resources/8da1f12bf60d70380061609b0b1da685.png)


如果计算机提供了巨大的虚拟地址空间，那么使用上述的地址空间布局会工作得非常好。但在32位计算机上可能会出现问题:
堆只有1 GiB空间可供使用，继续增长则会进入到mmap区域

内核版本2.6.7开发期间为IA-32计算机引入一个新的虚拟地址空间布局的原因（经典布局仍然可以使用）。

![0e6092ffa1ccb9924d1cc400e9a9e25b.png](../_resources/0e6092ffa1ccb9924d1cc400e9a9e25b.png)


。。百度了下，【经典布局】是mmap向上， 【灵活布局】mmap向下

其想法在于使==用固定值限制栈的最大长度==。
由于栈是有界的，因此安置内存映射的区域可以在栈末端的下方立即开始。
与经典方法相反，该区域现在是自顶向下扩展。


### 建立布局
使用load_elf_binary装载一个ELF二进制文件时，将创建进程的地址空间，而exec系统调用刚好使用了该函数。
加载ELF文件涉及大量纷繁复杂的技术细节

。。Executable and Linkable Format

![178048198a74f68585fcf13837c322a5.png](../_resources/178048198a74f68585fcf13837c322a5.png)

选择布局的工作由arch_pick_mmap_layout完成。如果对应的体系结构没有提供一个具体的函数，则使用内核的默认例程

如果用户通过/proc/sys/kernel/legacy_va_layout给出明确的指示，或者要执行为不同的UNIX变体编译、需要旧的布局的二进制文件，或者栈可以无限增长（最重要的一点），则系统会选择旧的布局。这使得很难确定栈的下界，亦即mmap区域的上界。


灵活布局时，可以根据栈的最大长度，来计算栈最低的可能位置，用作mmap区域的起始点。但内核会确保栈至少跨越128 MiB的空间。

AMD64系统上对虚拟地址空间==总是使用经典布局==


## 4.3 内存映射的原理

由于所有用户进程总的虚拟地址空间比可用的物理内存大得多，因此只有最常用的部分才与物理页帧关联。
这不是问题，因为大多数程序只占用实际可用内存的一小部分。
我们考察一下通过文本编辑器操作文件的情况。
通常用户只关注文件结尾处，因此尽管整个文件都映射到内存中，实际上只使用了几页来存储文件末尾的数据。
至于文件开始处的数据，内核只需要在地址空间保存相关信息，如数据在磁盘上的位置，以及需要数据时如何读取。

text段的情形类似，始终需要的只是其中一部分

内核必须提供数据结构，以建立虚拟地址空间的区域和相关数据所在位置之间的关联
文件数据在硬盘上的存储通常并不是连续的，而是分布到若干小的区域

内核利用address_space数据结构，提供一组方法从后备存储器读取数据。
例如，从文件系统读取。因此address_space形成了一个辅助层，将映射的数据表示为连续的线性区域，提供给内存管理子系统。

按需分配和填充页称之为按需调页法（demand paging）。它基于处理器和内核之间的交互

![15482504937381299cf11a8ede34f5be.png](../_resources/15482504937381299cf11a8ede34f5be.png)


-  进程试图访问用户地址空间中的一个内存地址，但使用==页表==无法确定物理地址（==物理内存中没有关联页==）。
- 处理器接下来触发一个==缺页异常==，发送到内核。
- ==内核==会检查负责缺页区域的进程地址空间数据结构，找到==适当的后备存储器==，==或==者确认该访问==实际上是不正确==的。
- ==分配物理内存页==，并==从后备存储器读取==所需数据填充。
- 借助于==页表==将==物理内存页并入到用户进程的地址空间==，应用程序恢复执行。

。。在页表中，虚拟地址已经有了，但是 对应的 物理地址还没有，就会 缺页异常，由 CPU 负责 确认 加载 磁盘(后备存储器) 的数据 到 物理内存，然后 填入 页表。然后 app 继续运行。

这些操作对用户进程是透明的。换句话说，进程不会注意到页是实际在物理内存中，还是需要通过按需调页加载。
。。这个透明 原文 是什么？ 。。 始终感觉应该是  "无感的" 。。不过中文 无感 是指 没有兴趣。
。。transparent， 透明的;清澈的;显而易见的;易懂的;易看穿的;易识破的 。。 后面的 中文含义 (在这个场景下) 更离谱。。



## 4.4 数据结构

```C
<mm_types.h> 
struct mm_struct { 
  struct vm_area_struct * mmap; /* 虚拟内存区域列表 */ 
  struct rb_root mm_rb; 
  struct vm_area_struct * mmap_cache; /* 上一次find_vma的结果 */ 
... 
} 
```

### 树和链表

每个区域都通过一个vm_area_struct实例描述，进程的各区域按两种方法排序。
- 在一个单链表上（开始于mm_struct->mmap）。
- 在一个红黑树中，根结点位于mm_rb

mmap_cache缓存了上一次处理的区域


![f16273f013118a485998ff4fcb90ccf7.png](../_resources/f16273f013118a485998ff4fcb90ccf7.png)

增加新区域时，内核首先搜索红黑树，找到刚好在新区域之前的区域。因此，内核可以向树和线性链表添加新的区域，而无需扫描链表


### 虚拟内存区域的表示

每个区域表示为vm_area_struct的一个实例
`<mm_types.h>`

里面包含了
- vm_mm 反向指针，指向 该区域所属的mm_struct 实例
- vm_start,vm_end, 该区域在 用户空间的 起止地址
- vm_next, vm_rb, vm_area_struct 实例的 链表 和 红黑树
- vm_page_prot, 访问权限
- vm_flags, 描述该区域的一组标志 (下面有详细说明)
- 共享映射
- anon_vma_node, anon_vma，管理源自匿名映射(annoymous mapping)的共享页
- vm_ops，指针，指向 许多方法的 集合，这些方法用于在 区域上执行各种标准操作。
  ```C
  <mm.h>
  struct vm_operations_struct {
    void (*open)(struct vm_area_struct * area); 
    void (*close)(struct vm_area_struct * area); 
    int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf); 
    struct page * (*nopage)(struct vm_area_struct * area, unsigned long address, int *type); 
  ... 
  }; 
  ```
  - 创建和删除区域时，调用open，close。这2个接口通常不使用，设置为NULL指针
  - fault 非常重要，如果地址空间中的某个虚拟内存页不在物理内存中，自动触发的==缺页==异常处理程序会调用该函数，将对应的数据读取到一个映射在用户地址空间的物理内存页中。
  - nopage 是内核原来用于响应缺页异常的方法，不如fault那么灵活。出于兼容性的考虑，该成员仍然保留，但不应该用于新的代码。

- vm_pgoffset 执行了文件映射的偏移量，用于只映射了文件部分内容时(如果映射了整个文件，则偏移量为0)。单位是 页
- vm_file 指向 file实例，被映射的文件 (如果映射的不是文件，则NULL)
- 取决于映射类型，vm_private_data可用于存储私有数据，不由通用内存管理例程操作

vm_flags存储了定义区域性质的标志。这些都是`<mm.h>`中声明的预处理器常
- VM_READ, VM_WRITE, VM_EXEC, VM_SHARED，页的内容是否可读，写，执行，或由几个进程共享
- VM_MAYREAD, VM_MAYWRITE, VM_MAYEXEC, VM_MAYSHARE 用于确定是否可以设置 对应的 VM_* 标志。 这是 mprotect 系统调用 所需要的
- VM_GROWDOWN,VM_GROWUP，表示 一个区域是否可以 向下或向上 扩展 ( 到更低 或更高的 虚拟地址)。 由于 堆是 自底向上，所以 区域 需要设置为 VM_GROWUP。
- 如果区域很可能从头到尾顺序读取，则设置 VM_SEQ_READ。 VM_RAND_READ指明读取可能是随机的。 这2个标记 用于 提示 内存管理子系统 和 块设备层，以优化其性能( 例如，如果是顺序访问，则启用 页的预读)
- 如果设置了 VM_DONTCOPY，则相关区域 在 ==fork== 时不复制
- VM_DONTEXPAND 禁止区域通过 mremap 系统调用扩展
- 如果区域 是基于 巨型页，则 VM_HUGETLB
- VM_ACCOUNT 指定区域是否被归入 overcommit 特性的计算中。


### 4.4.3 优先查找树

priority search tree 用于建立文件中的一个区域与该区域映射到的所有虚拟地址空
间之间的关联。

每个打开文件（和每个块设备，因为这些也可以通过设备文件进行内存映射）都表示为struct file的一个实例。该结构包含了一个指向地址空间对象struct address_space的指针。
该对象是优先查找树（prio tree）的基础，而文件区间与其映射到的地址空间之间的关联即通过优先树建立。

![046aeded2e8c35fa832e822bde1b8813.png](../_resources/046aeded2e8c35fa832e822bde1b8813.png)


地址空间是优先树的基本要素，而优先树包含了所有相关的vm_area_struct实例，描述了与inode关联的文件区间到一些虚拟地址空间的映射。
由于每个struct vm_area的实例都包含了一个指向所属进程的mm_struct的指针，关联就已经建立起来了！
要注意，vm_area_struct还可以通过以i_mmap_nonlinear为表头的双链表与一个地址空间关联。
这是非线性映射（nonlinear mapping）所需要的，我现在暂时忽略该内容。我们将在4.7.3节再讲解非线性映射。



## 对区域的操作

创建和删除区域

内核还负责在管理这些数据结构时进行优化

![d71b62bd0f48f597692e422cdcd8664b.png](../_resources/d71b62bd0f48f597692e422cdcd8664b.png)

。。3个箭头 对应下面的 3个情况

- 如果一个新区域紧接着现存区域前后直接添加（因此也包括在两个现存区域之间的情况），内核将涉及的数据结构合并为一个。当然，前提是涉及的所有区域的访问权限相同，而且是从同一后备存储器映射的连续数据。
- 如果在区域的开始或结束处进行删除，则必须据此截断现存的数据结构
- 如果删除两个区域之间的一个区域，那么一方面需要减小现存数据结构的长度，另一方面需要为形成的新区域创建一个新的数据结构


### 将虚拟地址关联到区域

通过虚拟地址，find_vma可以查找用户地址空间中结束地址在给定地址之后的第一个区域，即满足addr < vm_area_struct->vm_end条件的第一个区域。
该函数的参数不仅包括虚拟地址（addr），还包括一个指向mm_struct实例的指针，后者指定了扫描哪个进程的地址空间

内核首先检查上次处理的区域（现在保存在mm->mmap_cache）中是否包含所需的地址，即是否该区域的结束地址在目标地址之后，而起始地址在目标地址之前。倘若如此，内核不会执行if语句，而是立即将指向该区域的指针返回。
否则必须逐步搜索红黑树。rb_node是用于表示树中各个结点的数据结构。rb_entry用于从结点取出“有用数据”（在这里是vm_area_struct实例）。
树的根结点位于mm->mm_rb.rb_node。如果相关的区域结束地址大于目标地址而起始地址小于目标地址，内核就找到了一个适当的结点，可以退出while循环，返回指向vm_area_struct实例的指针。否则，再继续搜索



### 区域合并
vm_merge在可能的情况下，将一个新区域与周边区域合并。它需要很多参数


### 插入区域

insert_vm_struct是内核用于插入新区域的标准函数。实际工作委托给两个辅助函数
- find_vma_prepare
- vma_link




### 创建区域
在向数据结构插入新的内存区域之前，内核必须确认虚拟地址空间中有足够的空闲空间，可用于给定长度的区域。该工作分配给get_unmapped_area辅助函数完成



## 4.6 地址空间
文件的内存映射可以认为是两个不同的地址空间之间的映射
一个地址空间是用户进程的虚拟地址空间，另一个是文件系统所在的地址空间。

在内核创建一个映射时，必须建立两个地址空间之间的关联，以支持二者以读写请求的形式通信。
vm_operations_struct结构即用于完成该工作


address_space_operations
提供的方法
- readpage，从潜在的块设备读取一页到物理内存，readpages一次读取多页
- writepage，一页内容从物理内存回写到 块设备
- set_page_dirty，表示 一页的内容已经改变，即与块设备上的原始内容不再匹配



## 4.7 内容映射
C标准库提供了 mmap
内核，提供了 mmap,mmap2

mmap,mmap2 的差别在于 偏移量的语义，mmap的单位是字节，mmap2的单位是 页

### 创建映射

mmap,mmap2

do_mmap_pgoff

### 删除映射

munmap


### 非线性映射

按照上文的描述，普通的映射将文件中一个连续的部分映射到虚拟内存中一个==同样连续==的部分

如果需要将文件的不同部分以不同顺序映射到虚拟内存的连续区域中，通常必须使用几个映射

简单的方法是使用非线性映射

```C
// mm/fremap.c 
long sys_remap_file_pages(unsigned long start, unsigned long size,  unsigned long __prot, unsigned long pgoff, unsigned long flags) 
```

该系统调用允许==重排==映射中的==页==，使得内存与文件中的顺序不再等价。实现该特性无需移动内存中的数据，而是通过操作进程的==页表==实现


## 4.8 反向映射

内核利用此前讨论的数据结构，可以建立虚拟和物理地址之间的联系（通过页表），以及进程的一个内存区域与其虚拟内存页地址之间的关联。

仍然缺失的一个联系是，物理内存页和该页所属进程（或更精确地说，所有使用该页的进程的对应页表项）之间的联系。
在换出页时，刚好需要该关联（参见第18章），以便更新所有涉及的进程。因为页已经换出，必须在页表中标明

内核使用一些附加的数据结构和函数，采用一种逆向映射方法


page结构（在3.2.2节讨论过）包含了一个用于实现逆向映射的成员。
```C
// mm.h 
struct page { 
.... 
  atomic_t _mapcount; /* 内存管理子系统中映射的页表项计数，用于表示页是否
              * 已经映射，还用于限制逆向映射搜索。
              */ 
... 
}; 
```
_mapcount表明共享该页的位置的数目。计数器的初始值为1。在页插入到逆向映射数
据结构时，计数器赋值为0。页每次增加一个使用者时，计数器加1。这使得内核能够快速检查在所有者之外该页有多少使用者。


这没有多少帮助，因为逆向映射的目的在于：给定page实例，找到所有映射了该物理内存页的位置。因此，还有两个其他的数据结构需要发挥作用。
- 优先查找树中嵌入了属于非匿名映射的每个区域。
- 指向内存中同一页的匿名区域的链表。

内核在实现逆向映射时采用的技巧是，不直接保存页和相关的使用者之间的关联，而只保存页和页所在区域之间的关联。
包含该页的所有其他区域（进而所有的使用者）都可以通过刚才提到的数据结构找到。
该方法又名基于对象的逆向映射（object-based reverse mapping），因为没有存储页和使用者之间的直接关联。相反，在两者之间插入了另一个对象（该页所在的区域）



---

在创建逆向映射时，有必要区分两个备选项：匿名页和基于文件映射的页。




## 4.9 堆的管理

malloc和内核之间的经典接口是brk系统调用，负责扩展/收缩堆。
新近的malloc实现（诸如GNU标准库提供的）使用了一种组合方法，使用brk和匿名映射。

brk系统调用只需要一个参数，用于指定堆在虚拟地址空间中新的结束地址（如果堆将要收缩，当然可以小于当前值）

brk系统调用实现的入口是sys_brk函数
sys_brk第一个重要操作是将请求的地址按页长度对齐
在需要收缩堆时将调用do_munmap
扩大堆的实际工作委托给do_brk


## 4.10 缺页异常的处理

在实际需要某个虚拟内存区域的数据之前，虚拟和物理内存之间的关联不会建立。
如果进程访问的==虚拟地址空间==部分尚==未与页帧关联==，处理器自动地引发一个缺页异常，内核必须处理此异常。这是内存管理中==最重要、最复杂==的方面之一，因为必须考虑到无数的细节。
例如，内核必须确定以下情况。
- 缺页异常是由于访问用户地址空间中的有效地址而引起，还是应用程序试图访问内核的受保护区域？
- 目标地址对应于某个现存的映射吗？
- 获取该区域的数据，需要使用何种机制

![284c2f927fda76638eb8f4e6c318a617.png](../_resources/284c2f927fda76638eb8f4e6c318a617.png)



arch/x86/kernel/entry_32.S中的一个汇编例程用作缺页异常的入口，但其立即调用了arch/x86/mm/fault_32.c中的C例程do_page_fault。（大多数CPU对应的特定于体系结构的源代码中，都包含一个同名例程。）
图4-18给出了该例程的代码流程图

![1a24761fc75934478739696001b7273b.png](../_resources/1a24761fc75934478739696001b7273b.png)






## 4.13 在内核和用户空间之间复制数据

copy_from_user
get_user
strncopy_from_user
put_user
copy_to_user

clear_user
strlen_user
strnlen_user

根据表的内容，大多数函数都有两个版本。
在没有下划线前缀的版本中，还会调用access_user，对用户空间地址进行检查。
所执行的检查依体系结构而不同。
例如，一种平台的检查可能是确认指针确实指向用户空间中的位置。
而另一种可能在内存中找不到页时，调用handle_mm_fault以确保数据已经读入内存，可供处理。
所有函数都应用了上述用于检测和校正缺页异常的修正机制。


sys_chroot



# ch05 锁与进程间通信 277


几个进程在访问资源时彼此干扰的情况通常称之为==竞态条件==（race condition）。


这个问题的本质是：进程的执行在不应该的地方被中断，从而导致进程工作得不正确。
显然，一种可能的解决方案是标记出相关的代码段，==使之无法被调度器中断==。
尽管这种方法原则上是可行的，但有几个内在问题。
在某种情况下，有问题的程序可能迷失在标记的代码段中无法退出，因而无法放弃CPU，进而导致计算机不可用。
因此我们必须立即==放弃这种解决方案==。
。。原子性

问题的解决方案不一定要求临界区是不能中断的。只要==没有其他的进程进入临界区==，那么在临界区中执行的==进程完全是可以中断==的。
在给定时刻，只有一个进程可以进入临界区代码。

---

信号量
信号量只是受保护的特别变量，能够表示为正负整数。其初始值为1。
为操作信号量定义了两个标准操作：up和down。这两个操作分别用于控制关键代码范围的进入和退出，且假定相互竞争的进程访问信号量机会均等。

在执行down操作时，有一点特别重要。
即从应用程序的角度来看，该操作应视为一个==原子操作==。它==不能被调度器调用中断==，这意味着竞态条件是无法发生的。
从内核视角来看，查询变量的值和修改变量的值是两个不同的操作，但用户将二者视为一个原子操作。

当进程在信号量上睡眠时，内核将其置于阻塞状态，且与其他在该信号量上等待的进程一同放到一个等待列表中。

在进程退出关键代码段时，执行up操作。这不仅会将信号量的值加1（恢复为1），而且还会唤醒一个在该信号量上睡眠的进程。

如果没有内核的支持，这个过程是不可能的，因为用户空间库无法保证down操作不被中断


## 5.2 内核锁机制
内核可以不受限制地访问整个地址空间。

如果几个处理器同时处于核心态，则理论上它们可以同时访问同一个数据结构

内核使用了由锁组成的细粒度网络，来明确地保护各个数据结构。
如果处理器A在操作数据结构X，则处理器B可以执行任何其他的内核操作，但不能操作X。

内核为此提供了各种锁选项，分别优化不同的内核数据使用模式
- 原子操作
- 自旋锁
- 信号量
- 读写锁

锁已经成为内核开发一个非常重要的方面，无论是基础的核心内核代码还是设备驱动程序。

---
### 5.2.4 RCU机制
read-copy-update 同步机制

RCU的性能很好，不过对内存有一定的开销,但大多数情况下可以忽略

RCU对潜在使用者提出的一些约束
- 对共享资源的访问在大部分时间应该是只读的，写访问应该相对很少。
- 在RCU保护的代码范围内，内核不能进入睡眠状态。
- 受保护资源必须通过指针访问。

RCU的==原理==很简单：该机制记录了指向共享数据结构的指针的所有使用者。
在该结构将要改变时，则首先创建一个副本（或一个新的实例，填充适当的内容，这没什么差别），在副本中修改。
在所有进行读访问的使用者结束对旧副本的读取之后，指针可以替换为指向新的、修改后副本的指针。
请注意，这种机制允许读写并发进行！


RCU能保护的，不仅仅是一般的指针。
内核也提供了标准函数，使得能通过RCU机制保护双链表，这是RCU机制在内核内部最重要的应用。
此外，由struct hlist_head和struct hlist_node组成的散列表也可以通过RCU保护。


### 5.2.5 内存和优化屏障

一个有利于提高性能的技术是指令重排。

尽管锁足以确保原子性，但对编译器和处理器优化过的代码，锁不能永远保证时序正确。与竞态条件相比，这个问题不仅影响SMP系统，也影响单处理器计算机。

- mb(),rmb(),wmb(), 将硬件内存屏障插入到代码流程中，rmb是读访问内存屏障，它保证屏障之后的 读 在执行前，屏障之前的 所有 读已完成。 wmb是写访问屏障， mb是 两者合一。
- barrier，知编译器，保存在CPU寄存器中、在屏障之前有效的所有内存地址，在屏障之后都将失效。本质上，这意味着编译器在屏障之前发出的读写请求完成之前，不会处理屏障之后的任何读写请求。
- smp_mb(),smp_rmb(),smp_wmb(), 只用于 smp系统。它们在单处理器系统上 产生的是 软件屏障
- read_barrier_depends()，会考虑读操作之间的依赖性。如果屏障之后的读请求，依赖于屏障之前执行的读请求的数据，那么编译器和硬件都不能重排这些请求。

与开启优化时相比，停用优化后程序的速度会减慢

优化屏障的一个特定应用是内核抢占机制



### 5.2.7 大内核锁
内核锁遗迹之一，它可以锁定整个内核，确保没有处理器在核心态并行运行。
该锁称为大内核锁（big kernel lock），通常用缩写表示，即BKL。
使用lock_kernel可锁定整个内核，对应的解锁使用unlock_kernel。

它已经是过时的概念，因为从性能和可伸缩性的角度来看，BKL简直是个灾难。新的代码决不应该使用BKL




## 5.3 System V 进程间通信

信号量，消息队列，共享内存
它们都使用了全系统范围的资源，可以由几个进程同时共享

IPC对象必须在系统内唯一标识

每种IPC结构在创建时分配了一个号码。凡知道这个魔数的各个程序，都能够访问对应的结构

如果独立的应用程序需要彼此通信，则通常需要将该魔数永久地编译到程序中。一种备选方案是动态地产生一个保证唯一的魔数（静态分配的号码无法保证唯一）

每个IPC对象都有一个用户ID和一个组ID，依赖于产生IPC对象的程序在何种UID/GID之下运行。
读写权限在初始化时分配。


### 5.3.2 信号量

System V信号量在sem/sem.c实现，对应的头文件是`<sem.h>`。
==这种信号量与上文讲述的内核信号量没有任何关系==

System V的信号量接口决==不直观==，因为信号量的概念已经远超其实际定义了。
信号量不再当作是用于支持原子执行预定义操作的简单类型变量。
相反，一个System V信号量现在是指==一整套信号量==，可以==允许几个操作同时进行==（尽管==用户看上去它们是原子的==）。
当然可以请求只有一个信号量的信号量集合，并定义函数模拟原始信号量的简单操作。

以下示例程序说明了信号量的使用方式：
```C
#include<stdio.h> 
#include<sys/types.h> 
#include<sys/ipc.h> 
#include<sys/sem.h> 

#define SEMKEY 1234L /* 标识符 */ 
#define PERMS 0666 /* 访问权限： rwrwrw */ 
struct sembuf op_down[1] = { 0, -1 , 0 }; 
struct sembuf op_up[1] = { 0, 1 , 0 }; 
int semid = -1; /* 信号量 ID */ 
int res; /* 信号量操作的结果 */ 
void init_sem() { 
  /* 测试信号量是否已经存在 */ 
  semid = semget(SEMKEY, 0, IPC_CREAT | PERMS); 
  if (semid < 0) { 
    printf("Create the semaphore\n"); 
    semid = semget(SEMKEY, 1, IPC_CREAT | PERMS); 
    if (semid < 0) { 
      printf("Couldn't create semaphore!\n"); 
      exit(-1); 
    }
    /* 初始化为1 */ 
    res = semctl(semid, 0, SETVAL, 1); 
  } 
} 
void down() { 
  /* 执行down操作 */ 
  res = semop(semid, &op_down[0], 1); 
} 
void up() { 
  /* 执行up操作 */ 
  res = semop(semid, &op_up[0], 1); 
} 
int main(){ 
  init_sem(); 
  /* 正常的程序代码 */ 
  printf("Before critical code\n"); 
  down(); 
  /* 临界区代码 */ 
  printf("In critical code\n"); 
  sleep(10); 
  up(); 
  /* 其余代码 */ 
  return 0; 
} 
```

用一个持久定义的魔数（1234）创建了一个新的信号量，以便在系统内建立标识
需要测试对应的信号量是否已经存在。如果没有，则创建信号量

semget分配一个信号量集合，需要以下参数：魔数（SEMKEY），集合中信号量的数目（1），所需的访问权限

然后使用semctl系统调用，将信号量集合中唯一的信号量的值初始化为1

semid变量在内核中标识了该信号量（任何其他程序，可以借助于魔数来获得该值）。

一个指向数组的指针，数组元素类型为sembuf，每个元素表示对一个信号量的操作。数组中操作的数目由另一个整数参数定义，否则内核将无法得知操作的数目。


---

![0e3d2f512b6e8deb99f4b13d496a1033.png](../_resources/0e3d2f512b6e8deb99f4b13d496a1033.png)

。。。。。。




### 5.3.3 消息队列

![98686109eb1b2669ec0a431129ec3b91.png](../_resources/98686109eb1b2669ec0a431129ec3b91.png)


产生消息并将其写到队列的进程通常称之为==发送者==，而一个或多个其他进程（逻辑上称之为==接收者==）则从队列获取信息。
各个消息包含==消息正文==和一个（==正）数==，以便在消息队列内实现几种类型的消息。
接收者可以==根据该数字检索消息==，例如，可以指定==只接受编号1的消息==，或接受编号不大于5的消息。
在消息已经==读取后，内核将其从队列删除==。
即使几个进程在同一信道上监听，==每个消息==仍然只能==由一个进程读取==

同一编号的消息按先进先出次序处理。
放置在队列开始的消息将首先读取。
但如果有选择地读取消息，则先进先出次序就不再适用


### 5.3.4 共享内存

![e41fb294d843fdb535d21c18cb8e8ab2.png](../_resources/e41fb294d843fdb535d21c18cb8e8ab2.png)



## 5.4 其他IPC机制

SysV IPC通常只对应用程序员有意义，但几乎所有使用过shell的用户，都会知道==信号和管道==。


### 5.4.1 信号

信号是一种比较原始的通信机制。尽管提供的选项较少，但是它们非常有用。

其底层概念非常简单，==kill==命令根据PID向进程==发送信号==。
信号通过`-s sig`指定，是一个正整数，最大长度取决于处理器类型。
该命令有两种最常用的变体：一种是kill==不指定信号==，实际上是==要求进程结束==（==进程可以忽略该信号==）；另一种是==kill -9==，等价于在死刑批准上签字（导致某些进程死亡）。

信号引入了几种特性，必须永远切记。进程可以决定阻塞特定的信号（有时称之为信号屏蔽）如果发生这种情况，会一直忽略该信号，直至进程决定解除阻塞

因而，进程是否能感知到发送的信号，是不能保证的。在信号被阻塞时，内核将其放置到待决列表上。如果同一个信号被阻塞多次，则在待决列表中只放置一次。

SIGKILL信号无法阻塞，也不能通过特定于进程的处理程序处理。

它与SIGTERM信号不同，后者可以通过用户定义的信号处理程序处理，实际上只是向进程发出的一个客气的请求，要求进程尽快停止工作而已。

init进程属于特例。内核会忽略发送给该进程的SIGKILL信号

---

sigaction系统调用用于设置新的处理程序

```C
#include<signal.h> 
#include<stdio.h> 
/* 处理程序函数 */ 
void handler(int sig) { 
  printf("Receive signal: %u\n", sig); 
}; 
int main(void) { 
  struct sigaction sa; 
  int count; 
  /* 初始化信号处理程序结构 */ 
  sa.sa_handler = handler; 
  sigemptyset(&sa.sa_mask); 
  sa.sa_flags = 0; 
  /* 给SIGTERM信号分配一个新的处理程序函数 */ 
  sigaction(SIGTERM, &sa, NULL); 
  sigprocmask(&sa.sa_mask); /* 接收所有信号 */ 
  /* 阻塞，一直等到信号到达 */ 
  while (1) { 
    sigsuspend(&sa.sa_mask); 
    printf("loop\n"); 
  } 
  return 0; 
};
```

如果没有为某个信号分配用户定义的处理程序函数，内核会自动设置预定义函数，提供合理的标准操作来处理相应的情况。

标准信号的默认处理操作有4种：忽略，结束，停止，内存转储


### 5.4.2 管道和套接字

管道：|

套接字 第12章








# ch06 设备驱动程序 312

字符设备，块设备

寻址，注册，

与文件系统关联

向系统添加磁盘和分区

# ch07 模块 377

# ch08 虚拟文件系统 413

为支持各种本机文件系统，且在同时允许访问其他操作系统的文件，Linux内核在用户进程（或C标准库）和文件系统实现之间引入了一个抽象层。该抽象层称之为虚拟文件系统（Virtual File System），简称VFS

![9cf8ec0307bc7fa6a85d346ce1c66cf3.png](../_resources/9cf8ec0307bc7fa6a85d346ce1c66cf3.png)


VFS用来提供一种操作文件、目录及其他对象的统一方法,它必须能够与各种方法给出的具体文件系统的实现达成妥协


## 8.1 文件系统类型

分为3种
1. 基于磁盘的文件系统，在非易失介质上存储文件的经典方法
2. 虚拟文件系统，内核中生成，是一种使用户应用程序与用户==通信==的方法。如proc文件系统。
3. 网络文件系统，是基于磁盘的文件系统和虚拟文件系统之间的折中。这种文件系统允许访问另一台计算机上的数据，该计算机通过网络连接到本地计算机。


## 通用文件模型

有些操作（对“普通”文件是不可缺少的）对某些对象完全没有意义，例如集成到==VFS中的命名管道==。

定义一个最小的通用模型，来支持内核中所有文件系统都实现的那些功能，这是不实际的。这样会损失许多本质性的功能特性，或者导致这些特性只能通过特定文件系统的路径访问

VFS的方案完全相反：提供一种结构模型，包含了一个强大文件系统所应具备的所有组件。但该模型只存在于虚拟中，必须使用各种对象和函数指针与每种文件系统适配


对==用户程序==来说，一个文件由一个==文件描述符==标识
两个==不同进程==可以使用==同样的文件描述符==，但二者并==不指向同一个文件==。

内核处理文件的关键是inode。
每个文件（和目录）都有且只有一个对应的indoe，其中包含元数据（如访问权限、上次修改的日期，等等）和指向文件数据的指针。
但inode并==不包含==一个重要的信息项，即==文件名==，这看起来似乎有些古怪。
通常，假定文件名称是其主要特征之一，因此应该被归入用于管理文件的对象（inode）中


### 8.2.1 inode

inode 既描述文件，也描述 目录。目录只是一种特殊的文件

成员分为2类
- 描述文件状态的==元数据==。例如，访问权限或上次修改的日期
- 保存实际==文件内容==的数据段（或指向数据的指针）。就文本文件来说，用于保存文本


> 内核查找对应于/usr/bin/emacs 的inode过程

查找起始于inode，它表示根目录/，对系统来说必须总是已知的。其数据段并不包含普通数据，而是根目录下的各个目录项。这些项可能代表文件或其他目录
每个项由两个成员组成。
- 该目录项的数据所在inode的编号。
- 文件或目录的名称。

所有inode都有一个特定的编号，用于唯一地标识各个inode

查找操作中的第一步是查找子目录usr的inode。这一步会扫描 根 inode的数据段，直至找到一个名为usr的目录项（如果查找失败，则返回File not found错误）。
相关的inode可以根据inode编号定位

重复上述步骤，但这一次在usr对应inode的数据段中查找名为bin的目录项，以便根据其inode编号定位inode。

下一步在bin的inode数据段中，将查找名为emacs的目录项。这仍然会返回一个inode编号，这一次的inode表示==文件==而非目录。

前三个inode都表示目录，其==文件内容是目录项的一个列表==，包括子目录和文件。
与emacs文件关联的inode，其==数据段存储了文件的内容==。

上面的基本思想与VFS的实际实现相同，但有一些细节上的差异。实际的实现使用了缓存来加速查找操作
VFS层必须与提供实际信息的底层文件系统通信


### 链接

链接（link）用于建立文件系统对象之间的联系，这不符合经典的树模型。
有两种类型的链接，符号链接与硬链接

。符号链接，就是 软连接。是一个快捷方式。(快捷方式也是一个文件， 实际文件也是一个文件， 这是2个不同的文件)

对于符号链接，可以区分原始文件和链接
。。就是 上面的 一个是 快捷方式，  一个是 真实的文件。 可以区分

在硬链接已经建立后，==无法区分哪个文件是原来的，哪个是后来建立的==。
在硬链接建立时，创建的目录项==使用了一个现存的inode编号==。

在inode中加入一个==计数器==，每次对文件创建一个==硬链接==时，都将==计数器加1==。
如果其中一个硬链接或原始文件被删除（不可能区分这两种情况），那么将计数器减1
只有在==计数器归0==时，我们才能确认该inode不再使用，可以从系统删除



### 编程接口

==用户进程==和==内核的VFS==实现之间的接口照例由==系统调用==组成，其中大多数涉及对文件、目录和一般意义上的文件系统的操作。

内核提供了50多个系统调用

文件使用之前，必须用==open或openat系统调用==打开。在成功打开文件之后，内核向用户层返回一个==非负的整数==。

尽管传统上文件描述符在内核中足以标识一个文件，但现在情况不再如此。
由于多个命名空间和容器的引入，具有==相同数值的多个文件描述符==可以共存于内核中。
对文件的==唯一表示==由一个特殊的==数据结构（struct file）提供==

close 关于与文件的连接(释放fd)
read 读取文件内容

在一个打开文件中的当前位置保存在 文件位置指针（file pointer）中，这是一个整数，指定了当前位置与文件起始点的偏移量。

对==随机存取文件==而言，该指针可以设置为任何值，只要不超出文件存储容量范围即可
其他文件类型，如命名管道或字符设备的设备文件，不支持这种做法。它们只能从头至尾==顺序读取==。

在文件打开时，可以指定各种标志（如O_RDONLY），用来规定文件的存取模式


### 将文件作为通用接口

### 万物皆文件

使用文件作为其主要通信手段的一部分内核子系统
- 字符和块设备
- 进程之间的管道
- 用于所有网络协议的套接字
- 用于交互式输入和输出的终端。



## VFS的结构

### 结构概览

VFS由两个部分组成：文件和文件系统

1. 文件的表示
inode是内核选择用于表示文件内容和相关元数据的方法。
数据分散到一系列较小的、布局清晰的结构中

![906e14fd81c0cf8bb71f6a9033cc69df.png](../_resources/906e14fd81c0cf8bb71f6a9033cc69df.png)


在抽象对底层文件系统的访问时，并未使用固定的函数，而是使用了==函数指针==。这些函数指针保存在两个结构中，包括了所有相关的函数。
- inode_operations, 创建链接、文件重命名、在目录中生成新文件、删除文件
- file_operations, 作用于文件的数据内容。它们包含一些显然的操作（如读和写），还包括如设置文件位置指针和创建内存映射之类的操作

与每个inode关联的数据段，其中存储了文件的内容或目录项表。
每个inode还包含了一个指向底层文件系统的超级块对象的指针，用于执行对inode本身的操作

==打开==的文件==总是分配==到系统中一个特定的==进程==，内核必须在数据结构中存储文件和进程之间的==关联==。

task_struct包含一个成员，其中保存了所有打开的文件（通过一种迂回方式）。
该成员是一个数组，访问时使用==文件描述符==作为==索引==。
各个数组项包含的对象不仅关联到对应文件的inode，还包含一个指针，指向用于加速查找操作的目录项缓存的一个成员。

VFS支持的文件系统类型通过一种特殊的内核对象连接进来，该对象提供了一种读取==超级块==的方法。
除了文件系统的关键信息（块长度、最大文件长度，等等）之外，==超级块==还包含了读、写、操作inode的函数指针。

内核还建立了一个链表，包含所有活动文件系统的超级块实例。
之所以使用活动（active）这个术语替代已装载（mounted），是因为在某些环境中，有可能使用一个超级块对应几个装载点。

超级块结构的一个重要成员是一个列表，包括相关文件系统中所有修改过的inode（内核相当不敬地称之为脏inode）。
内核会周期性扫描脏块的列表，并将修改传输到底层硬件


### inode

419

fs.h
struct inode {};

这里的inode结构是用于在内存中进行处理，因而包含了一些实际介质上存储的inode所没有的成员。
这些是由内核自身在从底层文件系统读入信息时生成或动态建立

fs.h
struct inode_operations {};

每个inode都有一个i_list成员，可以将inode存储在一个链表中。根据inode的状态，它可能有3种主要的情况
- inode存在内存中，未关联到任何文件，也不处于活动使用状态
- inode结构在内存中，正在由一个或多个进程使用，通常表示一个文件。两个计数器（i_count和i_nlink）的值都必须大于0。文件内容和inode元数据都与底层块设备上的信息相同。也就是说，从上一次与存储介质同步以来，该==inode没有改变==过。
- inode处于活动使用状态。其数据内容已经改变，与存储介质上的内容不同。这种状态的inode被称作脏的。

在fs/inode.c中内核定义了两个全局变量用作表头，
- inode_unused用于有效但非活动的inode（上述第1类），
- inode_in_use用于所有使用但未改变的inode（第2类）。

脏的inode（第3类）保存在一个特定于超级块的链表中

第4种可能性出现得不那么频繁，一般是与一个超级块相关的所有inode都无效时。在检测到可移动设备的介质改变时，此前使用的inode就都没有意义了，另外文件系统重新装载时也会发生这种情况

每个inode不仅出现在==特定于状态的链表==中，还在一个==散列表==中出现，以支持根据inode编号和超级块快速访问inode，这两项的组合在系统范围内是唯一的。

除了散列表之外，inode还通过一个特定于超级块的链表维护，表头是super_block->s_inodes。i_sb_list用作链表元素。

超级块管理了更多的inode链表
内容已经被修改，则列入==脏链表==，表头为super_block->s_dirty，链表元素是i_list
外两个链表（表头为super_block->s_io和super_block->s_more_io）使用同样的链表元素i_list。
这两个链表包含的是已经选中==向磁盘回写==的inode，但正在==等待回写==进行



文件描述符（就是整数）用于在一个进程内唯一地标识打开的文件。
这假定了内核能够在用户==进程中的描述==符和==内核内部使用的结构==之间，建立一种==关联==


sched.h
```C
struct task_struct {
... 
  /* 文件系统信息 */ 
  int link_count, total_link_count; 
... 
  /* 文件系统信息 */ 
  struct fs_struct *fs; 
  /* 打开文件信息 */ 
  struct files_struct *files; 
  /* 命名空间 */ 
  struct nsproxy *nsproxy; 
... 
} 
```

sched.h
```C
struct files_struct { 
  atomic_t count; 
  struct fdtable *fdt; 
  struct fdtable fdtab; 
  int next_fd;  // 下次打开新文件时使用的 fd
  struct embedded_fd_set close_on_exec_init;  // bit map, 执行exec时将关闭的 fd
  struct embedded_fd_set open_fds_init; 
  struct file * fd_array[NR_OPEN_DEFAULT]; 
}; 
```

file.h
```C
struct embedded_fd_set { 
  unsigned long fds_bits[1]; 
}; 
```
。。只是一个 long吗？ 那就 64个，而且还得 < 64

内核允许每个进程打开 NR_OPEN_DEFAULT个文件。该值定义在 include/ linux/sched.h中，==默认值为BITS_PER_LONG==
。。long有几个bit，就默认可以打开几个。 如果比 64大，会怎么处理？
。下页

如果一个进程试图同时打开更多的文件，内核必须对files_struct中用于管理与进程相关的所有文件信息的各个成员，分配更多的内存空间。



最重要的信息包含在fdtab中。

file.h
```C
struct fdtable { 
  unsigned int max_fds; // 进程当前可以处理的文件对象和文件描述符的最大数目。没有固定上限，在需要时 增加。
  struct file ** fd; /* 当前fd_array */ // 指针数组，指向file实例，管理一个 打开的文件的所有信息。 用户空间的 fd 当做 数组index。 数组的当前长度是 max_fds。
  fd_set *close_on_exec; // 指向位域，exec时需要关闭的 fd
  fd_set *open_fds; // 指向位域的 指针，管理者 当前 所有打开的 fd。 最大数目由 max_fdset 指定。
  struct rcu_head rcu; 
  struct files_struct *free_files; 
  struct fdtable *next; 
}; 
```

使用了RCU，所以可以无锁。
。read copy update。 修改时 复制一个副本，然后修改。
。。不过这样 必须保证 只有一个副本啊， 不然 建立2个副本后， 先后更新，保存，第一个更新的 信息 就被 后续的 覆盖掉了啊
。嗯，有限制的

file_struct 中是真正的实例，fdtable中是 指针， 指向了 file_struct 中的实例。

NR_OPEN_DEFAULT设置为BITS_PER_LONG，需要打开更多的文件，内核会分配一个 fd_set 的实例，替换最初的 embedded_fs_set 

posix_types.h
```C
#define __NFDBITS (8 * sizeof(unsigned long)) 
#define __FD_SETSIZE 1024 
#define __FDSET_LONGS (__FD_SETSIZE/__NFDBITS) 
typedef struct { 
unsigned long fds_bits [__FDSET_LONGS]; 
} __kernel_fd_set; 
typedef __kernel_fd_set fd_set;
```

struct embedded_fd_set可以转换为struct fd_set。在这种意义上讲，embedded_ fd_set是fd_set的缩小版，可以同样使用，但占用的空间较小


fs.h
```C
struct file { 
  struct list_head fu_list; 
  struct path f_path; // 文件名和inode之间的关联，文件所在文件系统的有关信息
#define f_dentry f_path.dentry 
#define f_vfsmnt f_path.mnt 
  const struct file_operations *f_op; // 文件操作调用的各个函数
  atomic_t f_count; 
  unsigned int f_flags; 
  mode_t f_mode; // 打开文件时的 模式参数
  loff_t f_pos; // 与文件起始处的字节偏移
  struct fown_struct f_owner; // 处理该文件的进程有关的信息
  unsigned int f_uid, f_gid;  // 用户uid，gid
  struct file_ra_state f_ra; // 预读特征，是否预读，如何预读
  unsigned long f_version; // n由文件系统使用，以检查一个file实例是否仍然与相关的inode内容兼容
... 
  struct address_space *f_mapping; // 指向属于文件相关的inode实例的地址空间映射。通常它设置为inode->i_mapping
... 
}; 
```

namei.h
```C
struct path { 
  struct vfsmount *mnt; 
  struct dentry *dentry; 
};
```


每当内核打开一个文件或做其他的操作时，如果需要file_struct提供比初始值更多的项，则调用expand_files。
该函数检查是否有必要增大数组，如果是这样则调用expand_fdtable。



### 8.3.4 文件操作

各个file实例都包含一个指向struct file_operations实例的指针，该结构保存了指向所有可能文件操作的函数指针

fs.h

```C
struct file_operations { 
  // 仅当文件系统以模块形式装载并未编译到内核中时，才使用owner项。该项指向在内存中表示模块的数据结构。
  struct module *owner; 

  loff_t (*llseek) (struct file *, loff_t, int); 
  
  // 读写数据，参数包括，fd，缓冲区，偏移量，读写的字节数
  ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); 
  ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); 

  // 异步读写
  ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long,  loff_t); 
  ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t); 

  // 读取目录内容，只对目录对象适用
  int (*readdir) (struct file *, void *, filldir_t); 

  // 用于 poll 和 select，以便实现 同步的IO多路复用。
  // select 系统调用 也基于 poll方法，它设置了一个超时限制。
  unsigned int (*poll) (struct file *, struct poll_table_struct *); 

  // 与硬件设备通信，只用于设备文件，其他文件中ioctl 是NULL
  int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); 
  long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); 
  long (*compat_ioctl) (struct file *, unsigned int, unsigned long); 

  // 将文件的内容 映射到 进程的虚拟地址空间
  int (*mmap) (struct file *, struct vm_area_struct *); 
  int (*open) (struct inode *, struct file *); 

  // fd关闭时调用，同时将 计数器-1。网络文件系统需要这个函数，以标记传输结束
  int (*flush) (struct file *, fl_owner_t id); 

  // file对象的 使用计数器 变成0时，调用 release，释放资源
  int (*release) (struct inode *, struct file *); 

  // 由 fsync,fdatasync 系统调用使用，将内存中的文件数据与 存储介质同步
  int (*fsync) (struct file *, struct dentry *, int datasync); 

  int (*aio_fsync) (struct kiocb *, int datasync); 

  // 启用/停用 由信号控制的输入和输出 (通过信号通知进程 文件对象发生了改变)
  int (*fasync) (int, struct file *, int); 

  // ==锁定文件==。用于同步 多个进程的并发文件访问
  int (*lock) (struct file *, int, struct file_lock *); 
  ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); 
  unsigned long (*get_unmapped_area)(struct file *, unsigned long,  unsigned long, unsigned long, unsigned long); 
  int (*check_flags)(int); 
  int (*dir_notify)(struct file *filp, unsigned long arg); 
  int (*flock) (struct file *, int, struct file_lock *); 

  // 用于从管道向文件传输数据，反之亦然。 只用于splice2 系统调用。
  ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); 
  ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); 
}; 
```

- readv,writev 
  用于同名系统调用的实现，用以实现 向量的读取和写入。
  向量本质上是一个结构，用以提供一个==非连续的内存==区域，放置读取的结果或写入的数据。
  该技术称之为快速分散—聚集（fast scatter-gather）。
  它用于避免多次read/write调用，以免降低性能。

- revalidate
  由网络文件系统使用，以确保在介质改变后远程数据的一致性。

- check_media_change
  只适用于设备文件，由于检查在上一次访问以来是否发生了介质改变

- sendfile
  sendfile系统调用，在2个fd 间交换数据。因为socket 也是 fd，所以该函数可以实现 网络上的 简单，高效的数据交换。

两种方法可以指定某个方法不可用
- 将函数指针设置为NULL
- 将函数指针指向一个占位函数，该函数直接返回错误值

---
例如，以下file_operations实例用于块设备
fs/block_dev.c 
```C
const struct file_operations def_blk_fops = { 
  .open = blkdev_open, 
  .release = blkdev_close, 
  .llseek = block_llseek, 
  .read = do_sync_read, 
  .write = do_sync_write, 
  .aio_read = generic_file_aio_read, 
  .aio_write = generic_file_aio_write_nolock, 
  .mmap = generic_file_mmap, 
  .fsync = block_fsync, 
  .unlocked_ioctl = block_ioctl, 
  .splice_read = generic_file_splice_read, 
  .splice_write = generic_file_splice_write, 
}
```

Ext3文件系统使用一个不同的函数集
fs/ext3/file.c 
```C
const struct file_operations ext3_file_operations = { 
  .llseek = generic_file_llseek, 
  .read = do_sync_read, 
  .write = do_sync_write, 
  .aio_read = generic_file_aio_read, 
  .aio_write = ext3_file_write, 
  .ioctl = ext3_ioctl, 
  .mmap = generic_file_mmap, 
  .open = generic_file_open, 
  .release = ext3_release_file, 
  .fsync = ext3_sync_file, 
  .splice_read = generic_file_splice_read, 
  .splice_write = generic_file_splice_write, 
}; 
```

有些指针是相同的，例如以generic_前缀开头的函数。这些是VFS层的通用辅助函数


除了打开文件描述符的列表之外，还必须管理其他特定于进程的数据。因而每个task_struct实例都包含一个指针，指向另一个结构，类型为fs_struct

fs_struct.h
```C
struct fs_struct { 
  atomic_t count; 
  int umask; 
  struct dentry * root, * pwd, * altroot; 
  struct vfsmount * rootmnt, * pwdmnt, * altrootmnt; 
};
```

---
VFS 命名空间

单一的系统可以提供许多容器，但容器中的进程无法感知容器外部的世界，也无法得知所在容器的有关信息。容器彼此完全独立，从VFS角度来看，这意味着需要针对每个容器分别跟踪装载的文件系统。单一的全局视图是不够的。

VFS命名空间是所有已经装载、构成某个容器目录树的文件系统的集合

通常调用fork或clone建立的进程会继承其父进程的命名空间。但可以设置==CLONE_NEWNS==标志，以建立一个新的VFS命名空间

struct task_struct包含的成员nsproxy，该成员负责命名空间的处理

nsproxy.h
```C
struct nsproxy { 
... 
  struct mnt_namespace *mnt_ns; 
... 
}; 
```

mnt_namespace.h
```C
struct mnt_namespace { 
  atomic_t count; // 使用该命名空间的进程数
  struct vfsmount * root; // 指向根目录的 vfsmount 实例
  struct list_head list; // VFS命名空间中所有文件系统的 vfsmount实例
... 
}; 
```

命名空间操作（如mount和umount）并不作用于内核的全局数据结构。相反，它们操作的是当前进程的命名空间实例



### 目录项缓存

块设备较慢，所以需要很长时间才能找到与 文件名关联的 inode。 即使设备数据已经在 页缓存 (16章)，仍然每次都会重复整个查找操作

Linux使用目录项缓存（简称dentry缓存）来快速访问此前的查找操作的结果
该缓存围绕着struct dentry建立

在VFS连同文件系统实现读取的一个目录项（目录或文件）的数据之后，则创建一个dentry实例，以缓存找到的数据

dcache.h
struct dentry {}

dentry结构的主要用途是建立文件名和相关的inode之间的关联。结构中有3个成员用于该目的。
- d_inode
  指向相关的inode实例的指针。
  如果文件名不存在，也会建立 dentry，只是 d_inode 是NULL。
- d_name
  文件名称。qstr是内核的字符串包装器，存储了 char*，长度，==散列值==
  这里并不存储绝对路径，只有路劲的最后一个分量，因为上述链表结构已经映射了目录结构
- d_iname
  如果文件名只由少量字符组成，则保存在d_iname中，加速访问

短文件名的长度上限有 DNAME_INLINE_NAME_LEN 指定，最多不超过16个字符。

内存中所有 ==活动==的dentry实例 都==保存在 散列表中==。
该散列表使用fs/dcache.c中的全局变量dentry_hashtable实现。用d_hash实现的溢出链，用于解决散列碰撞。在下文中，我将该散列表称为全局dentry散列表。

内核中还有另一个dentry的链表，表头是全局变量dentry_unused（也在fs/dcache.c中初始化）。该链表包含哪些项？所有使用==计数器（d_count==）到达==0==（因而任何进程都不再使用）的dentry实例都自动地放置到该链表上。


dentry结构不仅使得易于处理文件系统，对提高系统性能也很关键。他们通过最小化与底层文件系统实现的通信，加速了VFS的处理。
每个由VFS发送到底层实现的请求，都会导致创建一个新的dentry对象，以保存请求的结果。这些对象保存在一个缓存中，在下一次需要时可以更快速地访问，这样操作就能够更快速地执行

dentry对象在内存中的组织，涉及下面两个部分
- 一个散列表（dentry_hashtable）包含了所有的dentry对象
- 一个LRU（最近最少使用，least recently used）==链表==，其中不再使用的对象将授予一个最后宽限期，宽限期过后才从内存移除


在dentry对象的使用计数器（d_count）到达0时，会被置于LRU链表上，这表明没有什么应用程序正在使用该对象。新项总是置于该链表的起始处。换句话说，一项在链表中越靠后，它就越老，这是经典的LRU原理
有时候dentry对象可能临时处于该链表上，尽管这些对象仍然处于活动使用状态，而且其使用计数大于0。这是因为内核进行了一些优化：在LRU链表上的dentry对象==恢复使用时==，==不会立即==将其==从LRU链表移除==，这可以省去一些==锁==操作，从而提高了性能。


## 8.4 处理VFS对象

如何通过同样的接口表示文件和所有其他的对象

### 文件系统操作

。。这一小节 14页。 跳了很多很多

app基本都会用到 文件操作，
但是 文件系统操作 只有少数几个 系统程序使用，比如 mount，umount

文件系统 在内核中 是模块化的， 大约有50个文件系统，都编译到内核中 毫无意义。
因此，每个文件系统在使用前，必须先注册到内核。


注册文件系统
fs/super.c中的register_filesystem用来向内核注册文件系统

装载和卸载
vfsmount结构
mount.h
struct vfsmount {}

超级块管理
装载操作开始于超级块的读取。
fs.h
struct super_block {}
保存了
- 文件系统的块长度
- 文件系统可以处理的最大文件长度
- 文件系统的一般类型的信息
- s_root 将超级块 与 全局根目录的 dentry 关联起来

各个超级块都连接到另一个链表中，表示同一类型文件系统的所有超级块实例

fs.h
struct super_operations {}


mount，umount
mount系统调用的入口点是sys_mount函数，其定义在fs/namespace.c中
文件系统通过umount系统调用卸载，其入口点是fs/namespace.c中的sys_umount

共享子树


内核也提供了一些基础设施，允许装载自动过期。
在任何进程或内核本身都未使用某个装载时，如果使用了自动过期机制，那么该装载将自动从vfsmount树中移除。


伪文件系统
伪文件系统是不能装载的文件系统，因而不可能从用户层直接看到。


### 文件操作

查找inode
根据给定的文件名查找inode

fs/namei.c 
```C
int fastcall path_lookup(const char *name, unsigned int flags, 
struct nameidata *nd)
```
。。nd 是输入输出形参

fs.h
```C
struct nameidata { 
  struct dentry *dentry;
  struct vfsmount *mnt; // 查找完成后， 和dentry 一起 被填充数据
  struct qstr last;  // 查找所需的名称，是一个快速字符串 (quick string)，包含字符串，长度，hash值
  unsigned int flags;  // 标志，用于 微调 查找操作。
... 
} 
```

查找必须考虑
- 一个文件可能通过==符号链接==引用另一个文件，需要识别出循环
- 必须检测装载点，而后据此重定向查找操作
- 必须检查所有目录的==访问权限==
- ==格式==奇怪但正确的名称，如/./usr/bin/../local/././bin//emacs



打开文件
从应用程序的角度来看，这是通过标准库的open函数完成的，该函数返回一个文件描述符。
该函数使用了同名的open系统调用，调用了fs/open.c中的sys_open函数。


读写
在文件成功打开之后，进程将使用内核提供的read或write系统调用，来读取或修改文件的数据。
照例，入口例程是sys_read和sys_write，二者都在fs/read_write.c中实现。

read函数需要3个参数：文件描述符、保存数据的缓冲区和指定读取字符数目的长度参数。这些参数直接传递到内核中。

sys_write与sys_read的参数相同：一个文件描述符、一个指针变量、一个长度
指示（表示为整数）。
显然，write语义稍有不同。指针并非指向存储读取数据的缓冲区，而是指向需要写入文件的数据。长度参数指定了数据的字节长度



## 标准函数

这些操作对所有文件系统来说，在一定程度上都是相同的。
如果数据所在的块是已知的，则首先查询页缓存。如果数据并未保存在其中，则向对应的块设备发出读请求。

大多数文件系统在其file_operations实例中，都将read和write分别指向do_sync_read和do_sync_write标准例程


读取
```C
// mm/filemap.c 
ssize_t do_sync_read(struct file *filp, char __user *buf, size_t len, loff_t *ppos) 
{ 
  struct iovec iov = { .iov_base = buf, .iov_len = len }; 
  struct kiocb kiocb; 
  ssize_t ret; 
  init_sync_kiocb(&kiocb, filp); 
  kiocb.ki_pos = *ppos; 
  kiocb.ki_left = len; 
  ret = filp->f_op->aio_read(&kiocb, &iov, 1, kiocb.ki_pos); 
  if (-EIOCBQUEUED == ret) 
    ret = wait_on_sync_kiocb(&kiocb); 
  *ppos = kiocb.ki_pos;
  return ret; 
} 
```


mm/filemap.c中的generic_file_aio_read异步读取数据


do_generic_mapping_read 从映射读取


内存映射通常调用由VFS层提供的filemap_fault标准例程来读取未保存在缓存中的页

权限检查
fs/namei.c 
`int permission(struct inode *inode, int mask, struct nameidata *nd)`





# ch09 Ext文件系统族 464

# ch10 无持久存储的文件系统 512

proc保存了
- 内存管理；
- 系统进程的特征数据；
- 文件系统；
- 设备驱动程序；
- 系统总线；
- 电源管理；
- 终端；
- 系统控制参数。


sysfs
sysfs是一个向用户空间导出内核对象的文件系统，它不仅提供了察看内核内部数据结构的能力，还可以修改这些数据结构。


# ch11 扩展属性和访问控制表 565





# ch12 网络 586

网络功能的实现是内核最复杂、牵涉最广的一部分。除了经典的因特网协议（如TCP、UDP）和相关的IP传输机制之外，Linux还支持许多其他的互联方案，使得所有想得到的计算机/操作系统能够互操作。Linux也支持大量用于数据传输的硬件，如以太网卡和令牌环网适配器及ISDN卡和调制解调器，但这并没有使内核的工作变得简单

include/net

- 如何建立物理连接？使用何种线缆？通信介质有哪些限制和特殊要求？
- 如何处理传输错误？
- 如何识别网络中的每一台计算机？
- 如果两台计算机通过其他计算机连接，那么二者之间的数据交换如何进行？如何查找最佳的路由？
- 如何打包数据，使之不依赖于特定计算机的特性？
- 如果一台计算机提供了几个网络服务，如何识别这些服务？



## 12.2 ISO/OSI 和 TCP/IP 参考模型

![31341e4b602bc919b33e3c1aa71f8890.png](../_resources/31341e4b602bc919b33e3c1aa71f8890.png)




## 12.3 通过套接字通信

套接字现在用于定义和建立网络连接，以便可以用操作inode的普通方法（特别是读写操作）来访问网络

在创建套接字时，不仅要区分==地址和协议族==，还要区分基于==流==的通信和==数据报==的通信。（对面向流的套接字来说）同样重要的一点是，套接字是==为客户端==程序建立的，还是==为服务器==程序建立的。


### 12.3.1 创建套接字

在创建套接字时，必须指定所需要的==地址==和==协议==类型的组合。

套接字是使用socket库函数生成的

可使用第三个参数来选择协议。这是不必要的，因为前两个参数已经唯一地定义了协议。将第三个参数指定为0，即通知函数使用适当的默认协议。

在调用socket函数后，套接字地址的格式（或它属于哪个地址族）已经很清楚，但尚未给套接字分配本地地址。

bind函数用于该目的，必须向该函数传递一个sockaddr_type结构作为参数。该结构定义了本地地址。

因特网地址由IP地址和端口号唯一定义，这也是sockaddr_in定义为下列形式的原因：
in.h
```C
struct sockaddr_in {
  sa_family_t sin_family; /* 地址族 */ 
  __be16 sin_port; /* 端口号 */   // big end 大端
  struct in_addr sin_addr; /* 因特网地址 */ 
... 
} 
```

IP地址不能使用常见的点分十进制记法，而必须以数字形式指定。
库函数inet_aton可以将一个ASCII字符串格式（点分十进制）的IP地址转换为内核（和C库）所需的格式。例如，地址192.168.1.20的数字表示是335653056。

为确保不同字节序的机器之间能够彼此通信，显式定义了一种网络字节序（network byte order），它等价于大端序格式。

协议首部出现的数值都必须使用网络字节序。IP地址和端口号实际上都是数字，因而在定义sockaddr_in结构中的数值时，必须考虑到这个事实。

内核提供了几种数据类型。__be16、__be32和__be64分别表示位长为16、32、64位的大端序数据类型，而前缀为__le的变体则表示对应的小端序数据类型。


### 12.3.2 使用套接字

2个例子，echo客户端和echo服务器
客户端
```C
#include<stdio.h> 
#include<netinet/in.h> 
#include<sys/types.h> 
#include<string.h> 
int main() { 
  /* echo服务器的主机地址和端口号 */ 
  char* echo_host = "192.168.1.20"; 
  int echo_port = 7;    // . 端口号:7 服务:Echo,访问自己的服务器需要修改
  int sockfd; 
  struct sockaddr_in *server = (struct sockaddr_in*)malloc(sizeof(struct sockaddr_in)); 
  /* 设置将要连接的服务器的地址 */ 
  server->sin_family = AF_INET; 
  server->sin_port = htons(echo_port); // 注意，是网络字节序！ 
  server->sin_addr.s_addr = inet_addr(echo_host); 


  /* 创建套接字（因特网地址族、流套接字和默认协议） */ 
  sockfd = socket(AF_INET, SOCK_STREAM, 0); 


  /* 连接到服务器 */ 
  printf("Connecting to %s \n", echo_host); 
  printf("Numeric: %u\n", server->sin_addr); 
  connect(sockfd, (struct sockaddr*)server, sizeof(*server)); 


  /* 发送消息 */ 
  char* msg = "Hello World"; 
  printf("\nSend: '%s'\n", msg); 
  write(sockfd, msg, strlen(msg)); 


  /* 接收返回结果 */ 
  char* buf = (char*)malloc(1000); // 接收用的缓冲区，最大为1000个ASCII字符 
  int bytes = read(sockfd, (void*)buf, 1000); 
  printf("\nBytes received: %u\n", bytes); 
  printf("Text: '%s'\n", buf); 


  /* 结束通信，即关闭套接字*/ 
  close(sockfd); 
} 
```
因特网超级守护进程（inetd、xinetd或其他类似程序）通常使用内建的echo服务器。因此，上述源代码在编译之后可以立即测试

- 创建sockaddr_in实例，用来描述要连接的服务器的地址，AF_INET表明它是一个
因特网地址
- 通过socket函数在内核中创建一个套接字，该函数基于内核提供的socketcall系统调用（下文会说明这一点）。返回的结果是一个整数，可解释为文件描述符
- 对套接字文件描述符和server变量调用connect函数（也基于socketcall系统调用），即可建立到服务器的连接，server变量存储服务器连接数据。 
- 用write向服务器发送一个文本串
- 通过read读取服务器的响应


echo服务器
```C
#include<stdio.h> 
#include<netinet/in.h> 
#include<sys/types.h> 
#include<string.h> 
int main() { 
  char* echo_host = "192.168.1.20"; 
  int echo_port = 7777; 
  int sockfd; 
  struct sockaddr_in *server = (struct sockaddr_in*)malloc(sizeof(struct sockaddr_in)); 
  /* 设置自身地址 */ 
  server->sin_family = AF_INET; 
  server->sin_port = htons(echo_port); // 注意，是网络字节序！
  server->sin_addr.s_addr = inet_addr(echo_host); 
  /* 创建套接字 */ 
  sockfd = socket(AF_INET, SOCK_STREAM, 0); 
  /* 绑定到一个地址 */ 
  if (bind(sockfd, (struct sockaddr*)server, sizeof(*server))) { 
    printf("bind failed\n"); 
  } 
  /* 启用套接字的服务器模式（即开始监听） */ 
  listen(sockfd, SOMAXCONN); 
  /* 等待客户端发送的数据进入 */ 
  int clientfd; 
  struct sockaddr_in* client = (struct sockaddr_in*)malloc(sizeof(struct sockaddr_in)); 
  int client_size = sizeof(*client); 
  char* buf = (char*)malloc(1000); 
  int bytes; 
  printf("Wait for connection to port %u\n", echo_port); 
  /* 接受连接请求 */ 
  clientfd = accept(sockfd, (struct sockaddr*)client, &client_size); 
  printf("Connected to %s:%u\n\n", inet_ntoa(client->sin_addr), 
  ntohs(client->sin_port)); 
  printf("Numeric: %u\n", ntohl(client->sin_addr.s_addr)); 
  while(1) { /* 无限循环 */ 
    /* 接收传输的数据 */ 
    bytes = read(clientfd, (void*)buf, 1000); 
    if (bytes <= 0) { 
      close(clientfd);
      printf("Connection closed.\n");
      exit(0);
    } 
    printf("Bytes received: %u\n", bytes); 
    printf("Text: '%s'\n", buf); 
    /* 发送响应数据 */ 
    write(clientfd, buf, bytes); 
  } 
} 
```

服务器并不会主动与另一个程序建立连接，服务器只会被动地等待
建立一个被动连接需要以下三个库函数
- bind 将套接字绑定到一个地址
- listen 通知套接字被动地等待客户端连接请求的到来
- accept 接受等待队列上第一个客户端的连接请求，在队列为空时，该函数将阻塞


一个四元组（192.168.1.20:7777, 192.168.1.10:3506）用来唯一标识一个连接。

如果元组中某个分量仍然是未定的，则用星号`*`表示。因而，在被动套接字上监听尚未有客户端连接的服务器进程，可以表示为`192.168.1.20:7777, *:*`。

netstat工具可以显示并检查系统上所有TCP/IP连接的状态


### 12.3.3 数据报套接字

UDP表示User Datagram Protocol（用户数据报协议）

在如下几个基本方面与TCP有所不同。
- UDP是面向分组的。在发送数据之前，无须建立显式的连接。
- 分组可以在传输期间丢失。不保证数据一定能到达其目的地。
- 分组接收的次序不一定与发送的次序相同。

UDP通常用于视频会议、音频流及类似的服务。在此类环境下，丢失几个分组并不要紧

分别使用TCP和UDP协议的进程，可以==同时==使用==同样==的IP地址和端口号
在多路复用时，内核会根据分组的传输协议类型，将其转发到适当的进程



## 12.4 网络实现的分层模型

![7cf8d22c51eba4a13aafba2aa93cf98c.png](../_resources/7cf8d22c51eba4a13aafba2aa93cf98c.png)

网络子系统是内核中涉及面最广、要求最高的部分之一。为什么是这样呢?

该子系统处理了大量特定于协议的细节和微妙之处，穿越各层的代码路径中有大量的函数指针，而没有直接的函数调用。
这是不可避免的，因为各个层次有多种组合方式，这显然不会使代码路径变得更清楚或更易于跟踪。此外，其中涉及的数据结构通常彼此紧密关联。
。。myserver 中招了。 确实 函数指针 满天飞

以太网帧由下列 顺序 组成
Mac首部 IP首部 TCP首部 HTTP首部 HTTP数据


## 12.5 网络命名空间

照例需要一个中枢结构来跟踪 所有可用的 命名空间。

```C
// include/net/net_namespace.h 
struct net { 
  atomic_t count; /* 用于判断何时释放网络命名空间 */ 
... 
  struct list_head list; /* 网络命名空间的链表 */ 
... 
  struct proc_dir_entry *proc_net; 
  struct proc_dir_entry *proc_net_stat; 
  struct proc_dir_entry *proc_net_root; 
  struct net_device *loopback_dev; /* 环回接口设备 */ 
  struct list_head dev_base_head; 
  struct hlist_head *dev_name_head; 
  struct hlist_head *dev_index_head; 
}; 
```

## 12.6 socket缓冲区
```C
// <skbuff.h> 
struct sk_buff { 

};
```

套接字缓冲区用于在网络实现的各个层次之间交换数据，而无须来回复制分组数据，对性能的提高很可观。
套接字结构是网络子系统的基石之一，因为在产生和分析分组时，在各个协议层次上都需要处理该结构

。。所以myserver 使用 TcpConnection 来 代替 socket？ 。 不，这个要看下 fd 是怎么申请的。


### 使用socket缓冲区管理数据
套接字缓冲区通过其中包含的各种指针与一个内存区域相关联，网络分组的数据就位于该区域中

下图是32位系统

![baa3334d7130e012a2bf9d093844e2cb.png](../_resources/baa3334d7130e012a2bf9d093844e2cb.png)

。。左侧的指针 是 sk_buff 结构中的。

- head,end 指向 数据在内存中的 起止位置
- data,tail 指向 协议数据的 起止
- mac_header 指向 MAC协议 首部的起始，network_header,transport_header指向 网络层，传输层 协议首部的 起始。

tcp_hdr, udp_hdr 方法 用于 从 套接字缓冲区 获得 TCP,UDP 首部。
```C
<tcp.h> 
static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb) 
{ 
  return (struct tcphdr *)skb_transport_header(skb); 
} 
```
tcphdr 结构在 12.9.2 讨论。

还有其他类似的 转换函数，比如 ip_hdr


在不同协议层 之间传递数据时，只需要修改 data 的位置即可：

![a48c473f8f4aaadc40ed24e19a67896d.png](../_resources/a48c473f8f4aaadc40ed24e19a67896d.png)


在一个新分组产生时，TCP层首先在用户空间中分配内存来容纳该分组数据（首部和净荷）。
分配的空间大于数据实际需要的长度，因此较低的协议层可以 进一步 增加 首部

内核提供了一些用于操作套接字缓冲区的标准函数
alloc_skb
skb_copy
skb_clone
skb_tailroom
skb_headroom
skb_realloc_headroom

---

套接字缓冲区需要很多指针来表示缓冲区中内容的不同部分。
由于网络子系统必须保证较低的内存占用和较高的处理速度，因而对struct sk_buff来说，我们需要保持该结构的长度尽可能小。
在64位 CPU上，可使用一点小技巧来节省一些空间。sk_buff_data_t的定义改为整型变量：
```C
// <skbuff.h> 
typedef unsigned int sk_buff_data_t;
```
由于在此类体系结构上，整型变量占用的内存只有指针变量的一半（前者是4字节，后者是8字节），该结构的长度缩减了20字节。
但套接字缓冲区中包含的信息仍然是同样的。
data和head仍然是常规的指针，而所有sk_buff_data_t类型的成员现在都解释为 相对于 前两者的==偏移量==

4字节偏移量足以描述长达4 GiB的内存区，套接字缓冲区不可能超过这个长度。
。。2字节，或3字节呢。

由于假定套接字缓冲区的内部表示对通用网络代码是不可见的，所以提供了如下几个辅助函数来访问struct sk_buff的成员。
这些函数都定义在 skbuff.h 中
- skb_transport_header(const struct sk_buff *skb)，获得 传输层首部的地址
- skb_reset_transport_header(struct sk_buff *skb)，将传输层首部重置为数据部分的起始位置
- skb_set_transport_header(struct sk_buff *skb, const int offset)，根据数据部分中给定的偏移量来设置传输层首部的 起始地址

对MAC层和网络层首部来说，也有同样一组函数可用，只需将transport分别替换为mac或network即可。


### 12.6.2 管理socket缓冲区数据
一些重要成员
- tstamp，分组到达的时间
- dev，指定了处理分组的网络设备
- iif，输入设备的接口索引号
- sk，指针，指向用于 处理该分组的 socket实例
- dst，表示接下来 该分组通过内核网络实现的路由
- next,prev，将socket 缓冲区 保存到一个 双链表


## 12.7 网络访问层

网络实现的 第一层，网络访问层，负责在计算机之间传输信息，与网卡的设备驱动程序直接协作

每个网络设备都 表示为 net_device 的一个实例。
创建并初始化 net_device 后，需要使用 net/core/dev.c 中的 register_netdev 函数 将其注册到 内核。 这会创建一个 sysfs 项。

内核如何跟踪可用的网络设备，如何查找？
这些设备不是全局的，而是按照 命名空间进行管理的。 每个命名空间(net实例)中，有如下3个 机制可用
- 所有网络设备都保存在一个 单链表中，表头为 dev_base
- 按设备名散列， dev_get_by_name(struct net * net, const char * name) 根据设备名在 该散列上 查找网络设备
- 按接口索引散列， dev_get_by_index(struct net * net, int ifindex) 根据给定的接口索引查找net_device实例

netdevice.h
struct net_device 

结构中出现的 rx，tx 代表 receive, transmit，是 接收和发送

net_device 中的一些成员 定义了 与网络层 和网络访问层相关的设备属性
- ==mtu==，max transfer unit 最大传输单元，指 一个传输帧的最大长度。
- type，设备的硬件类型
- dev_addr，设备的硬件地址 (如以太网卡的 MAC地址)，addr_len 指定该地址的长度。 broadcast 用于向附接的所有站点发送消息的 广播地址
- ip_ptr,ip6_ptr,atalk_ptr，指向 特定与协议的数据
- open,stop，初始化和 终止 网卡。通常在 内核外部 通过 ifconfig命令触发
- hard_start_xmit，用于从等待队列删除 已经完成的分组 并将其发送出去
- header_ops，指向结构的指针，该结构提供了更多的函数指针，用于操作硬件首部，最重要的是 create创建硬件首部，parse分析硬件首部
- get_stats，查询统计数据，并将数据封装到 net_device_stats
- tx_timeout，解决分组传输失败的问题
- do_ioctl，将特定于 设备的命令发送到网卡
- nd_det，指向 设备所属的 网络命名空间 (struct net 的一个实例)
- change_mtu，用于修改 mtu。许多网卡不允许这样做。

注册网络设备

---
12.7.2 接收分组

分组到达内核的时间是 不可预测的。 所有现代的 设备驱动程序都使用 ==中断== 来通知内核(或 系统) 有分组到达。
网络驱动程序 对特定于设备的 ==中断设置了一个 处理例程==，每当 该==中断 被引发==，==内核都调用 该处理程序==，将 数据==从网卡 传输到 物理内存==， ==或==通知内核 在==一定时间后进行处理==

几乎所有网卡都支持 DMA，能够==自行将 数据传输到 物理内存==。但这些数据 仍需要 解释 和处理。


内核为分组的接收 提供了2个框架。
传统方法，在与 超高速网络适配器协作时，会出问题。
所以有了一种新的 API，称为 NAPI

传统方法比较易于理解

下图是 一个分组到达 网络适配器后，该分组 穿过内核 到达 网络层函数的 路径。

![a6aa4cf0d9dc5ca8612d5839abe27b38.png](../_resources/a6aa4cf0d9dc5ca8612d5839abe27b38.png)

- net_interrupt，由设备驱动程序设置的中断处理程序。它将确定该中断是否真的是由 接收到的分组引发的 (可能是由于 报告错误或 传输任务导致的)。 如果确实如此，则调用 net_rx
- net_rx函数 也是特定于网卡的，首先==创建一个 新的 socket 缓冲区==。然后，分组的内容 从网卡传输到 缓冲区 (也就是进入物理内存)， 然后使用 内核源码中 针对 各种传输类型的库函数 来分析首部数据。这项分析 将==确定 分组所使用的网络层协议==，如IP协议
- netif_rx 不是特定于网络驱动程序的，该函数位于 net/core/dev.c。调用该函数，标志着 控制 由特定于网卡的代码 转移到 网络层的通用接口部分。
  该函数的作用在于，将接受到的分组 放置在 一个特定于CPU的等待队列上，并退出上下文，使得CPU可以执行其他任务。

内核在全局定义的 softnet_data 数组中 管理进出分组的等待队列。 
为了提高多处理器系统的性能，对每个CPU都会创建等待队列，支持分组的并行处理

netif_rx在结束工作之前将软中断NET_RX_SOFTIRQ标记为即将执行（更多信息请参考第14章），然后退出中断上下文。
net_rx_action用作该软中断的处理程序
- __skb_dequeue从等待队列移除一个套接字缓冲区，该缓冲区管理着一个接收到的分组。
- netif_receive_skb函数分析分组类型，以便根据分组类型将分组传递给网络层的接收函数
- deliver_skb函数使用一个特定于分组类型的处理程序func，承担对分组的更高层（例如互联网络层）的处理。

所有用于从底层的网络访问层接收数据的网络层函数都注册在一个散列表中，通过全局数组ptype_base实现
新的协议通过dev_add_pack增加。各个数组项的类型为struct packet_type


---
对高速接口的支持

每次一个以太网帧到达时，都使用一个IRQ来通知内核。这里暗含着“快”和“慢”的概念。 
对低速设备来说，在下一个分组到达之前，IRQ的处理通常已经结束。

由于下一个分组也通过IRQ通知，如果前一个分组的IRQ尚未处理完成，则会导致问题，高速设备通常就是这样。

。。IRQ 估计是 中断

现代以太网卡的运作高达10 000 Mbit/s，如果使用旧式方法来驱动此类设备，将造成所谓的“==中断风暴==”。
如果在分组等待处理时接收到新的IRQ，内核不会收到新的信息：在分组进入处理过程之前，内核是可以接收IRQ的，在分组的处理结束后，内核也可以接收IRQ，这些不过是“旧闻”而已。
为解决该问题，NAPI使用了==IRQ和轮询==的组合。

- 第一个分组将导致网络适配器发出IRQ。为防止进一步的分组导致发出更多的IRQ，驱动程序会==关闭该适配器的Rx IRQ==。并将该适配器放置到一个==轮询表==上。
- 只要适配器上还有分组需要处理，内核就一直对轮询表上的设备进行==轮询==
- 重新启用Rx中断

NAPI的另一个优点是可以高效地丢弃分组。如果内核确信因为有很多其他工作需要处理，而导致无法处理任何新的分组，那么网络适配器可以直接丢弃分组，无须复制到内核。

只有设备满足如下两个条件时，才能实现NAPI方法。
(1) 设备必须能够保留多个接收的分组，例如保存到DMA环形缓冲区中。下文将该缓冲区称为Rx缓冲区。
(2) 该设备必须能够禁用用于分组接收的IRQ。而且，发送分组或其他可能通过IRQ进行的操作，都仍然必须是启用的

如果系统中有多个设备，会怎么样呢？这是通过循环轮询各个设备来解决的
![614b757c6f846d0c878b92904f2bd185.png](../_resources/614b757c6f846d0c878b92904f2bd185.png)


内核以循环方式处理链表上的所有设备：内核依次轮询各个设备，如果已经花费了一定的时间来处理某个设备，则选择下一个设备进行处理。
此外，某个设备都带有一个相对==权重==，表示与轮询表中其他设备相比，该设备的相对重要性。

与旧的API相比，关键性的变化在于，支持NAPI的设备必须提供一个poll函数。
该方法是特定于设备的，在用netif_napi_add注册网卡时指定。调用该函数注册，表明设备可以且必须用新方法处理。

```C
<netdevice.h> 
static inline void netif_napi_add(struct net_device *dev, 
                struct napi_struct *napi, 
                int (*poll)(struct napi_struct *, int), 
                int weight); 
```
poll指定了在IRQ禁用时用来轮询设备的函数

weight, 通常10/100 Mbit网卡的驱动程序指定为16，而1 000/10 000 Mbit网卡的驱动程序指定为64。无论如何，权重都==不能超过==该设备可以在==Rx缓冲区中存储的分组的数目==

poll函数需要两个参数：一个指向napi_struct实例的指针和一个指定了“预算”的整数，预算表示内核允许驱动程序处理的分组数目。

---
12.7.3 发送分组

net/core/dev.c中的dev_queue_xmit用于将分组放置到发出分组的队列上

只要知道，在分组放置到等待队列上一定的时间之后，分组将发出即可。
这是通过特定于适配器的函数hard_start_xmit完成的，在每个net_device结构中都以函数指针的形式出现，由硬件设备驱动程序实现


## 12.8 网络层

网络层（具体地说是IP协议）与网络适配器的硬件性质几乎是完全分离的。

该层不仅负责==发送和接收数据==，还负责在彼此不直接连接的系统之间==转发和路由分组==。
查找最佳路由并选择适当的网络设备来发送分组，也涉及对底层地址族的处理（如特定于硬件的MAC地址），这是该层至少要与网卡松散关联的原因。

如果不考虑底层硬件，是无法将较大的分组分割为较小单位的（事实上，硬件的性质是需要分割分组的首要原因）。
因为每一种传输技术所支持的分组长度都有一个最大值，==IP协议==必须方法将较大的分组==划分为较小==的单位，由接收方==重新组合==，更高层协议不会注意到这一点。
划分后分组的长度取决于特定传输协议的能力。


### 12.8.1 IPv4

![60dfad52518457604204efe54c90603f.png](../_resources/60dfad52518457604204efe54c90603f.png)

- version(版本) 指定了IP的版本，当前，有效值是 4或6。
- IHL (IP首部长度) 定义了首部的长度
- 代码点(codepoint) 或 服务类型 (type of service) 用于更复杂的协议选项
- Length(长度)  代表分组的总长度，即 首部+数据的长度
- fragment ID (分片标识) 标识了一个分片的 IP分组的各个部分。分片方法将同一个分片ID指定到 同一原始分组的各个数据片，使之可以标识为同一分组的成员。各个部分的相对位置由fragment offset（分片偏移量）字段定义。偏移量的单位是64 bit。
- 有3个状态标志位用于启用或禁用特定的特性，目前只使用了其中2个。
  - DF，don't fragment，即 指定分组 不可拆分为 更小的单位
  - MF，表示当前分组 是一个更大分组的分片，后面还有其他分片 (除了最后一个分片之外，所有分片都会设置 该标志位)
- TTL，time to live，从发送者到接受者的 传输路径上 中间节点 的最大数目
- Protocol 标识了IP分组 承载的 高层协议(传输层)。例如TCP，UDP
- checksum，校验和，根据首部 和 数据的内容计算。 如果校验和出错，则丢弃分组
- options用于扩展IP选项
- data 保存了分组数据 (净荷)

IP首部中 所有数值都是 网络字节序 (大端序)

在内核源代码中，该首部由iphdr数据结构实现：
ip.h
struct iphdr


ip_rcv函数 是网络层的入口点。分组向上穿过内核的路线如图

![8203caf7f5a58c127448ce2abd47ff4b.png](../_resources/8203caf7f5a58c127448ce2abd47ff4b.png)


12.8.2 接收分组

分组转发到 ip_rcv 之后，必须检查 收到的消息，确保它是正确的，主要检查计算的校验和 是否和 首部中的 一致， 还检查 分组是否达到了 IP首部的最小长度，分组的协议是否是IPv4
检查完成后，内核并不立即 继续对分组 进行处理， 而是调用 netfilter 挂钩，使得==用户空间 可以对分组数据进行操作==。

netfilter挂钩插入到内核源代码中定义好的各个位置，使得分组能够被外部动态操作。

在内核到达一个挂钩位置时，将在用户空间调用对该标记支持的例程。接下来，在另一个内核函数中继续内核端的处理（分组可能被修改过）。

在下一步中，接收到的分组到达一个十字路口，此时需要判断该分组的目的地是本地系统还是远程计算机。根据对分组目的地的判断，需要将分组转发到更高层，或转到互联网络层的输出路径上（这里不打算讨论第三种选项，即通过多播将分组发送到一组计算机）
。。这岂不是可以秒杀哪个 什么 A 往B发数据，但是不知道B的地址 的问题？。也不是，转发给谁？ 怎么判断是不是我接收？

ip_route_input负责选择路由。这个相对复杂的决策过程在12.8.5节详细讨论

ip_local_deliver和ip_forward 用于处理 交付到本地计算机下一个更高层协议，还是 转发到 另一台计算机


12.8.3 交付到本地传输层

如果分组的目的地是本地计算机，ip_local_deliver必须设法找到一个适当的传输层函数，将分组转送过去。
IP分组通常对应的传输层协议是TCP或UDP

分片合并
该函数的第一项任务，就是通过ip_defrag重新组合分片分组的各个部分

内核在一个独立的缓存中管理原本属于一个分组的各个分片，该缓存称为分片缓存（fragment cache）。
在缓存中，属于同一分组的各个分片保存在一个独立的等待队列中，直至该分组的所有分片都到达

在分组的分片合并完成后，调用netfilter挂钩NF_IP_LOCAL_IN，恢复在ip_local_deliver_finish函数中的处理。
在其中，根据分组的协议标识符确定一个传输层的函数，将分组传递给该函数。所有基于互联网络层的协议都有一个net_protocol结构的实例
include/net/protocol.h 
struct net_protocol
- handler是协议例程，分组将（以套接字缓冲区的形式）被传递到该例程进行进一步处理。
- 在接收到ICMP错误信息并需要传递到更高层时，需要调用err_handler。


12.8.4 分组转发


12.8.5 发送分组
内核提供了几个通过互联网络层发送数据的函数，可由较高协议层使用。其中==ip_queue_xmit==是最常使用的一个

第一个任务是查找可用于该分组的路由。
内核利用了下述事实：起源于同一套接字的所有分组的目标地址都是相同的，这样不必每次都重新确定路由。

转移到网络访问层
ip_output


### 分组分片
![0812cfc1ff0e2fc7bb5e7029f9e5e40b.png](../_resources/0812cfc1ff0e2fc7bb5e7029f9e5e40b.png)

如果忽略RFC 791中记载的各种微妙情形，那么IP分片是非常简单的。
在循环的每一轮中，都抽取出一个数据分片，其长度与对应的MTU兼容。
创建一个==新套接字缓冲区==来保存抽取的数据分片，旧的IP首部可以==稍作修改后重用==。
所有的分片都会分配一个==共同的分片ID==，以便在目标系统上==重新组装分组==。
分片的==顺序==基于分片偏移量建立，此时也需要适当地设置。
==MF==（more fragments）标志位也需要设置。
只有序列中的==最后一个分片可以将该标志位置0==。
每个分片都在使用ip_send_check产生校验和之后，用ip_output发送


路由


### 12.8.6 netfilter

netfilter是一个==Linux内核框架==，使得可以根据==动态定义的条件==来==过滤和操作分组==。
这显著增加了可能的网络选项的数目，从简单的防火墙，到对网络通信数据的详细分析，到复杂的、依赖于状态的分组过滤器。
由于netfilter的精巧设计，网络子系统只需要少量代码就可以达到上述目的。

扩展网络功能
netfilter框架向内核添加了下列能力
- 根据状态及其他条件，对不同数据流方向 (进，出，转发) 进行分组过滤
- ==NAT==，根据某些规则 来转换 源地址 和目标地址
- 分组处理 和操作， 根据特定的规则拆分和 修改分组

可以通过在运行时向内核载入模块来增强netfilter功能。

==iptables==由管理员用来==配置防火墙、分组过滤器和类似功能==，这些只是==建立在netfilter框架上的模块==，它提供了一个功能全面、定义良好的库函数集合，以便分组的处理


调用挂钩函数
netfilter挂钩通过 netfilter.h 中的NF_HOOK宏调用。
宏参数语义
- pf，调用的netfilter 挂钩 源自哪个协议族， IPv4层的所有调用都使用 PF_INET
- hook 是挂钩编号，netfilter_ipv4.h 定义了 可能的值。
- skb 是所处理的套接字缓冲区
- indev，outdev， 指向网络设备的 net_device
- okfn，在 netfilter 挂钩结束时执行

扫表挂钩表

激活挂钩函数


### 12.8.7 IPv6

![27dd154aa607f021001ad7a3ee066c5f.png](../_resources/27dd154aa607f021001ad7a3ee066c5f.png)

尽管IPv6也支持将分组数据划分为更小的单元，但相关信息保存在一个 扩展首部 中，next header字段即指向该首部。




## 12.9 传输层

### UDP
ip_local_deliver负责分发IP分组传输的数据内容

net/ipv4/udp.c中的udp_rcv用于进一步处理UDP数据报

![cfcc91361f6dcb954d98ff5675c64bdd.png](../_resources/cfcc91361f6dcb954d98ff5675c64bdd.png)



![f794eb7ab9aa2450602d16173c6b58ab.png](../_resources/f794eb7ab9aa2450602d16173c6b58ab.png)

UDP分组的首部在内核中由下列数据结构表示：
udp.h
```C
struct udphdr { 
  __be16 source; 
  __be16 dest; 
  __be16 len; 
  __be16 check; 
}; 
```

在有某个监听进程对该分组感兴趣时，在udphash全局数组中则会有与分组目标端口匹配的sock结构实例，__udp4_lib_lookup可采用散列方法查找并返回该实例。
如果找不到这样的套接字，则向源系统发送一个“==目标不可到达==”的消息，并丢弃分组的内容

内核中有两种数据结构用于表示套接字。
==sock==是到网络访问层的接口，
而==socket==是到用户空间的接口。


### TCP

TCP协议的3个主要部分（连接建立、连接终止和数据流的按序传输）

TCP连接总处于某个明确定义的状态。这些状态包括上文提到的listen和established状态。还有其他状态，以及明确定义的规则，用于各个状态之间的迁移，如图


![451a4e16b3641e3ac43fcda0a6dfa921.png](../_resources/451a4e16b3641e3ac43fcda0a6dfa921.png)

上图包含的信息几乎完全描述了TCP实现的行为

内核可以区分各个状态并实现状态间的迁移（使用称为有限状态机的工具）。
但这既不特别高效，也不快速，因此内核采用了一种不同的方法。


TCP首部

![1e7b87131cbdf7327d9fd42460a06608.png](../_resources/1e7b87131cbdf7327d9fd42460a06608.png)

- source,dest，端口号，各占 2字节
- seq，序列号，指定了 TCP分组在 数据流中的位置， 在需要重传时很重要
- ack_seq，确认收到的TCP分组
- doff，data offset，数据偏移量，指定了 TCP首部结构的长度。
- reserved 保留， 不可用，因而总是0
- urg,ack,psh,rst,syn,fin，控制标志，用于 检查，建立 和 结束 连接
- window，窗口，告诉 另一方，在 本方的 接收缓冲区满之前，可以发送多少字节。
- checksum，校验和
- option 可变长列表，包含了 额外的连接选项
- 实际数据在 首部之后， 需要==补齐，因为 数据必须起始于 32位边界位置==

首部由tcphdr数据结构实现
必须注意系统的 字节序，因为其中使用了位域字段
tcp.h
```C
struct tcphdr { 
  __be16 source; 
  __be16 dest; 
  __be32 seq; 
  __be32 ack_seq; 
#if defined(__LITTLE_ENDIAN_BITFIELD) 
  __u16 res1:4, 
  doff:4, 
  fin:1, 
  syn:1, 
  rst:1, 
  psh:1, 
  ack:1, 
  urg:1, 
  ece:1, 
  cwr:1; 
#elif defined(__BIG_ENDIAN_BITFIELD) 
  __u16 doff:4, 
  res1:4, 
  cwr:1, 
  ece:1, 
  urg:1, 
  ack:1, 
  psh:1, 
  rst:1, 
  syn:1, 
  fin:1; 
#else 
#error "Adjust your <asm/byteorder.h> defines" 
#endif 
  __be16 window; 
  __sum16 check; 
  __be16 urg_ptr; 
}; 
```


接收TCP数据

所有TCP操作（连接建立和关闭，数据传输）都是通过发送带有各种属性和标志的分组来进行的。

在互联网络层处理过分组之后，tcp_v4_rcv是TCP层的入口

系统中的每个TCP套接字都归入3个散列表之一，分别接受下列状态的套接字。
- 完全连接的socket
- 等待连接(监听状态)的socket
- 处于建立连接过程(三次握手)中的 socket

![eb106e842b21ef1dbcfe709d7410f8a1.png](../_resources/eb106e842b21ef1dbcfe709d7410f8a1.png)

在对分组数据进行各种检查并将首部中的信息复制到套接字缓冲区的控制块之后，内核将查找等待该分组的套接字的工作委托给__inet_lookup函数。
该函数唯一的任务就是调用另两个函数，扫描各种散列表。
__inet_lookup_established企图返回一个==已连接==的套接字。
如果没有找到适当的结构，则调用inet_lookup_listener函数检查所有的==监听==套接字。

tcp_v4_do_rcv是一个多路分解器，基于套接字状态将代码控制流划分为不同的分支


三次握手

在连接建立之前，会看到下述情形：客户端进程的套接字状态为CLOSED，而服务器套接字的状态是LISTEN

建立TCP连接的过程需要交换3个TCP分组，因而称为三次握手（three-way handshake）。

- 客户端通过向服务器发送SYN来发出连接请求。客户端的套接字状态由CLOSED变为
SYN_SENT。
- 服务器在一个监听套接字上接收到连接请求，并返回SYN和ACK。服务器套接字的状态由LISTEN变为SYN_RECV。
- 客户端套接字接收到SYN/ACK分组后，切换到ESTABLISHED状态，表明连接已经建立。一个ACK分组被发送到服务器
- 服务器接收到ACK分组，也切换到ESTABLISHED状态。这就完成了两端的连接建立工作，可以开始数据交换。

原则上，可以仅使用一个或两个分组建立连接。但这可能带来一种风险，由于与同一地址（IP地址和端口号）之间的旧连接的延期分组的存在，可能导致建立有缺陷的连接。
三次握手的目的就是要==防止==这种情况

```text
1 192.168.0.143 192.168.1.10 TCP 1025 > http [SYN] Seq=2895263889 Ack=0 
2 192.168.1.10 192.168.0.143 TCP http > 1025 [SYN, ACK] Seq=2882478813 Ack=2895263890
3 192.168.0.143 192.168.1.10 TCP 1025 > http [ACK] Seq=2895263890 Ack=2882478814 
```

客户端对第一个分组生成随机的序列号2895263889，保存在在TCP首部的SEQ字段。
服务器对该分组的到达，响应一个组合的SYN/ACK分组，序列号是新的（在本例中是2882478813）。
我们在这里感兴趣的是SEQ/ACK字段的内容（数值字段，不是标志位）。
服务器填充该字段时，将接收到的字节数目加1，再加到接收的序列号上（底层的原理，在下文讨论）



被动连接建立

被动连接建立并不源于内核本身，而是在接收到一个连接请求的SYN分组后触发的

因而其起点是tcp_v4_rcv函数，如上文所述，该函数查找一个监听套接字，并将控制权转移到tcp_v4_do_rcv

![26a6ab1eecab2ecb8564e1daf28ac52f.png](../_resources/26a6ab1eecab2ecb8564e1daf28ac52f.png)

---

主动连接建立
主动连接建立发起时，是通过用户空间应用程序调用open库函数，发出socketcall系统调用到达内核函数tcp_v4_connect

![d8d3fc937d848efeab96051b116d364c.png](../_resources/d8d3fc937d848efeab96051b116d364c.png)

该函数开始于查找到目标主机的IP路由，使用的框架如上所述。
在产生TCP首部并将相关的值设置到套接字缓冲区中之后，套接字状态从CLOSED改变为SYN_SENT。
接下来tcp_connect将一个SYN分组发送到互联网络层，接下来到服务器端。
此外，在内核中创建一个定时器，确保如果在一定的时间内没有接收到确认，将重新发送分组。
现在客户端必须等待服务器对SYN分组的确认以及确认连接请求的一个SYN分组，这是通过普通的TCP机制接收的（图12-29的下半部）。
这又通向了tcp_rcv_state_process分配器，在这种情况下，控制流又转移到tcp_rcv_synsent_state_process。
套接字状态设置为ESTABLISHED，而tcp_send_ack向服务器返回另一个ACK分组，完成连接建立


分组传输
TCP有几个特性，要求在相互通信的主机之间进行广泛的控制和提供一些安全过程
- 按可保证的次序传输字节流。
- 通过自动化机制重传丢失的分组。
- 每个方向上的数据流都独立控制，并与对应主机的速度匹配。

基于序列号来确认分组的概念，也用于普通的分组

在建立连接时，生成一个随机数
在最初发送的序列号基础上，会为TCP传输的每个字节都分配一个唯一的序列号。例如，假定TCP系统的初始随机数是100。因而，发送的前16个字节是序列号是100、101……115。

TCP使用一种累积式确认（cumulative acknowlegment）方案。这意味着一次确认将涵盖一个连续的字节范围。通过ack字段发送的数字将确认数据流在上一个ACK数目和当前ACK数目之间的所有字节
例如，ACK数目166确认了字节索引165之前（含）的所有字节，预期下一个分组中从字节166开始

TCP没有提供显式的重传请求机制。换句话说，接收方不能请求发送方重传丢失的分组。
。。有吧，可以告诉 发送方 缺少的 序号啊。 select xxx 


发送分组

638

![302e9e0c398d0a85d93db25ad2f3d3e4.png](../_resources/302e9e0c398d0a85d93db25ad2f3d3e4.png)

- 接收方等待队列上必须有足够的空间可用于该数据。
- 必须实现防止连接拥塞的ECN机制。
- 必须检测某一方出现失效的情况，以免通信出现停顿。
- TCP慢启动（slow-start）机制要求在通信开始时，逐渐增大分组长度。
- 发送但未得到确认的分组，必须在一定的超时时间间隔之后反复重传，直至接收方最终确认。


连接终止

(1) 计算机A调用标准库函数close，发出一个TCP分组，首部中的==FIN==标志置位。A的套接字切换到==FIN_WAIT_1==状态。
(2) B收到FIN分组并返回一个ACK分组。其套接字状态从ESTABLISHED改变为==CLOSE_WAIT==。收到FIN后，以“文件结束”的方式通知套接字。
(3) 在收到ACK分组之后，计算机A的套接字状态从FIN_WAIT_1==变为FIN_WAIT_2==。
(4) 计算机B上与对应套接字相关的应用程序也执行close，从B向A发送FIN分组。计算机B的套接字状态变为LAST_ACK。
(5) 计算机A用一个ACK分组确认B发送的FIN，然后首先进入==TIME_WAIT==状态，接下来在一定时间后自动切换到CLOSED状态。
(6) 计算机B收到ACK分组，其套接字也切换到CLOSED状态

。。established -> 发送fin，变成fin_wait_1 -> 收到fin的ack，变成fin_wait_2 -> 收到对方的fin并ack，变成time_wait -> 一段时间后，变成closed
。。established -> 收到fin并ack，变成close_wait -> 发送fin，last_ack -> 收到fin的ack，closed


## 12.10 应用层

内核与用户空间socket 之间的接口实现在 C标准库中，使用 socketcall 系统调用

socketcall充当一个多路分解器，将各种任务分配由不同的过程执行，例如打开一个套接字、绑定或发送数据

Linux采用了内核套接字的概念，使得与用户空间中的套接字的通信尽可能简单。对程序使用的==每个套接字来说，都对应于一个socket结构和sock结构的实例==。二者分别充当向下（到内核）和向上的（到用户空间）接口。

socket结构定义如下（稍作简化）：
```C
// <net.h> 
struct socket { 
  socket_state state; // socket 的连接状态，。不是TCP的那种状态，而是：未分配，未连接到任何套接字，处于连接过程中，已经连接到另一个套接字，处于断开连接过程中。 net.h 的 socket_state enum
  unsigned long flags; 
  const struct proto_ops *ops; 
  struct file *file; // 指向一个伪文件的file实例，用于与套接字通信
  struct sock *sk; // 指向一个更冗长的结构，包含了对 内核有意义的 附加的套接字管理数据。
  short type;   // 协议类型的数字标识符
};
```


12.10.2 套接字和文件

在连接建立后，用户空间进程使用 ==普通的文件操作来访问套接字==。这在内核中是如何实现的呢？
由于VFS层（在第8章讨论）的开放结构，只需要很少的工作

套接字使用的file_operations如下：
```C
// net/socket.c 
struct file_operations socket_file_ops = { 
  .owner = THIS_MODULE, 
  .llseek = no_llseek, 
  .aio_read = sock_aio_read, 
  .aio_write = sock_aio_write, 
  .poll = sock_poll, 
  .unlocked_ioctl = sock_ioctl, 
  .compat_ioctl = compat_sock_ioctl, 
  .mmap = sock_mmap, 
  .open = sock_no_open, /* 专用的open代码，禁止通过/proc打开 */ 
  .release = sock_close, 
  .fasync = sock_fasync, 
  .sendpage = sock_sendpage, 
  .splice_write = generic_splice_sendpage, 
};
```

内核提供了两个宏来进行必要的指针运算，根据inode找到相关的套接字实例（SOCKET_I），或反过来（SOCK_INODE）。


### 12.10.3 socketcall 系统调用

socket读写可以通过 虚拟文件系统 进行。
但是 创建socket，bind，listen 等 无法通过 文件系统来进行。
所以提供了 socketcall

17个套接字操作只对应到一个系统调用

第一个参数是一个数值常数，选择所要的系统调用

net/socket.c 
```C
asmlinkage long sys_socketcall(int call, unsigned long __user *args)
{
...
  switch(call) 
  { 
  case SYS_SOCKET: 
    err = sys_socket(a0,a1,a[2]); 
    break;
  case SYS_BIND: 
    err = sys_bind(a0,(struct sockaddr __user *)a1, a[2]); 
    break; 
... 
  case SYS_SENDMSG: 
    err = sys_sendmsg(a0, (struct msghdr __user *) a1, a[2]); 
    break; 
  case SYS_RECVMSG: 
    err = sys_recvmsg(a0, (struct msghdr __user *) a1, a[2]); 
    break; 
  default: 
    err = -EINVAL; 
    break; 
  }
}
```


下面是 socketcall 的 "子调用"
|函数|语义|
|--|--|
|sys_socket |创建一个新的套接字|
|sys_bind |将一个地址绑定到一个套接字|
|sys_connect |将一个套接字连接到一个服务器|
|sys_listen |打开被动连接，在套接字上监听|
|sys_accept |接受一个进入的连接请求|
|sys_getsockname |返回套接字的地址|
|sys_getpeername |返回参与通信的另一方的地址|
|sys_socketpair |创建一对套接字，可用于双向通信（两个套接字都在同一系统上）|
|sys_send |通过现存连接发送数据|
|sys_sendto |向明确指定的目标地址发送数据（用于UDP连接）|
|sys_recv |接收数据|
|sys_recvfrom |从一个数据报套接字接收数据，同时返回源地址|
|sys_shutdown |关闭连接|
|sys_setsockopt |返回套接字设置的有关信息|
|sys_getsockopt |安置套接字选项|
|sys_sendmsg |以BSD风格发送消息|
|sys_recvmsg |以BSD风格接收信|


### 12.10.4 创建套接字

sys_socket 创建套接字

### 接收数据

使用recvfrom和recv以及与文件相关的readv和read函数来接收数据

### 发送数据
网络有关的库函数 send,sendto
文件层的 write，writev


## 12.11 内核内部的网络通信

各个内核组件之间的通信，以及用户层和内核之间的通信。netlink机制提供了所需的框架

12.11.1 通信函数
内核内部的网络API。其定义基本上与用户层相同：
net.h
```C
int kernel_sendmsg(struct socket *sock, struct msghdr *msg, struct kvec *vec, size_t num, size_t len); 
int kernel_recvmsg(struct socket *sock, struct msghdr *msg, struct kvec *vec, size_t num, size_t len, int flags); 
int kernel_bind(struct socket *sock, struct sockaddr *addr, int addrlen); 
int kernel_listen(struct socket *sock, int backlog); 
int kernel_accept(struct socket *sock, struct socket **newsock, int flags); 
int kernel_connect(struct socket *sock, struct sockaddr *addr, int addrlen, int flags); 
int kernel_getsockname(struct socket *sock, struct sockaddr *addr, int *addrlen); 
int kernel_getpeername(struct socket *sock, struct sockaddr *addr, int *addrlen); 
int kernel_getsockopt(struct socket *sock, int level, int optname, char *optval, int *optlen); 
int kernel_setsockopt(struct socket *sock, int level, int optname, char *optval, int optlen); 
int kernel_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags); 
int kernel_sock_ioctl(struct socket *sock, int cmd, unsigned long arg); 
int kernel_sock_shutdown(struct socket *sock, enum sock_shutdown_cmd how); 
```

12.11.2 netlink机制
netlink是一种基于网络的机制，允许在内核内部以及内核与用户层之间进行通信。

现在，该机制最重要的用户是通用对象模型，它使用netlink套接字将各种关于==内核内部事务的状态信息传递到用户层==。其中包括新设备的注册和移除、硬件层次上发生的特别的事件，等等

内核中还有其他一些可选的方法能够实现类似的功能，比如procfs或sysfs中的文件。但与这些方法相比，netlink机制有一些很明显的优势。
- 任何一方都不需要轮询
- 系统调用和ioctl也能够从用户层向内核传递信息，但比简单的netlink连接更难于实现。
- 内核可以直接向用户层发送信息，而无须用户层事先请求
- 除了标准的套接字，用户空间应用程序不需要使用其他东西来与内核交互

netlink只支持数据报信息，但提供了双向通信。另外，netlink不仅支持单播消息，也可以进行多播。
类似于任何其他基于套接字的机制，netlink的工作方式是异步的。










# ch13 系统调用 655

用户态和核心态不同的虚拟地址空间和利用各种处理器特性的不同方式。
控制权如何在应用程序和内核之间来回传递，参数和返回值如何传递


追踪系统调用
一个简单些的方案是使用==strace==工具，它可以==记录应用程序==发出的==所有系统调用==并将该信息提供给程序员，在调试程序时，这个工具是不可缺少的。
内核自然需要为记录系统调用提供专门的支持，这将在13.3.3节讨论［不出所料，这项支持功能也是一个系统调用（ptrace），我们只对其输出感兴趣］。

`strace -o log.txt ./a.out`


重启系统调用
如果在一个进程执行系统调用时，向该进程发送一个信号，那么在处理时，二者的优先级如何分配呢?

尽管该方案易于实现，但它迫使用户空间应用程序的程序员必须明确检查所有系统调用的返回值，并在返回值为-EINTR的情况下，重新启动被中断的系统调用，直至该调用不再被信号中断。用这种方法重启的系统调用称作可重启系统调用（restartable system call），该技术则称为重启（restarting）


BSD内核将中断系统调用的执行并切换到用户态执行信号处理程序。在发生这种情况时，该系统调用不会有返回值，内核在信号处理程序结束后将==自动重启该调用==。因为该行为对用户应用程序是透明的，也不再需要重复实现对-EINTR返回值的检查和调用的重启，所以与System V方法相比，这种方案更受程序员的欢迎。
Linux通过SA_RESTART标志支持BSD方案



## 13.2 可用的系统调用

进程管理
- fork,vfork,clone，创建进程， clone还可以创建线程
- exit， 结束进程 并释放资源
- 一大堆系统调用用于 查询和设置进程的属性，主要是对 task_struct 中字段进行操作
- personality，定义了 app的 执行环境
- ptrace，使得 可以跟踪系统调用，是 strace 工具的基础
- nice，设置 普通进程的优先级，-20 - 19 之间
- setrlimit 设置 资源限制， getrlimit。。。 getrusage查询当前资源使用情况

时间操作
- adjtimex，读取和设置基于时间的 内核变量，控制 内核在时间方面的行为
- alarm,setitimer，警报器 和 间隔定时器，getitimer
- gettimeofday,settimeofday, 获取和设置 当前系统时间。 和time不同，这2个函数 考虑了 时区 和 夏令时
- sleep,nanosleep，让进程 暂停 一个指定的时间段。
- time， 1970.1.1 到现在的 秒数， stime修改这个值，也会修改 当前系统的 时间。

信号处理
信号是进程间交换有限信息 以及 促进进程间通信的 最简单的方法
- signal 设置信号处理函数，sigaction是 signal 的增强版，支持附加的选项。
- sigpending 检查进程 当前是否有 待决信号 被阻塞
- sigsuspend 将进程 置于等待队列，直到 某个特定(一组信号中的一个) 信号达到
- setmask 启用信号的 阻塞机制， getmask
- kill 向一个进程发送任何信号
- 一组处理实时信号的系统调用， 函数名前缀是 rt_，例如 rt_sigaction设置一个 实时信号处理程序，rt_sigsuspend 将进程置于等待状态，直到某个特定(一组信号中的一个)的信号到达

调度
- setpriority,getpriority，设置，获取 进程的 优先级 。 。。nice什么区别？
- sched_setscheduler,sched_getscheduler 设置和查询 调度类， sched_setparam，sched_getparam 设置，查询进程的附加调度参数
- sched_yield 自愿==释放CPU的控制权==， 即使进程 仍有CPU可用。

模块
- init_module，添加一个模块
- delete_module，从内核移除一个模块

文件系统
- 创建，修改目录结构：chdir,mkdir,rmdir,rename,symlink,getcwd,chroot,umask,mknod
- chown,chmod, 修改文件和目录属性
- open,close,read,readv,write,writev,truncate,llseek, 处理文件内容
- readdir，getdents 读取目录结构
- link,symlink,unlink，创建和删除链接 (或文件，如果该文件时 某个硬链接的最后一个成员)， readlink
- mount,umount， 文件系统的 装载和 卸载
- poll,select，等待某些事情
- execve，装载一个 新进程，替换就的进程。 与fork联合使用时，会启动一个 新的程序

内存管理
- brk，动态内存管理，修改进程数据段的长度。
- mmap,mmap2,munmap,mremap， 执行内存映射，解除映射，重新映射， mprotect控制对 虚拟内存中特定区域的访问， madvice 提出对 特定虚拟内存区域的 使用建议。
- swapop,swapoff  启用，禁用 外存储器 设备上 (附加) 的交换区

进程间通信IPC 和网络功能
只有2个系统调用 来处理 所有可能的任务， 但是有很多封装。
- socketcall 处理网络方面的问题，用于实现 socket抽象。 管理各种类型的 连接和协议，总共有 17种功能。 12.10.3
- ipc，用于处理计算机本地的连接。 11种功能，5个参数

系统信息和设置
- syslog，向系统日志写入消息
- sysinfo，返回 系统状态的信息，特别是 有关于内存使用的 统计量(物理内存，缓冲区，交换区)
- sysctl，微调 内核参数。

系统安全和能力
- capset，capget， 设置，查询 进程的能力
- security，系统调用的 多路分接器，用于实现 LSM

。。log struct merge tree ?


## 13.3 系统调用的实现

用于实现系统调用的处理程序函数，在形式上有如下几个共同的特性
- 函数名称前缀都是 sys_
- 所有处理函数 最多接收 5个参数
- 所有的系统调用 都在 核心态执行


![5accc334ac7948ab60bfa3e9e07ef32f.png](../_resources/5accc334ac7948ab60bfa3e9e07ef32f.png)


参数传递
IA-32系统，使用 汇编语言指令 `int &0x80` 来引发 软件中断 128。这是一个调用门，为此指派了一个特定的函数 来继续进行 系统调用 的处理。 ==系统调用编号 通过 寄存器 eax 传奇，参数通过 ebx,ecx,edx,esi,edi传递==
2个汇编指令(sysenter,sysexit) 来快速进入 和退出 核心态

alpha 处理器提供了一种特权系统状态。v0用于传递系统调用编号，而5个可能的参数分别保存在a0到a4

PowerPC处理器提供了一条优雅的汇编语言指令，称作sc（system call）。该指令专门用于实现系统调用。==寄存器r3保存系统调用编号，而参数保存在寄存器r4到r8中==。

AMD64体系结构在实现系统调用时，也提供了自身的汇编语言指令，其名称为syscall。系统调用编号保存在==raw寄存器中，而参数保存在rdi、rsi、rdx、r10、r8和r9==中。


系统调用表
Sparc64系统上的sys_call_table，定义在arch/sparc/kernel/systlbs.S中（其他系统的系统调用表，通常可以在与处理器类型对应的目录下的entry.S文件中找到）

sys_call_table是基指针，指向数组的起始处，即（按C语言的术语）指向索引为0的数组项。如果一个用户空间程序调用open系统调用，传递的系统调用编号是5。
分配器例程将编号5加到sys_call_table的基地址，得到该数组的第6项，其中保存了sys_open的地址，这是独立于处理器的处理程序函数。


因为核心态和用户态使用两个不同的栈，如第3章所述，系统调用参数不能像通常那样在栈上传递。在两个栈之间的切换，或者由进入核心态时调用的体系结构相关的汇编语言代码进行，或者在特权级别从用户态切换到核心态时由处理器自动进行


返回用户态
返回值语义
asm-generic/errno-base.h
asm-generic/errno.h


访问用户空间
有些情况下，内核代码必须访问用户应用程序的虚拟内存。


追踪系统调用






# ch14 内核活动 678

- 硬件中断， hardware interrupt
- 软中断, SoftIRQ


中断可以分为2个类别
- 同步中断和异常，由CPU自身产生，针对当前执行的程序。
- 异步中断， 由外部设备产生， 不和特定进程关联，可能发生在任何时间。

两类中断的共同特性是什么？如果CPU当前不处于核心态，则发起从用户态到核心态的切换。
接下来，在内核中执行一个专门的例程，称为中断服务例程（interrupt service routine，简称ISR）或中断处理程序（interrupt handler）。



边缘触发中断
电平触发中断


注册IRQ
由设备驱动程序动态注册ISR的工作，可以使所述的数据结构非常简单地进行。在

### 14.1.7 处理IRQ
1. 切换到核心态
2. IRQ栈
3. 调用电流处理程序例程
4. 调用高层ISR
5. 实现处理程序例程
6. 实现处理程序



## 14.2 软中断

软中断使得内核可以延期执行任务。


有几种方法可 开启 软中断处理，但这些都归结为调用do_softirq函数


软中断守护进程
软中断守护进程的任务是，与其余内核代码异步执行软中断。为此，系统中的每个处理器都分配了自身的守护进程，名为ksoftirqd。


内核中有两处调用wakeup_softirqd唤醒了该守护进程。
- 在do_softirq中
- 在raise_softirq_irqoff末尾。


## 14.3 tasklet

软中断是将操作推迟到未来时刻执行的最有效的方法。
但该延期机制处理起来==非常复杂==。因为多个处理器可以同时且独立地处理软中断，同一个软中断的处理程序例程可以在几个CPU上同时运行。
对软中断的效率来说，这是一个关键，多处理器系统上的网络实现显然受惠于此。但处理程序例程的设计==必须是完全可重入且线程安全==的。另外，==临界区必须用自旋锁保护（或其他IPC机制==，参见第5章），而这需要大量审慎的考虑。


tasklet 和 工作队列是延期执行工作的机制，其实现基于软中断，但它们更易于使用，因而更适合于设备驱动程序（以及其他一般性的内核代码）。



## 14.4 等待队列 和完成量

等待队列（wait queue）用于使进程等待某一特定事件发生，而无须频繁轮询。进程在等待期间睡眠，在事件发生时由内核自动唤醒。
完成量（completion）机制基于等待队列，内核利用该机制等待某一操作结束。


### 14.4.3 工作队列
工作队列是将操作延期执行的另一种手段。因为它们是通过守护进程在用户上下文执行，函数可以睡眠任意长的时间，这与内核是无关的。









# ch15 时间管理 714



# ch16 页缓存和块缓存 761

性能和效率是内核开发中两个非常重要的因素。

内核不仅依赖于一个精巧的整体框架来规范各个部分之间交互，还需要一个==功能广泛的缓冲和缓存框架==来提高系统的速度。


缓存也有==负面效应==，内核必须审慎地采用
- 物理内存 比 块设备 小很多，只能缓存 仔细挑选的 部分数据
- 缓存占用了一部分内存，减少了 实际可用的物理内存
- 如果系统崩溃，缓存中的数据 可能没有回写到 底层的块设备，导致 不可恢复的数据丢失


利大于弊，缓存特性已经永久性地集成到了内核的结构中。

缓存是页交换或调页操作的逆操作。实现页交换时，则是用低速的块设备来代替物理内存

内核为块设备提供了两种通用的缓存方案。
- 页缓存，page cache
- 块缓存, buffer cache


## 16.1 页缓存的结构

页缓存的任务在于，获得一些物理内存页，以加速在块设备上按页为单位执行的操作。


从大量数据的集合（页缓存）中快速获取单个数据元素（页）的问题，并不是Linux内核特有的。
它对信息技术的所有领域来说都是一个共同的问题，在发展过程中衍生出了许多精巧复杂的数据结构，并经受住了时间的考验。
对此用途而言，树数据结构是非常流行的，Linux也采用了这种结构来==管理页缓存中包含的页==，称为==基数树==（radix tree）。

树的结点具备两种搜索标记(search tag)。二者用于指定给定页当前是否是脏的（即
页的内容与后备存储器中的数据是不同的），或该页是否正在向底层块设备回写。
重要的是，==标记不仅对叶结点设置，还一直向上设置到根结点==。

16.1.2 回写修改的数据
由于页缓存的存在，写操作不是直接对块设备进行，而是在内存中进行，修改的数据首先被收集起来，然后被传输到更低的内核层，在那里可以对写操作进一步优化，以完全利用各个设备的具体功能

内核同时提供了如下几个同步方案。
- 几个专门的内核守护进程在后台运行，称为pdflush，它们将周期性激活，而不考虑页缓存中当前的情况。这些守护进程扫描缓存中的页，将超出一定时间没有与底层块设备同步的页写回。
- pdflush的第二种运作模式是：如果缓存中修改的数据项数目在短期内显著增加，则由内核激活pdflush。
- 提供了相关的系统调用，可由用户或应用程序通知内核写回所有未同步的数据。最著名的是sync调用


每个地址空间都有一个“宿主”，作为其数据来源。大多数情况下，宿主都是表示一个文件的inode。因为所有现存的inode都关联到其超级块（第8章讨论过），内核只需要扫描所有超级块的链表，并跟随相关的inode，即可获得被缓存页的列表。

通常，修改文件或其他按页缓存的对象时，只会修改页的一部分，而非全部。
内核在写操作期间，将缓存中的每一页划分为较小的单位，称为缓冲区。
在同步数据时，内核可以将回写操作限制于那些实际发生了修改的较小的单位上


## 16.2 块缓存的结构

内核的早期版本只包含了块缓存，来加速文件操作和提高系统性能

来自于底层块设备的块缓存在内存的缓冲区中，可以加速读写操作。其实现包含在fs/buffers.c中。

随着日渐倾向于使用基于页操作实现的通用文件存取方法，块缓存作为中枢系统缓存的重要性已经逐渐失去，==主要的==缓存任务现在由==页缓存==承担

基于块的I/O的标准数据结构，现在已经不再是缓冲区，而是第6章讨论的struct bio。


缓冲区用作小型的数据传输，一般涉及的数据量是与块长度可比拟的。
文件系统在处理元数据时，通常会使用此类方法。
而裸数据的传输则按页进行，而缓冲区的实现也基于页缓存。

块缓存在结构上由两个部分组成。
- 缓冲头，保存了与缓冲区状态相关的所有管理数据，包括块号、块长度、访问计
数器等
- 有用数据保存在专门分配的页中，这些页也可能同时存在于页缓存中



## 16.3 地址空间

地址空间是内核中最关键的数据结构之一。对该数据结构的管理，已经演变为内核面对的最中心的问题之一。
大量子系统（文件系统、页交换、同步、缓存）都围绕地址空间的概念展开。
因而，这个概念可以认为是内核最根本的抽象机制之一，以重要性而论，该抽象可跻身于传统抽象如进程、文件之列。

尽管在Linux及其他UNIX衍生物的初期，inode是缓存数据的唯一来源，但内核现在采用了更为通用的地址空间方案，来建立缓存数据与其来源之间的关联。尽管文件的内容构成缓存数据的一大部分，但地址空间的接口非常通用，使得缓存也能够容纳其他来源的数据，并快速访问。

地址空间如何融入到页缓存的结构中呢？它们实现了两个单元之间的一种转换机制。
(1) 内存中的页分配到每个地址空间。这些页的内容可以由用户进程或内核本身使用各式各样的方法操作。
这些数据表示了缓存的内容。
(2) 后备存储器(。。一般来说就是指 硬盘(块设备)) 指定了填充地址空间中页的数据的来源。地址空间关联到处理器的虚拟地址空间，是由处理器在虚拟内存中管理的一个区域到源设备（使用块设备）上对应位置之间的一个映射。
如果访问了虚拟内存中的某个位置，该位置没有关联到物理内存页，内核可根据地址空间结构来找到读取数据的来源


16.3.1 数据结构

地址空间的基础是address_space结构

```C
// <fs.h> 
struct address_space { 
  struct inode *host; /* 所有者：inode，或块设备 */ 
  struct radix_tree_root page_tree; /* 地址空间中所有页的基数树 */ 
  unsigned int i_mmap_writable; /* VM_SHARED映射的计数 */ 
  struct prio_tree_root i_mmap; /* 私有和共享映射的树 */ 
  struct list_head i_mmap_nonlinear; /* VM_NONLINEAR映射的链表元素 */ 
  unsigned long nrpages; /* 缓存页的总数 */ 
  pgoff_t writeback_index; /* 回写由此开始 */ 
  struct address_space_operations *a_ops; /* 方法，即地址空间操作 */ 
  unsigned long flags; /* 错误标志位/gfp掩码 */ 
  struct backing_dev_info *backing_dev_info; /* 设备预读 */ 
  struct list_head private_list; 
  struct address_space *assoc_mapping;
} __attribute__((aligned(sizeof(long)))); 
```

后备存储器是指与地址空间相关的外部设备，用作地址空间中信息的来源。它通常是块设备
```C
// <backing-dev.h>
struct backing_dev_info {
  unsigned long ra_pages; /* 最大预读数量，单位为PAGE_CACHE_SIZE */ 
  unsigned long state; /* 对该成员，总是使用原子位操作 */ 
  unsigned int capabilities; /* 设备能力 */ 
... 
}; 
```


![85291ef260a574c95fb77bebb0f5ae15.png](../_resources/85291ef260a574c95fb77bebb0f5ae15.png)



16.3.2 页树

内核使用 基数树 来管理 与一个地址空间关联的 所有页。

```C
// <radix_tree_root.h> 
struct radix_tree_root { 
  unsigned int height; 
  gfp_t gfp_mask; 
  struct radix_tree_node *rnode; 
};
```
- 树的高度，根据该信息 和 每个节点的项数，内核可以快速计算出 给定树中 数据项的最大数目。 。。。？how？
- gfp_mask 从哪个内存域 分配内存
- 指向树的第一个结点。


基数树的 node
```C
// <lib/radix_tree.c> 
#define RADIX_TREE_TAGS 2 
#define RADIX_TREE_MAP_SHIFT (CONFIG_BASE_SMALL ? 4 : 6) 
#define RADIX_TREE_MAP_SIZE (1UL << RADIX_TREE_MAP_SHIFT) 
#define RADIX_TREE_TAG_LONGS \ 
((RADIX_TREE_MAP_SIZE + BITS_PER_LONG - 1) / BITS_PER_LONG) 
struct radix_tree_node {
  unsigned int height; /* 从底部开始计算的高度 */ 
  unsigned int count; 
  struct rcu_head rcu_head; 
  void *slots[RADIX_TREE_MAP_SIZE];
  unsigned long tags[RADIX_TREE_TAGS][RADIX_TREE_TAG_LONGS]; 
}; 
```
- slots 是一个 void* 数组，指向 数据 或 其他结点。
- count 结点中已使用的 数据项的数目。

根据slots数组的长度， 每个树节点都可以 进一步 指向 64个子节点。

。。 1<<6 == 64, ulong 是64bit。 所以tag可行。 也限制了 树的最大分叉。

基数树的每个node 都包含了 额外的 标记 信息，内核对脏页进行了标记，在扫描脏页期间，如果没有标记，就跳过。

当前支持如下两种标记。
(1) PAGECACHE_TAG_DIRTY指定页是否是脏的。
(2) PAGECACHE_TAG_WRITEBACK表示该页当前正在回写。

标记信息保存在一个二维数组中（tags）
数组的第一维区分不同的标记，而第二维包含了足够数量的unsigned long，使得对该结点中可能组织的每个页，都能分配到==一个比特位==。

radix_tree_tag_set用于对一个特定的页设置一个标志: 
```C
// <radix-tree.h> 
void *radix_tree_tag_set(struct radix_tree_root *root, unsigned long index, unsigned int tag); 
```

操作
```C
// <radix-tree.h> 
int radix_tree_insert(struct radix_tree_root *, unsigned long, void *); 
void *radix_tree_lookup(struct radix_tree_root *, unsigned long); 
void *radix_tree_delete(struct radix_tree_root *, unsigned long); 
int radix_tree_tag_get(struct radix_tree_root *root, unsigned long index, unsigned int tag); 
void *radix_tree_tag_clear(struct radix_tree_root *root, unsigned long index, unsigned int tag); 
```


为确保基数树的操作快速，内核使用了一个独立的slab缓存来保存radix_tree_node的实例，以便快速分配此类型的结构实例

每个基数树都还有一个CPU池，其中存放了预分配的结点，以便进一步加速向树插入新数据项的操作


---
锁
基数树没有针对通常的并发访问提供任何形式的保护。



16.3.3 地址空间操作

fs.h
struct address_space_operations

- 将地址空间的 页 写回 底层块设备
- 从后备存储器 读取 连续的页 到 页帧
- 标记 一页 为脏的
- sync_page，真正同步到 块设备， 上面的 writepage(s) ，可能还是在 硬盘的缓冲中。
- 将地址空间内的逻辑块偏移 映射为 物理块号
- 释放页
- 从地址空间移除页
- 直接读写，绕过 块设备的 缓冲机制。
- 就地执行，启动可执行代码，无需加载到 页缓存


文件系统的很多操作，实际上最终还是 引用了 内核提供的 通用函数



共享内存文件系统的address_space_operations实例特别简单，因为只需要对3个字段赋值：
```C
// mm/shmem.c 
static struct address_space_operations shmem_aops = { 
  .writepage = shmem_writepage, 
  .set_page_dirty = __set_page_dirty_no_writeback, 
  .migratepage = migrate_page, 
};
```


## 16.4 页缓存的实现

页缓存的实现基于基数树。
实现简单得惊人


16.4.1 分配页

page_cache_alloc用于为一个即将加入页缓存的新页分配数据结构
后缀为_cold的变体工作方式相同，但试图获取一个冷页（对CPU高速缓存而言）

```C
<pagemap.h> 
struct page *page_cache_alloc(struct address_space *x) 
struct page *page_cache_alloc_cold(struct address_space *x) 
```
最初，不会访问基数树，因为工作委托给alloc_pages，该函数从 ==伙伴系统==获取一个页帧 。但需要地址空间参数，确定该页所来自的内存域。


新页添加到页缓存稍微复杂一点，这是add_to_page_cache的职责
```C
mm/filemap.c 
int add_to_page_cache(struct page *page, struct address_space *mapping, pgoff_t offset, gfp_t gfp_mask) 
{ 
... 
  error = radix_tree_insert(&mapping->page_tree, offset, page); 
  if (!error) { 
    page_cache_get(page); 
    SetPageLocked(page); 
    page->mapping = mapping; 
    page->index = offset; 
    mapping->nrpages++; 
  } 
... 
  return error; 
} 
```

内核还提供了另一个可选的函数add_to_page_cache_lru


16.4.2 查找页

mm/filemap.c 
struct page * find_get_page(struct address_space *mapping, pgoff_t offset) 

所有繁重的工作都已经由基数树的实现完成：radix_tree_lookup查找位于给定偏移量的页，而page_cache_get在找到页的情况下，将其引用计数加1。


为方便使用，内核提供了两个辅助函数
pagemap.h
struct page * find_or_create_page(struct address_space *mapping, pgoff_t index, gfp_t gfp_mask); 
struct page * find_lock_page(struct address_space *mapping, pgoff_t index); 


查找多个页。对应的辅助函数原型如下：
pagemap.h
```C
unsigned find_get_pages(struct address_space *mapping, pgoff_t start, unsigned int nr_pages, struct page **pages); 
unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t start, unsigned int nr_pages, struct page **pages); 
unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index, int tag, unsigned int nr_pages, struct page **pages); 
```


16.4.3 在页上等待

内核经常需要在页上等待，直至其状态改变为某些预期值。
内核提供了wait_on_page_writeback函数，用于等待页的该标志位清除：
pagemap.h
static inline void wait_on_page_writeback(struct page *page)


16.4.4 对整页的操作

虽然名为“块”设备，但现代块设备可以在一个操作中传输比 块 大得多的数据单位，以提升系统性能。

在整页处理数据时，逐 缓冲区/ 块 的传输实际上是对性能踩刹车

内核添加了4个新的函数，来支持读写一页或多页

```C
// <mpage.h> 
int mpage_readpages(struct address_space *mapping, struct list_head *pages, unsigned nr_pages, get_block_t get_block); 
int mpage_readpage(struct page *page, get_block_t get_block); 
int mpage_writepages(struct address_space *mapping, struct writeback_control *wbc, get_block_t get_block); 
int mpage_writepage(struct page *page, get_block_t *get_block, struct writeback_control *wbc); 
```


### 16.4.5 页缓存预读

对未来的预测公认是一个非常困难的问题，但有时候内核会禁不住尝试一下

有些情况下，不难知道接下来会发生什么，例如在进程从文件读取数据时。
页是顺序读取的

预读不能由页缓存独立解决，还需要VFS和内存管理层的支持

预读机制已经在8.5.2节和8.5.1节讨论过。回想可知，就内核直接关注的问题而言，预读是从3个地方控制的
- do_generic_mapping_read，通用的读取例程，大多数依赖内核的标准例程来读取数据的文件系统都结束于某些位置。
- 缺页异常处理程序filemap_fault，负责为内存映射读取缺页
- __generic_file_splice_read，调用该例程是为支持splice系统调用，该系统调用使得可以直接在==内核空间中在两个文件描述符之间传输数据==，而无须涉及用户空间




## 16.5 块缓存的实现

块缓存不仅仅用作页缓存的附加功能，对以块而不是页进行处理的对象来说，块缓存是一个独立的缓存。

两种类型的块缓存，即独立的块缓存和用作页缓存附加功能的块缓存，二者的数据结构是相同的

```C
// <buffer_head.h> 
struct buffer_head { 
  unsigned long b_state; /* 缓冲区状态位图（见上文） */ 
  struct buffer_head *b_this_page; /* 页的缓冲区的环形链表 */ 
  struct page *b_page; /* 当前缓冲头映射到的页 */ 
  sector_t b_blocknr; /* 起始块号 */ 
  size_t b_size; /* 映射长度 */ 
  char *b_data; /* 指向页内数据的指针 */ 
  struct block_device *b_bdev; 
  bh_end_io_t *b_end_io; /* I/O完成 */ 
  void *b_private; /* 保留给b_end_io使用 */ 
... 
  atomic_t b_count; /* 此缓冲头的使用计数 */ 
}; 
```


### 16.5.3 页缓存和块缓存的交互










# ch17 数据同步 793

pdflush
超级块，inode 同步
拥塞队列
强制回写
完全同步

# ch18 页面回收和页交换 821

内核将很少使用的部分内存换出到块设备，这相当于提供了更多的主内存。
这种机制称为页交换（swapping）或换页（paging）

如果一个很少使用的页的后备存储器是一个块设备（例如，文件的内存映射），那么就无须换出被修改的页，而是可以 直接 与 块设备 同步

。。就是 第三章 提到过了， 页分为3类， 不可移动，可移动，可回收(。就是可以删除实际的内容，留一个空壳，如果CPU要读取，那么从 硬盘重建 这一页)
。。当然，这里是 直接 与块设备同步。 不走 页的形式。  不清楚了，这里说的有点模糊。

腾出的页帧可以重用，如果再次需要该数据，可以从来源重新建立该页。
如果页的后备存储器是一个文件，但不能在内存中修改（例如，二进制可执行文件的数据），那么在当前不需要的情况下，可以直接丢弃该页

分配给核心内核（即并非用于缓存）的页是不能回收的



数据同步对动态产生的内存页是不适用的，因为这种页没有后备存储器。


在内核中考虑如何实现页交换和页面回收，必须回答下面两个问题。
(1) 根据何种方案来回收页，即内核为确保最大可能利益以及最小可能损失，如何==判断应该回收哪些页==？
(2) 换出的页在交换区中如何组织，内核如何将页写入到交换区，==如何在此后需要时再次读取==？内核如何将页与后备存储设备进行同步？


只有少量几种页可以换出==到交换区==
- 类别为MAP_ANONYMOUS的页，没有关联到文件
- 进程的私有映射用于映射修改后不向底层块设备回写的文件，通常换出到交换区。
- 所有属于进程堆以及使用malloc分配的页
- 用于实现某种进程间通信机制的页

对其他页来说，换出到块设备上与之==对应的后备存储器即可==



18.1.2 页颠簸

在换出重要的数据后不久再次需要该数据

为防止页颠簸，内核必须解决的主要的问题是，尽可能精确地确定一个进程的工作集（working set，即使用最频繁的那些内存页）

在系统中维护一个==交换令牌==（swap token），赋予换入页的进程。内核会试图避免从该进程换出页，以减轻该进程的颠簸，使之有时间完成任务。
不久之后，交换令牌==将传递到另一个进程==，该进程也经历了页交换，而且比当前令牌持有者更需要内存


### 18.1.3 页交换算法

1. 第二次机会
对经典FIFO算法有一点小的改进

在换出一页之前，向其提供第二次机会。
每页都指定一个专门的字段，包含一个由硬件控制的比特位。
在==访问该页==时，该比特位==自动设置为1==。软件（即内核）负责清除该比特位

在一页到达链表末尾时，内核不会立即将其换出，而是首先检查前述的比特位是否置位。
如果==置位==，则清除该比特位，并将该页移动到FIFO队列的开始。换言之，将其作为添加到系统的新页处理。
如果该比特位==没有置位==，则将其换出。

。。就是 访问的时候设置， 移除的时候 如果 被访问过，就 多活一次， 然后 在 多活的这一次中，又被访问了，那么 又可以多活一次。


1. LRU
最近最少使用
least recently used

过去一段时间内频繁使用的页，在不久的将来很可能再次使用。
LRU算法基于上述说法的逆命题，假定最近不使用的页在较短的时间内也不会频繁需要使用。

LRU的基本原理可能比较简单，但合理实现该算法却比较难


## 18.2 Linux 内核中的页面回收和页交换



在讨论页交换子系统的设计时，需要考虑以下各方面的问题
- 应该如何在块设备介质上组织交换区？
- 内核能够利用哪些方法来检查在何时将多少页换出？
- 根据何种原则选择换出的页？换言之，应该选用哪种页面替换算法？
- 如何尽可能高效而快速地处理缺页异常，页如何从交换区返回到系统物理内存？
- 哪些数据可以从各种系统缓存删除（例如，从inode或dentry缓存），而不需要与后备存储器同步（因为它们可以根据其他信息间接重建）？


18.2.1 交换区的组织

换出的页或者保存在一个==没有文件系统的专用分区==中，或者存储在某个==现存文件系统中的一个定长文件==中
可以同时使用几个这样的区域。还可以根据各个交换区的速度不同，为其指定优先级。

每个交换区都细分为若干连续的槽（slot），每个槽的长度刚好与系统的一个页帧相同。
在大多数处理器上，是4 KiB。但较新的系统通常会使用更大的页。

本质上，系统中的任何一页都可以容纳到交换区的任一槽中
但内核还使用了一种称为聚集（clustering）的构造法，使得能够尽快访问交换区。
进程内存区中连续的页（或至少是连续换出的页）将按照特定的聚集大小（通常是256页）逐一写到硬盘上。如果交换区中没有更多空间可容纳此长度的聚集，内核可以使用其他任何位置上的空闲槽位。

如果使用了几个优先级相同的交换区，内核将使用一种 循环进程 来确保尽可能均匀地利用各个交换区

两个用户空间工具可用于创建和启用交换区，分别是mkswap（用于“格式化”一个交换分区/文件）和swapon（用于启用一个交换区）。


在换出内存页之前，内核会检查内存的使用情况，确定可用内存容量是否较低。
与同步页的情况相似，内核联合使用了如下两种机制。
- 一个周期性的守护进程（kswapd）在后台运行，该进程不断检查当前的0内存使用情况，以便在达到特定的阈值时发起页的换出操作。
- 内核在某些情况下，必须能够预期可能突然出现的严重内存不足，例如在通过伙伴系统分配一大块内存时，或创建缓冲区时。如果没有足够的物理内存可用来满足对内存的请求，内核必须尽快换出页，以期释放一些内存空间。在紧急情况下的换出操作，属于直接回收（direct reclaim）的一部分。


如果内核无法满足对内存的请求，甚至在换出页之后也是如此，那么虚拟内存子系统只有一个选择，即通过OOM（out of memory，内存不足） killer来结束一个进程。



### 18.2.3 选择要换出的页

内核实现了一种粗粒度的LRU方法，只使用了一种硬件特性，即在访问一页之后设置一个==访问位==

内核对LRU的实现基于两个链表，分别称为活动链表和惰性链表（系统中的每个内存域都有这样的两个链表）。

所有处于活动使用状态的页在一个链表上，而所有惰性页则保存在另一个链表上

内核需要定期执行均衡操作，通过上述访问位来确定一页是活动的还是惰性的

页在两个链表之间可能会发生双向转移。
但这种转移不是每访问一页都会发生，它发生的时间间隔会比较长

随着时间的推移，最不常用的页将收集到惰性链表的末尾。在出现内存不足时，内核将选择换出这些页。


18.2.4 处理缺页异常


18.2.5 缩减内核缓存

用于缩减各种缓存的方法仍然是分别实现的，因为各种缓存的结构有很大的不同，很难采用一种通用的缓存收缩算法

在内核提供了一种通用框架，来管理各种缓存收缩方法。用于缩减缓存的函数在内核中称为收缩器（shrinker），可以动态注册。
在缺乏内存时，内核将调用所有注册的收缩器来获得内存。



## 18.3 管理交换区

可以用不同的优先级来管理几个交换区

交换区既可以是本地分区，也可以是具有预定义长度的文件

在活动的系统上，可以动态添加/删除交换分区，而无须重启。


---

交换区管理的基石是mm/swap-info.c中定义的swap_info数组

mm/swapfile.c
`struct swap_info_struct swap_info[MAX_SWAPFILES];`
常数通常定义为2^5 = 32

`struct swap_info_struct` 描述了一个交换区


如何确定最优先使用的交换区呢?
mm/swapfile.c中定义了全局变量 swap_list
它是swap_list_t数据类型的一个实例，该类型是专门为 查找 第一个交换区 而定义的

内核借助lowest_bit和highest_bit成员，来管理搜索区域的下界和上界。
在lowest_bit之下和highest_bit之上，是没有空闲槽位的


内核使用extent_list和curr_swap_extent成员来实现区间（extent），用于创建假定 连续的 交换区槽位与交换文件的磁盘块之间的映射。

内核可以依赖于一个事实，即分区中的块在磁盘上是线性排列的。因而槽位与磁盘块之间的映射会非常简单。


在使用文件作为交换区时，情况会更复杂，因为无法保证文件的所有块在磁盘上是连续的。


mkswap
swapop

setup_swap_extents用于创建区间链表


## 18.4 交换缓存

内核利用了另一个==缓存==，称为交换缓存（swap cache），该缓存在 选择换出页的操作 和 实际执行页交换 的机制之间，充当协调者。
。。交换缓存， 名词，不是动词。。

交换缓存与页交换子系统其他组件的交互
![9c50b6520d7e459466db3804f3f7272a.png](../_resources/9c50b6520d7e459466db3804f3f7272a.png)


- 在换出页时，页面选择逻辑首先选择一个适当的、很少使用的页帧。该页帧缓冲在页缓存中，然后将其转移到交换缓存
- 如果换出页由几个进程在同时使用，内核必须设置进程页目录中的对应页表项，使之指向交换文件中相关的位置
- 如果换出页由几个进程在同时使用，内核必须设置进程页目录中的对应页表项，使之指向交换文件中相关的位置。在其中某个进程访问该页的数据时，该页将再次换入

因而在换入共享页时，它们将停留在交换缓存中，直至所有进程都已经从交换区请求该页，并都知道了该页在内存中新的位置为止

没有交换缓存的帮助，内核不能确定一个共享的内存页是否已经换入内存，将不可避免地导致对数据的冗余读取。


### 18.4.1 标识换出页

换出页在页表中通过一种专门的页表项来标记，特定于CPU

在换出页的页表项中，所有CPU都会存储下列信息。
- 一个标志，表明页已经换出。
- 该页所在交换区的编号。
- 对应槽位的偏移量，用于在交换区中查找该页所在的槽位。

内核必须存储交换分区的标识（也称为类型）和该交换区内部的偏移量，以便唯一确定一页
该信息保存在一个专门的数据类型中，称为swap_entry_t
```C
// <swap.h> 
typedef struct { 
  unsigned long val; 
} swp_entry_t; 
```
0-59 bit 表示偏移量
60-64 表示 交换区标识符

swapops.h
`static inline unsigned swp_type(swp_entry_t entry)`
`static inline pgoff_t swp_offset(swp_entry_t entry)`
`static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)`

内核需要能够在体系结构无关和相关的两种页表项表示之间进行切换
```C
// <swapops.h> 
static inline swp_entry_t pte_to_swp_entry(pte_t pte) 
{ 
  swp_entry_t arch_entry; 
  arch_entry = __pte_to_swp_entry(pte); 
  return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry)); 
}
```

asm-arch/pgtable.h
`__pte_to_swp_entry`是一个体系结构相关的函数


---

18.4.2 交换缓存的结构

交换缓存无非是一个页缓存
其实现的核心是swapper_space

```C
// mm/swap_state.c 
struct address_space swapper_space = { 
  .page_tree = RADIX_TREE_INIT(GFP_ATOMIC|__GFP_NOWARN), 
  .tree_lock = RW_LOCK_UNLOCKED(swapper_space.tree_lock), 
  .a_ops = &swap_aops, 
  .i_mmap_nonlinear = LIST_HEAD_INIT(swapper_space.i_mmap_nonlinear), 
  .backing_dev_info = &swap_backing_dev_info, 
};
```


---
18.4.3 添加新页

16章的 add_to_page_cache
add_to_swap
add_to_swap_cache

18.4.4 搜索一页
lookup_swap_cache

## 18.5 数据回写
writepage
swap_writepage



## 18.6 页面回收

到现在为止，我们已经解释了回写的技术细节，下面把注意力转向页交换子系统的第二个主要的方面，即==交换策略==，该策略用于确定哪些页可以从物理内存换出而同时又不会严重降低内核性能。因为该策略可以释放页帧，使得有新的内存可用于紧急需求，所以该技术==也称为==页面回收

交换策略的原理可以适用于所有没有后备存储器的页

页面回收在两个地方触发
![cd7695d1f41d0a13982718c7e6759870.png](../_resources/cd7695d1f41d0a13982718c7e6759870.png)

1. 如果内核检测到在某个操作期间内存严重不足，将调用try_to_free_pages。该函数检查当前内存域中所有页，并释放最不常用的那些。
2. 一个后台守护进程，名为kswapd，会定期检查内存使用情况，并检测即将发生的内存不足。

对每个NUMA结点来说，都有一个单独的kswapd守护进程。每个守护进程负责一个NUMA结点上所有的内存域。

在非NUMA系统上，只有一个kswapd实例，负责系统中所有的内存域。
IA-32可以有最多3个内存域：ISA-DMA内存域、普通内存域、高端内存域。

判断给定页属于哪一类是内核的一项必要工作


---
18.6.2 数据结构

页向量
内核定义了以下结构，用于将几个页群集到一个小的数组中：
```C
// <pagevec.h> 
struct pagevec { 
  unsigned nr; 
  int cold; 
  struct page *pages[PAGEVEC_SIZE]; 
}; 
```

cold成员用于帮助内核区分热页（hot page）和冷页(cold page)。
如果页的数据保存在某个==CPU的高速缓存==中，称为热页


页向量使得可以对一组page结构整体执行操作。有时候，这比对各个页单独执行操作要快。


LRU缓存
内核提供了另一个缓存，称为LRU缓存，以加速向系统的LRU链表添加页的操作。

它利用页向量来收集page实例，将其逐组置于系统的活动链表或惰性链表上。
这两个链表在内核中是一个==热点==，但必须通过自旋锁保护
为降低锁竞争的几率，新页不会立即添加到链表，而是首先缓冲到每个CPU的CPU列表上


18.6.3 确定页的活动程度

两个页标志，称为referenced和active

![d244933c97eaaadd5a0b87d56bae7e19.png](../_resources/d244933c97eaaadd5a0b87d56bae7e19.png)



1. 如果页被认为是活动的，则设置PG_active标志；否则不设置。该标志是否设置，直接对应于页所在的LRU链表，即（特定于内存域的）活动链表或惰性链表。
2. 每次访问该页时，都设置PG_referenced标志
3. PG_referenced标志以及由逆向映射提供的信息用于确定页的活动程度。关键在于，每次清除PG_referenced标志时，都会检测页的活动程度。
4. 再次进入mark_page_accessed。在它检查内存页时，如果发现PG_referenced标志位已经设置，这意味着page_referenced没有执行检查。因而对mark_page_accessed的调用必定比page_referenced更频繁，这意味着该页经常被访问。
5. 反向的转移也是可能的。如果页位于活动链表上，受到很多关注，那么通常会设置PG_referenced标志位。在页的活动减少时，如果要将其转入惰性链表，则需要两次page_referenced调用，而其中不能插入mark_page_accessed调用。

。。3 的 清除 PG_reference 是什么时候？

总而言之，该解决方案确保了内存页不会在活动链表和惰性链表之间过快地跳跃

。。

18.6.4 收缩内存域


18.6.5 隔离LRU页和集中回收

内存域中，保存在链表上的活动和不活动页都需要由一个自旋锁保护，确切地说是zone->lru_lock。

页面回收代码都属于最热、最重要的代码路径之一，因而锁竞争的几率相当高


集中回收


18.6.6 收缩活动页链表

18.6.7 回收不活动页


## 18.7 交换令牌

避免页颠簸的一种方法是交换令牌

内核向某个当前换入页的进程颁发一枚所谓的交换令牌，且整个系统内只颁发一枚。
交换令牌的好处在于，持有交换令牌的进程，其内存页不会被回收，或至少可以尽可能免遭回收。

交换令牌的实现非常简单，大约只包括100行代码，这再次证明了，好主意不见得是复杂的。

交换令牌通过一个全局指针实现，该指针指向当前拥有令牌的进程的mm_struct实例
```C
// mm/thrash.h 
struct mm_struct *swap_token_mm; 
static unsigned int global_faults; 
```

mm_types.h
struct mm_struct

交换令牌通过调用grab_swap_token获取
mm/thrash.c 
void grab_swap_token(void) 


如果请求交换令牌的进程已经持有令牌（第二个else分支），这意味着该进程会换入大量内存页。相应地，由于进程对内存页的需求非常强烈，因而应该==增加令牌优先级==。
如果此时是另一个进程持有令牌，那么在当前进程的等待时间已经不少于其上一次等待时间时，将该进程的令牌优先级加1，否则将其令牌优先级减1。
如果当前进程的令牌优先级超出持有者的优先级，那么从持有者去掉交换令牌，赋予当前请求进程。


## 18.8 处理交换缺页异常

18.8.1 换入页
预读
获取交换令牌

18.8.2 读取数据

18.8.3 交换预读


## 18.9 发起内存回收

18.9.1 用kswapd进行周期性内存回收
kswapd是一个内核守护进程，每当系统启动时由kswap_init激活
mm/vmscan.c 
int kswapd_run(int nid)

对每个NUMA内存域，都会激活一个独立的kswapd实例。


18.9.2 在严重内存不足时换出页

try_to_free_pages例程用于紧急、非预期的内存回收操作。


## 18.10 收缩其他缓存
除了页缓存之外，内核还管理着其他缓存，这些缓存通常基于第3章讨论的slab

slab管理着常用的数据结构，以确保伙伴系统中按页管理的内存能够得到更有效的使用，并通过缓存，来更快速而容易地分配数据类型的实例


使用此类缓存的内核子系统可以向内核动态地注册“收缩器”函数。这些函数在可用内存较低时调用，释放一些已用的内存空间


# ch19 审计 882


# 附录A

内核严格隔离了体系结构相关和体系结构无关的代码

内核中特定于处理器的部分，包含定义和原型的头文件保存在include/asm-arch/目录下，C语言和汇编程序源程序码实现则保存在arch/arch/目录下

基本上有两类特定于体系结构的代码
- 专门由 内核 特定于体系结构的部分 使用和调用的组件。
- 每个体系结构都必须定义的接口函数，由 体系结构 无关的代码 调用。例如，每个移植版都必须提供一个switch_to函数，用于在两种进程之间切换时维护硬件控制方面的细节



内核区别下列3种基本数据类型。
- C语言程序所用的标准数据类型。例如unsigned long、void *、char。C语言标准并==未固定这些类型的比特位数目==。只保证一些不等式是成立的。例如，unsigned long的比特位数不少于int的。
- 具有==固定数目比特位==的数据类型内核提供了特殊的整数类型，具有预定义的比特位数，如u32和s16（u表示无符号，s表示有符号）。
- 特定于子系统的类型，从来都不会直接操作，而总是使用专门编写的函数。

位长固定的数据类型定义在 asm-arch/types.h 中。



将数据对齐到特定的内存地址，对于高效使用处理器高速缓存和提升性能，都很有必要

通常，对齐是指对齐到数据类型的字节长度可整除的字节地址。在某些情况下，会要求稍大一些的对齐尺度。

在内核中某些地方，可能需要访问非对齐的数据类型。各体系结构必须为此定义两个宏（在asm-arch/unaligned.h中）。
- get_unaligned(ptr)对位于非对齐内存位置的一个指针，进行反引用操作。
- put_unaligned(val, ptr)向ptr指定的一个非对齐（因而不适合直接访问）内存位置写入值val。


## A.4 内存页面
在许多（并非所有）体系结构上，内存页长度为4 KiB。更现代的处理器也支持长达几MiB的页。在特定于体系结构的文件asm-arch/page.h中必须定义以下宏，来表示所使用的页长度。
- PAGE_SHIFT指定了页长度以2为底的对数。内核隐式假定页长度可以表示为2的幂次，在内核支持的所有体系结构上，这一点都成立。
- PAGE_SIZE指定了内存页的长度，单位为字节。
- PAGE_ALIGN(addr)可以将任何地址对齐到页边界。

还必须实现对页的两个标准操作，通常是通过优化的汇编指令来完成。
- clear_page(start)删除从start开始的一页，并将其中填充0字节。
- copy_page(to, from)将from处的页数据复制到to处。


PAGE_OFFSET宏指定了物理页帧在虚拟地址空间中映射到的位置。在大多数体系结构上，这隐含地定义了用户地址空间的长度，或将整个地址空间划分为内核地址空间和用户地址空间。但这并不适用于所有体系结构
Sparc就是其中一个例外情况
AMD64是另一个例外，因为其虚拟地址空间在中部有一个不可寻址的空洞。

因而，必须使用asm-arch/process.h中定义的 TASK_SIZE 常数来确定用户空间的长度，而不能使用PAGE_OFFSET。



## A.5 系统调用

发出系统调用的机制，实际上是==进行一个从用户空间到内核空间的可控切换==，在所有支持的平台上都有所不同。
但标准文件 asm-arch/unistd.h 负责与系统调用相关的以下两方面任务。
- 它定义了预处理器常数，将所有 ==系统调用的描述符 关联到 符号常数==。这些常数的名称诸如__NR_chdir和__NR_send等
- 它定义了在内核内部调用系统调用所用的函数。通常，为此使用了一种预处理器机制，连同用于自动生成代码的内联汇编


## A.6 字符串处理
所有体系结构在 asm-arch/==string.h== 中都定义了自身的各种字符串操作。
- int strcmp(const char * cs, const char * ct)
  逐字符比较两个字符串。
- int strncmp(const char * cs,const char * ct,size_t count) 
  类似于strcmp，但最多只比较count个字符。
- int strnicmp(const char *s1, const char *s2, size_t len) 
  类似于strncmp，但比较字符时不考虑大小写。
- char * strcpy(char * dest,const char *src) 
  将一个0结尾字符串从src复制到dest。
- char * strncpy(char * dest,const char *src,size_t count) 
  类似于strcpy，但复制的最大长度不超过count个字节或字符。
- size_t strlcpy(char *dest, const char *src, size_t size) 
  类似于strncpy，但如果源字符串的字符数目多于size，那么目标字符串仍然是以0结尾的。
- char * strcat(char * dest, const char * src) 
  将src字符串附加到dest字符串。
- char * strncat(char *dest, const char *src, size_t count) 
  类似于strcat，但最多只复制count个字节
- size_t strlcat(char *dest, const char *src, size_t count) 类似于strncat，但结果字符串的长度（并非所复制的字节数目）不超出count个字节。
- char * strchr(const char * s, int c) 
  在字符串s中查找字符c出现的第一个位置。
- char * strrchr(const char * s, int c) 
  在字符串s中查找字符c出现的最后一个位置。
- size_t strlen(const char * s) 
  确定一个0结尾字符串的长度。
- size_t strnlen(const char * s, size_t count) 
  类似于strlen，但长度最大不超过count。
- size_t strspn(const char *s, const char *accept) 
  计算s中完全由accept中字符组成的子串的长度。
- size_t strcspn(const char *s, const char *reject) 
  类似于strspn，但计算的是s中完全不包含reject中字符的子串的长度。
- char * strstr(const char * s1,const char * s2) 
  在s1中查找子串s2。
- char * strpbrk(const char * cs,const char * ct) 
  查找字符串ct中的字符在字符串cs中出现的第一个位置。
- char * strsep(char **s, const char *ct) 
  将字符串划分为由ct分隔的标记。


下列操作作用于一般的内存区，而非字符串。
- void * memset(void * s,int c,size_t count) 
  用c指定的值，从地址s开始，填充count个字节。
- memset_io 所做的工作是相同的，但用于I/O内存区。
- char * bcopy(const char * src, char * dest, int count) 
  将一个长度为size的内存区从src复制到dest。
- memcpy_fromio所做的工作相同，但它是将数据从I/O地址空间的一个区域复制到普通的地址空间。
- void * memcpy(void * dest,const void *src,size_t count) 
  类似于bcopy，但使用void指针作为参数来定义所涉及的区域。
- void * memmove(void * dest,const void *src,size_t count) 
  类似于memcpy，但也适用于重叠的源和目标内存区。
- int memcmp(const void * cs,const void * ct,size_t count) 
  逐字节比较两个内存区。
- void * memscan(void * addr, int c, size_t size) 
  扫描由addr和size指定的内存区，查找字符c第一次出现的位置。
- void *memchr(const void *s, int c, size_t n) 
  类似于memscan，但如果未找到字符c，则返回NULL指针（memscan如果没有找到，则会返回一个指向扫描区域之后第一个字节的指针）。

所有这些操作，都是用来替换==用户空间==中所用的==C标准库的同名函数==，以便在内核中执行同样的任务。

对于每个由体系结构自身以优化形式定义的字符串操作来说，都必须定义相应的__HAVE_ARCH_OPERATION宏。
例如，对memcpy必须设置__HAVE_ARCH_MEMCPY。
体系结构相关代码未能实现的所有函数，都替换为lib/string.c中实现的体系结构无关的标准操作。


## A.7 线程表示

一个运行进程的状态，主要由处理器==寄存器==的内容定义。

当前未运行的进程，必须将该数据保存在相应的数据结构中，以便在调度器激活进程时，从中读取数据并==迁移到适当的寄存器==

- asm-arch/ptrace.h 中定义了用于保存==所有寄存器==的==pt_regs==结构，在进程因为系统调用、中断、或任何其他机制由用户状态切换到核心态时，会将保存各寄存器值的pt_regs结构实例放置在内核栈上。该文件也通过预处理器常数定义了各寄存器在栈上的顺序。在跟踪调试一个进程而需要从栈读取寄存器值时，这是必需的。
- asm-arch/processor.h中包含了==thread_struct==结构，它用于描述所有==其他寄存器==和所有==其他进程==状态信息。该结构通常进一步分为很多特定于处理器的部分。
- asm-arch/thread.h中定义了==thread_info==结构（不要与thread_struct混淆），其中包含了为实现==进入和退出内核态==、汇编代码必须访问的所有task_struct成员。


A.7.1 IA-32
A.7.2 IA-64
A.7.3 ARM
A.7.4 Sparc64
A.7.5 Alpha
A.7.6 Mips
A.7.7 PowerPC
A.7.8 AMD64



A.8 位操作和字节序

asm-arch/bitops.h

- void set_bit(int nr, volatile unsigned long * addr) 
  将位置nr的比特位置位，计数从addr开始。
- int test_bit(int nr, const volatile unsigned long * addr) 
  检查指定的比特位是否置位。
- void clear_bit(int nr, volatile unsigned long * addr) 
  清除位置nr上的比特位（计数从addr开始）。
- void change_bit(int nr, volatile unsigned long * addr) 
  将位置nr处的比特位取反（计数从addr开始）。换言之，置位者被清除，反之亦然。
- int test_and_set_bit(int nr, volatile unsigned long * addr) 
  将一个比特位置位，并返回该比特位此前的值。
- int test_and_clear_bit(int nr, volatile unsigned long * addr) 
  将一个比特位清除，并返回该比特位此前的值。
- int test_and_change_bit(int nr, volatile unsigned long* addr) 
  将一个比特位取反，并返回该比特位此前的值


所有这些函数的执行都是==原子的==，因为锁语句已经集成到汇编代码中。
这些函数==非原子==版本，前缀为双下划线（例如，__set_bit）


字节序
内核提供
- byteorder/big_endian.h
- byteorder/little_endian.h

当前处理器版本
- asm-arch/byteorder.h

默认的C函数
- byteoorder/swab.h


## A.9 页表

内核提供一个将各种体系结构不同点抽象出去的内存模型
asm-arch/pgtable.h

第3章已经讨论了该文件中的大部分定义，所以无须在此重复。


## A.10 杂项

### A.10.1 校验和

对数据包计算校验和，是通过IP网络通信的关键
如果可能的话，每个体系结构都应该采用==人工优化的汇编==代码来计算校验和

asm-arch/checksum.h
- unsigned short ip_fast_csum 根据IP报头和包头长度计算必要的校验和。
- csum_partial根据依次接收的各个分片，为每一个数据包计算校验和。

### A.10.2 上下文切换

在调度器决定通知当前进程放弃CPU以便另一个进程运行之后，会进行上下文切换中硬件相关的部分。
为此，所有体系结构都必须提供switch_to函数或对应的宏，原型如下，声明定义在

asm-arch/system.h
`void switch_to(struct task_struct *prev, struct task_struct *next, struct task_struct *last)`

该函数执行上下文切换，保存prev指定的进程的状态，并激活由next指定的进程。


### A.10.3 查找当前进程

current宏用于找到一个指向当前运行进程的task_struct的指针。

每个体系结构都必须在 asm-arch/current.h 中声明该宏
该指针保存在一个独立的处理器寄存器中，可以使用current直接或间接地查询




# 附录B 使用源代码
B.1 内核源代码的阻止
B.2 Kconfig进行配置
B.3 Kbuild编译内核

## B.4 有用的工具

### B.4.1 LXR

LXR是一个交叉引用工具。它分析内核源代码并生成一个HTML形式的超文本表示，供浏览器查看。
LXR使得用户可以查找变量、函数及其他符号，并可以跳转到其在源代码中的定义处，还可以列出所有使用该符号的位置。这在跟踪内核中的代码控制流路径时很有用。

为在本地使用LXR，需要一个浏览器和一个Web服务器，最好是Apache。为查找源代码中的随机字符串，还需要glimpse搜索引擎。
LXR的规范版本可以从sourceforge.net/projects/lxr下载。遗憾的是，该版本多年来没有什么发展，尽管其代码工作得还行，但缺少一些现代Web应用程序的特性。
LXR有一个试验版本，目前出于活跃维护状态，可以从git存储库git://lxr.linux.no/git/lxrng.git获取。它比规范版本提供了更多的特性

。。https://lxr.linux.no/#linux+v6.7.1/   可以啊。一直在维护。




B.5 调试和分析内核

GDB GNU Debugger
- info gdb
- gcc -g test.c

GDB能够完成下列工作。
- 逐行跟踪程序执行，可以按源代码逐行执行或按汇编语句逐行执行。
- 确定程序中使用的所有符号类型。
- 显示或操作符号的当前值。
- 反引用程序中的指针或访问随机的存储单元，以读取或修改其值。
- 设置断点，使得程序在执行到源代码中给定位置时暂停，同时启用调试器。
- 设置条件断点，在给定条件满足时暂停程序执行。例如，当某个变量的值设置为预定义值时。

GDB也是基于文本的。这有优点也有缺点
- 在文本界面下，无法通过图形指针可视化显示数据结构之间的关系。
- GDB的源代码视图也不怎么理想，因为它只能显示很短一段源代码。

DDD

https://www.gnu.org/software/ddd/
Data Display Debugger

为了改进GDB的这些不足，现在所有流行的发行版都包含了DDD。
作为X11下的一个图形化工具，它弥补了GDB的不足。DDD是GDB的一个用户界面，因而也支持GDB的所有特性。因为在DDD中也可用直接输入所有的GDB命令，所有选项都是可用的，不仅仅是那些直接集成到图形用户界面中的特性。


proc文件系统包含了一个文件，名为kcore。其中包含了内核当前状态的映像，格式为ELF内存转储格式（参见附录E）。
因为GDB内存转储文件是可以读取和处理的，因而可以与内核及其调试符号联用，来可视化显示数据结构并读取其内部状态。
GDB内存转储文件通常用于用户空间程序的事后分析，以查明其崩溃原因。

必须用内核映像（包含调试符号）的名称和kcore文件的名称作为参数来调用DDD：
`wolfgang@meitner> ddd /home/wolfgang/linux-2.6.24/vmlinux /proc/kcore`

这必须由root用户来完成，或修改/proc/kcore的访问权限

尽管不能对运行内核设置断点或类似项，DDD很适于考察系统的数据结构


### B.5.3 KGDB

https://elinux.org/Kgdb

两台通过网络或串行电缆连接的机器，提供了一个更好的调试环境，此时可用的选项几乎与调试普通应用程序相同。
KGDB补丁在内核中安装一段简短的存根代码，向在第二台系统上运行的调试器提供了一个接口。
因为GDB支持远程调试，内核可以利用这种调试形式，来提供断点、单步跟踪等特性。


B.6 用户模式Linux
内核在Linux系统上以==一个用户空间进程==的形式运行

这方便了许多应用程序，这些应用很难于运行于真实硬件上的经典Linux内核上实现。
特别是测试新的内核特性，而又无须频繁重启系统这样的需求。


# 附录C 有关C语言的注记

编译器的工作分为以下几个阶段
- 预处理
  从源文件和使用#include包含的所有头文件，生成一个 大的输入文件。接下来，编译器就不必考虑C语言程序分布在几个源文件中的问题了。
- 扫描和解析
  扫描器，又称词法分析器，分析源文件文本，查找关键字
  解析器，又称语法分析器，获得扫描器的输出，根据语法检查结构是否正确
- 中间代码生成
  将扫描器和解析器 建立的 ==语法分析树== 转换为另一种语言，称为 ==寄存器传输语言==(register transfer language, RTL)。这是一种用于理想机器的汇编语言。这种语言是可以优化的。
  RTL的语句已经非常底层，处于 C 和汇编 之间。主要任务是 操作寄存器值。
- 优化
  编译期间，==计算最密集==的阶段就是对 以RTL编写的中间代码进行优化。
  优化有很多，最重要的包括：
  - 对算术表达式的简化，对表达式进行==代数重写==
  - 消除死代码
  - 合并同一个程序中重复的表达式和代码项
  - 重写控制流使之更高效
  - 等等
- 代码生成
  最后一个阶段，只关注针对目标处理器生成实际的汇编代码。
  但这里并==不生成==一个可执行==二进制文件==，而是==产生一个汇编指令组成的文本文件==，由其他外部程序（==汇编器==和可能的==链接器==）==转换为二进制机器代码==。
  原则上，汇编代码与程序最终的机器代码是等同的


GNU汇编器采用了AT & T语法，而不是更流行的Intel/Microsoft语法形式。
当然，这两种语法实现了相同的功能，只是对源寄存器和目标寄存器的排列不同，所用的常数寻址方式也不同


C.1.2 汇编和链接

在实际编译过程末尾，原来的C语言程序已经被转换为汇编代码，而最后一步向二进制代码的转换基本上不需要编译器的工作，因为剩余的工作由汇编器和链接器（通常也称之为binder，联接器）完成。

汇编器的工作非常简单。
各个汇编语句（及其参数）被转换为依处理器类型而不同的专用二进制格式

汇编器的另一个任务是将常数（如固定的字符串或数值常数）放置到二进制代码中。
Linux下通常使用ELF格式（将在附录E中详细讲述）在二进制文件中保存程序代码和数据。

链接器必须调整汇编代码中的分支地址。
尽管==汇编==语言源代码中仍然==可以引用符号名称==（例如，前述的汇编代码调用了标准库中定义的printf函数），==但二进制机器码==则==必须指定相对或绝对的分支地址==。


C.1.3 过程调用

C语言中一个有趣的方面是过程和函数调用的实现，这不是特定于GNU编译器的

系统栈（system stack）是一个内存区，位于进程地址空间的末端,在将数据压栈时，栈自顶向下增长,该内存区用于为函数的局部变量提供内存

如果调用了嵌套的过程，栈会自上而下增长，并接受新的 活动记录（activation record）来保存一个过程所需的所有数据

当前执行过程的活动记录，由标记顶部位置的帧指针（frame pointer）和标记底部位置的栈指针（stack pointer）定义。在过程执行时，虽然其顶部的限制是固定的，但底部的限制是可以扩展的（在需要更多空间时）。

。。帧指针在上面，所以更靠近stack的底，并且不可变。 栈指针，在下面 是操作的地方，可变。

![4f04f53ab853c9db001e4327c38f77bd.png](../_resources/4f04f53ab853c9db001e4327c38f77bd.png)


第二个栈帧
- 在栈帧顶部的是返回地址，以及保存的帧指针值。
  返回地址指定了 当前过程结束时 代码的控制流 转向的内存地址，
  保存的帧指针是前一个活动记录的帧指针。当前过程结束后，该帧指针值可以用于重建调用过程的栈帧。
- 活动记录的主要部分是 为过程 局部变量 分配的空间。C中，这种变量也称为 自动变量
- 函数调用时 以参数形式传递到函数的值，存储在栈的底部。

所有常见的体系结构都提供了以下两个栈操作指令
- push，将一个值放置在栈上，并将 栈指针 减去该值 所占用的内存字节数。栈的末端下移到更低的地址
- pop，从栈删除一个值，并相应增加栈指针的值。也就是说，栈的末端上移。

还提供了一下2个指令，用于调用 和 退出函数，它们会自动操作栈
- call， 将指令指针的 当前值 压栈，跳转到 被调用函数的 起始地址
- return， 从栈上弹出返回地址，并跳转到该地址

过程调用 由下面2个步骤组成
- 在栈中建立参数列表。传递到被调用函数的第一个参数最后入栈。这使得可以传递可变数目的参数，然后将其从栈上逐一弹出(pop)。
- 调用call，这将指令指针的当前值（call之后的下一条指令）压栈，代码的控制流转向被调用的函数


被调用的过程负责管理帧指针，需要执行下列步骤。
(1) 前一个帧指针压栈，因而栈指针下移。
(2) 将栈指针的当前值赋值给帧指针，标记当前执行函数的栈区的起始位置。
(3) 执行当前函数的代码。
(4) 在函数结束时，存储的原帧指针位于栈的底部。其值从栈弹出到帧指针寄存器，使之指向前一个函数的栈区起始位置。现在，对当前函数执行call指令时压栈的返回地址位于栈底。
(5) 调用return，将返回地址从栈弹出。处理器转移到返回地址，代码的控制流也返回到调用函数。




### C.1.6 属性

属性向编译器提供了有关函数或变量用法的详细信息。这使得编译器可以应用更准确的优化选项，以生成质量更好的代码，或者容许以普通的C语言所无法表达的形式

本节将讲述内核所使用的属性

关键字是__attribute__((list))

```C
int add (int a, int b) __attribute__((regparam(3)); 
struct xyy { } __attribute((__aligned__(SMP_CACHE_BYTES))
```

- noreturn，
- regparam，指定以寄存器传递参数，而不是像通常那样使用栈
- section，允许编译器将变量和函数置于二进制文件的不同于通常设置的其他段中
- align，指定了数据对齐的最低限度。 该属性很重要，因为它通过将结构的关键部分放置到内存中最恰当的位置，来最大限度地发挥CPU高速缓存的作用。


### C.1.7 内联汇编
将汇编代码直接集成到C代码中
编译器来承担联合代码生成的工作

asm语句用于指定汇编代码和所用的寄存器。其语法如下（还可以使用等价的__asm__关键字）
```C
asm ("Assembler code"; 
  : Output operand specification 
  : Input operand specification 
  : Modified registers 
);
```

在IA-32系统上，汇编代码本身必须以AT & T表示法给出

就本附录的目的而言，将AT&T汇编语法总结为以下五条规则，就足够了。

- 寄存器通过在名称前加百分号前缀引用。例如，为使用eax寄存器，汇编代码中将使用%eax。
  在C源代码中必须指定两个百分号，才能在转给汇编器的输出中形成一个百分号。
- 源寄存器总是在目标寄存器之前指定。
  例如，在mov语句中，这意味着mov a, b将寄存器a的内容复制到寄存器b。
- 操作数的长度由汇编语句的后缀指定。
  b代表byte，l代表long，w代表word。
  为在IA-32系统上将一个长整数从eax寄存器移动到ebx寄存器，需要指定movl %eax, %ebx
- 间接内存引用（指针反引用）需要将寄存器包含在括号中。
  例如，movl (%eax), %ebx将寄存器eax的值指定的内存地址中的长整数值复制到寄存器ebx
- offset(register)指定寄存器值与一个偏移量联用，将偏移量加到寄存器的实际值上。
  例如，`8(%eax)`指定将 eax+8 用作一个操作数。该表示法主要用于内存访问，例如指定与栈指针或帧指针的偏移量，以访问某些局部变量

```C
int move() { 
  int a= 5; 
  int b; 
  asm ("movl %1, %%eax; 
    movl %%eax, %0;" 
    : "=r"(b) /* 输出寄存器 */ 
    : "r" (a) /* 输入寄存器 */ 
    : "%eax"); /* 修改的寄存器 */ 
  printf("b: %u\n", b); 
}
```
GCC不能检查asm部分的代码是否是特定平台的正确汇编指令，也不能检查所使用的寄存器是否实际上适用于特定的应用程序。这完全是程序员的责任。

输入和输出寄存器通过约束定义，形式如下：
`"constraint" (variable)`
- r, 表示使用了一个通用寄存器。
- m,指定使用了内存中的一个地址。
- I,J,在IA-32系统上定义了一个位于0～31或0～63的常数。这可以用于移位操作
- =,操作数是只写的。丢弃前一个值，替换为操作的输出值。
- +,操作数是读写的

r 指定了寄存器， 具体哪个寄存器 由 编译器确定。 所以编码时 使用 %0, %1 来表示 相关的寄存器

```C
// include/asm-x86/bitops_32.h 
static inline void set_bit(int nr, volatile unsigned long * addr) 
{ 
  __asm__ __volatile__( LOCK_PREFIX 
  "btsl %1,%0" 
  :"+m" (ADDR) 
  :"Ir" (nr)); 
} 
```

事实上，内联代码中不仅仅能够使用单条汇编语句。例如，在Alpha CPU上，将整型变量原子化加1是一个复杂操作，如下所示：
```C
include/asm-alpha/atomic.h 
static __inline__ void atomic_add(int i, atomic_t * v) 
{ 
  unsigned long temp; 
  __asm__ __volatile__( 
    "1: ldl_l %0,%1\n" 
    " addl %0,%2,%0\n" 
    " stl_c %0,%1\n" 
    " beq %0,2f\n" 
    ".subsection 2\n" 
    "2: br 1b\n" 
    ".previous" 
    :"=&r" (temp), "=m" (v->counter) 
    :"Ir" (i), "m" (v->counter)); 
} 
```


### C.1.8 __builtin函数

__builtin函数向编译器提供了其他选项，可以执行C语言常规==能力范围之外==的操作，又不必借助于内联汇编

每个体系结构都定义了自身的__builtin函数集合，详情可参见GCC文档

有若干__builtin函数变体对所有体系结构是共同的，内核使用了其中两个。

- __builtin_return_address(0)
  获得函数的返回地址，即函数结束时控制流将定位到的目标地址。
  该信息也可以从活动记录取得。这实际上是一个特定于体系结构的任务，但这里的__builtin函数为该功能提供了一个通用的前端。
  参数指定了该__builtin函数应该在活动记录中向上多少层。0表示当前运行函数将返回的地址，1表示调用当前函数的函数将返回的地址，依次类推。
  在某些体系结构上（如IA-64），确定活动记录有根本性的困难。为此，该函数对大于0的参数总是返回0。

- __builtin_expect(long exp, long c)
  帮助编译器优化分支预测。exp指定一个将计算的表达式的结果值，而c返回预期结果：0或1

内核定义了以下两个宏，来标识代码中很可能和不太可能的分支：
compiler.h
#define likely(x) __builtin_expect(!!(x), 1) 
#define unlikely(x) __builtin_expect(!!(x), 0) 

使用双重否定!!符号，有下面两个理由：
- 它使得宏可以用于隐式转换为真值的指针；
- 大于0的真值（C语言明确允许这种做法）标准化为1，这是__builtin_expect所预期的值。

。。 !! 6


C.1.9 指针运算
通常在C语言中，仅当指针具有显式类型时，才可用于计算。例如int *或long *。否则，不可能确定指针加1操作的语义。
GNU编译器拓宽了该限制，也支持==void指针==和==函数指针==的运算，内核在多处用到。在这两种情况下，加1操作的语义是==增加1字节==


## C.2 内核的标准数据结构和技术

C.2.1 引用计数器

需要长期使用的数据结构实例由内核在其动态内存空间中分配。在实例不再需要后，分配的内存空间可以返还给内存管理子系统

在几个进程或内核线程访问同一实例时，因为资源的共享，使情况变得复杂。
写时复制方法和通过进程分支来共享使用不同的进程资源，都是几个地方需要使用同一数据结构实例的例子。
在这种情况下，内核并不知道该数据何时不再需要，因而无法判断返还相关内存空间的时机。

为解决这个问题，内核采用了硬链接实现所用的一种技术。
数据结构中具有一个使用或==引用计数器，表示内核中有多少位置在使用该资源==。
使用计数器是一个原子化的整型变量，嵌入数据结构中某处，通常名为count


C.2.2 指针类型转换
在可移植C应用程序中，一个常见的错误来源于假定整数和指针类型的长度相同，可进行类型转换

。32位系统，int 和 指针 长度相同
。64位， int 是 32bit， 指针64bit


C.2.3 对齐问题
大多数现代RISC机器都强制要求内存访问是自然对齐的：一个基本类型数据所存储的位置，必须能够被数据类型的宽度整除

但如果访问任意地址上的内存，可能是非对齐的位置，则必须采用以下两个辅助函数：
- get_unaligned(ptr)，用于读取非对齐指针；
- put_unaligned(val, ptr)，将值val写入到非对齐的内存位置，由ptr表示

如果必须避免填充，例如，可能一个结构用于与外部设备交换数据，必须按数据结构的原定义接收数据，就可以在结构定义中指定 ==__packed== 属性，防止编译器加入填充字节
这样的结构中，那些可能未对齐的部分只能使用前述的两个函数访问


有时，除了自然对齐，还必须满足==其他的对齐条件==。
例如在某个数据结构必须沿缓存行对齐时，但在内存管理的实现中，还有大量的其他应用。
内核为此提供了ALIGN(x, y)宏：它返回将数据x对齐到y字节边界所需空间长度的最小值。此前在第3章，表3-9给出了这个宏用法的一些例子。


C.2.4 位运算

内核定义了辅助函数DECLARE_BITMAP来创建一个有足够空间的位图，以存储由bits
参数给定数目的比特位。
```C
// <types.h> 
#define DECLARE_BITMAP(name,bits) \ 
    unsigned long name[BITS_TO_LONGS(bits)] 
```


C.2.5 预处理器技巧

内核使用了两个不常用的结构

出现在字符串内部的宏参数通常不会进行替换。
如果需要根据一个参数来生成一个字符串，则必须使用专门的预处理器功能，称之为字符串化（stringification）。
==字符串内部==如果有需要替换为宏参数的，必须以#作为前缀

```C
#define warning(text)\ 
  printf("Warning: " #text "\n") 
```

`warning(foobar not found);`
预处理器展开后，将产生以下输出：
`printf("Warning: " "foobar not found" "\n");`

如果函数（其名称有一部分通过宏参数指定）借助于预处理器定义，则必须利用预处理器的连接功能（concatenation）。
该功能在下面的例子中说明，该例子取自内核中针对各种数据类型定义的端口I/O函数。
两个#用于在进行预处理器替换时，将两个连续的标记合成为一个复合标记。

```C
// include/asm-x86/io_32.h 
#define BUILDIO(bwl,bw,type) \ 
static inline void out##bwl##_local(unsigned type value, int port) { \ 
  __asm__ __volatile__("out" #bwl " %" #bw "0, %w1" : : "a"(value), "Nd"(port)); \ 
}
```

`BUILDIO(b,b,char)`
在预处理器进行处理之后，C语言文件包含下列代码（为便于阅读，已经增加了额外的换行符）：
```C
static inline void outb_local(unsigned char value, int port) { _ 
_asm__ __volatile__("out" "b" " %" "b" "0, %w1" 
  : 
  : "a"(value), "Nd"(port)); 
} 
```


C.2.6 杂项

内核中经常包括以下种类的宏：
```C
drivers/block/ataflop.c 
#define FDC_WRITE(reg,val) \ 
  do { \ 
    dma_wd.dma_mode_status = 0x80 | (reg); \ 
    udelay(25); \ 
    dma_wd.fdc_acces_seccount = (val); \ 
    MFPDELAY(); \ 
  } while(0) 
```
do语句在形式上保证宏被“调用”时代码只执行一次，与没有包含在do循环中的形式相比，并未改变语义。

。将一组代码 组合在一起， 宏替换时，不需要考虑 {} 的问题



### C.2.7 双链表
链表的起点是下列数据结构，它可以嵌入到其他数据结构中：
```C
// <list.h> 
struct list_head { 
  struct list_head *next, *prev; 
};

/* 
* 在两个连续的链表项之间添加一个新项。
* 
* 该函数只用于链表内部处理，适用于已经知道前后两个链表项prev/next的情况
*/ 
static inline void __list_add(struct list_head *new, 
    struct list_head *prev, 
    struct list_head *next) 
{ 
  next->prev = new; 
  new->next = next; 
  new->prev = prev; 
  prev->next = new; 
}

/** 
* list_add：添加一个新的链表项
* @new: 需要添加的新链表项
* @head: 添加在head链表项之后
* 
* 在指定的链表元素之后插入一个新的链表项。
* 这对实现栈是有用的。
*/ 
static inline void list_add(struct list_head *new, struct list_head *head) 
{ 
  __list_add(new, head, head->next); 
} 


#define LIST_POISON1 ((void *) 0x00100100) 
#define LIST_POISON2 ((void *) 0x00200200) 
/* 
* 通过使链表项的prev/next指向彼此，来删除一个链表项。
* 
* 这只用于内部的链表处理，这种情况下，链表项的prev/next项都是已知的！
*/ 
static inline void __list_del(struct list_head * prev, struct list_head * next) 
{ 
  next->prev = prev; 
  prev->next = next; 
} 
/** 
* list_del：从链表删除一项。
* @entry: 需要从链表删除的项。
* 请注意：此后，对该链表项调用list_empty并不返回true，此时链表项处于未定义状态。
*/ 
static inline void list_del(struct list_head *entry) 
{ 
  __list_del(entry->prev, entry->next); 
  entry->next = LIST_POISON1; 
  entry->prev = LIST_POISON2; 
}
// 删除项的next和prev指针指向的两个LIST_POISON值用于调试，以便在内存中检测已经删除的链表元素

// #define list_for_each_entry(pos, head, member)

```



### C.2.8 散列表

内核还提供了一个双链表的修改版本，特别适用于实现散列表中的溢出链表。在这种情况下，链表元素也嵌入到其他数据结构中，但表头和链表元素是不对称的：

```C
// <list.h> 
struct hlist_head { 
  struct hlist_node *first; 
};

struct hlist_node { 
  struct hlist_node *next, **pprev; 
}; 
```

### C.2.9 红黑树
在速度和实现复杂度之间，提供了一个很好的结合

红黑树是具有下列特征的二叉树
- 每个结点或红或黑。
- 每个叶结点（或树边缘的结点）是黑色的。
- 如果结点为红色，那么两个子结点都是黑色。因而，在从树的根结点到任意叶结点的任何路径上，都不会有两个连续的红色结点，但可能有任意数目的连续黑色结点。
- 从一个内部结点到叶结点的简单路径上，对所有叶结点来说，黑色结点的数目都是相同的。

```C
// <rbtree.h> 
#define RB_RED 0 
#define RB_BLACK 1 
struct rb_node 
{ 
  unsigned long rb_parent_color; 
  int rb_color; 
  struct rb_node *rb_right; 
  struct rb_node *rb_left; 
} __attribute__((aligned(sizeof(long)))); 
```

rb_parent_color中：只有一个比特位用于表示两种==颜色==，该信息包含在rb_parent_color最低的比特位中。该变量其余的比特位用于保存==父结点指针==

因为在所有体系结构上，指针都至少要求对齐到4字节边界，因而最低的两个比特位保证为0

```C
// <rbtree.h> 
#define rb_parent(r) ((struct rb_node *)((r)->rb_parent_color & ~3))

#define rb_color(r) ((r)->rb_parent_color & 1) 

#define rb_is_red(r) (!rb_color(r)) 
#define rb_is_black(r) rb_color(r) 
#define rb_set_red(r) do { (r)->rb_parent_color &= ~1; } while (0) 
#define rb_set_black(r) do { (r)->rb_parent_color |= 1; } while (0) 

#define rb_entry(ptr, type, member) container_of(ptr, type, member) 
```



### C.2.10 基数树

```C
// lib/radix-tree.c 
#define RADIX_TREE_MAP_SHIFT (CONFIG_BASE_SMALL ? 4 : 6) 
#define RADIX_TREE_MAP_SIZE (1UL << RADIX_TREE_MAP_SHIFT) 
#define RADIX_TREE_MAP_MASK (RADIX_TREE_MAP_SIZE-1) 

struct radix_tree_node { 
  unsigned int height; /* 从底部开始计算的高度 */ 
  unsigned int count; 
  struct rcu_head rcu_head; 
  void *slots[RADIX_TREE_MAP_SIZE];
  unsigned long tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS]; 
}; 
```
slots是一个指针数组，根据结点在树中的层次，指向其他结点或数据元素。
count表示数组中已占用的数组项的数目。
代码片段中定义的宏，指定了每个结点中包含的数组项的数目。
默认情况下，内核使用2^6 = 64。空的数组项设置为NULL指针。

每个树结点都可以与标记关联，标记对应于一个置位或未置位的比特位。每个结点最多有RADIX_TREE_MAX_TAGS个不同的标记，默认值是2。这对页缓存的使用已经足够了。

RCU机制（在第5章讲过）用于对基数树进行==无锁查找==。

树的根结点由下列数据结构定义
```C
// radix-tree.h
struct radix_tree_root { 
  unsigned int height; 
  gfp_t gfp_mask; 
  struct radix_tree_node *rnode; 
};
```

。。跳



# 附录D 内核启动

类似于任何其他程序，内核在执行通常的任务之前，会经历一个加载和初始化的阶段。

启动阶段划分为以下3个部分。
- 内核载入物理内存，并创建最小化的运行时环境。
- 转移到内核中（平台相关）的机器码，并初始化基本的系统功能，初始化代码是特定于系统的，用汇编语言编写。
- 转移到初始化代码中平台无关的部分，用C语言编写，并完成所有子系统的初始化，最后切换到正常运作模式

通常，由启动装载程序负责第一阶段
只有深入了解特定处理器的特性和问题，才能理解第一阶段的所有细节，特定于体系结构的参考手册是一个很好的信息来源

第二阶段与硬件的相关度也很高

在第三阶段，即系统无关的初始化阶段，内核已经载入内存，而（在某些体系结构上）处理器已经由==启动模式==切换为==执行模式==，内核接下来将开始运行



## D.1 IA-32 系统上与体系结构相关的设置

在使用启动装载器（如LILO、GRUB等）将内核载入物理内存之后，将通过跳转语句，将控制流切换到内存中适当的位置，来调用arch/x86/boot/header.S中的汇编语言“函数”setup。
这是可能的，因为setup函数总是位于目标文件中的同一位置。

该代码执行下列任务，这需要许多汇编代码。
(1) 它检查内核是否加载到内存中正确的位置。为此，它使用一个4字节的特征标记，该标记在编译时集成到内核映像中，并且总是位于物理内存中一个不变的正确位置。
(2) 它确定系统内存的大小。
(3) 初始化显卡。
(4) 将内核映像移动到内存中的某个位置，使得在后续的解压缩期间，映像不会自阻其路。
(5) 将CPU切换到保护模式


在完成这些任务后，代码分支到startup_32函数（位于arch/x86/boot/compressed/head_32.S），该函数将执行下列任务。
(1) 创建一个临时内核栈。
(2) 用0字节填充未初始化的内核数据。相关的区域位于_edata和_end常数指定的位置之间。在内核链接时，会根据内核二进制文件，自动对这些常数设置正确的值。
(3) 调用arch/x86/boot/compressed/misc_32.c中的C语言例程decompress_kernel。该函数将解压缩内核，并将未压缩的机器码写入从0x100000开始的内存区，① 这刚好紧接着内存的第1个MiB。解压缩是内核执行的第一个操作，屏幕上可以看到的消息是Uncompressing Linux...和Ok, booting the kernel

现在，开始特定于处理器的初始化过程的最后一部分工作，首先要将控制流重定向到arch/x86/kernel/head_32.S中的startup_32。

这部分代码负责执行下列任务。
(1) 激活分页模式，设置一个最终的内核栈。
(2) 用0字节填充__bss_start和__bss_stop之间的.bss段。
(3) 初始化中断描述符表。但所有中断的处理程序都设置为ignore_int空例程，实际的处理程序在之后安装。
(4) 检测处理器类型。cpuid语句可用于识别比较新的处理器模型。它可以返回有关处理器类型及其能力的有关信息，但它并不区分80386和80486处理器。这两种处理器是通过各种汇编语言技巧来区分的，不过这些技巧既无趣也不重要。


D.2 高层初始化

start_kernel用作一个分配器函数，来执行各种平台无关/相关的任务，所有这些都是用C语言实现的

![8d6d4c1da18dfc5e1ee343a6f6e7568f.png](../_resources/8d6d4c1da18dfc5e1ee343a6f6e7568f.png)


。。跳


# 附录E ELF二进制格式

Executable and Linkable Format

ELF一个特别的优点在于，同一文件格式可以用于内核支持的几乎所有体系结构上

文件格式相同并不意味着不同系统上的程序之间存在二进制兼容性


E.1 布局和结构

![03a42fd9dda9066654f1028d090bd319.png](../_resources/03a42fd9dda9066654f1028d090bd319.png)

程序头表（program header table）向系统提供了可执行文件的数据在进程虚拟地址空间中组织方式的相关信息。它还表示了文件可能包含的段数目、段的位置和用途。

节头表（section header table）包含了与各段相关的附加信息。

==readelf==是一个有用的工具，用于分析ELF文件的结构

`gcc test.c -o test`  可执行文件
`gcc test.c -c -o test.o` 可重定位文件

`file test`
`file test.o`

elf头
`readelf test`

程序头表
`readelf -l test`

节
`readelf -S test.o`
- .interp，解释器的文件名
- .data，初始化过的数据
- .rodata，只读数据
- .init, .fini，进程初始化，结束 所用的代码
- .hash，在不对全表元素进行线性搜索的情况下，快速访问所有的符号表项。

节（对于可执行文件）的address字段保存了有效的值，因为相应的代码必须映射到虚拟地址空间中某些定义好的位置。在Linux下，对应用程序通常使用0x08000000以上的内存区。

符号表
nm工具可生成程序定义和使用的所有符号列表
`nm test.o`

字符串表
.strtab不是默认情况下ELF文件中唯一的字符串表。
.shstrtab用于存放文件中各个节的文本名称（例如，.text）。



E.2 内核中的ELF相关的数据结构
```C
// <elf.h> 
/* 32位ELF基础类型。 */ 
typedef __u32 Elf32_Addr; 
typedef __u16 Elf32_Half; 
typedef __u32 Elf32_Off; 
typedef __s32 Elf32_Sword; 
typedef __u32 Elf32_Word; 

/* 64位ELF基础类型。 */ 
typedef __u64 Elf64_Addr; 
typedef __u16 Elf64_Half; 
typedef __s16 Elf64_SHalf; 
typedef __u64 Elf64_Off; 
typedef __s32 Elf64_Sword; 
typedef __u32 Elf64_Word; 
typedef __u64 Elf64_Xword; 
typedef __s64 Elf64_Sxword; 
```

。。跳


# 附录F 内核开发过程

维护者在内核代码树顶层的MAINTAINERS文件中列出
www.kernel.org

www.lwn.net是内核开发过程方面的首要信息源









































