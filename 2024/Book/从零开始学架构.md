
从零开始学架构：照着做，你也能成为架构师


2024-06-06 20:03

[[toc]]

---
---


# ch01 架构基础

系统与子系统
模块与组件，逻辑角度拆分得到模块，物理角度拆分得到组件，模块为了职责分离，组件为了单元复用
框架

软件架构指软件系统的顶层结构
1. 系统由一群关联个体组成。个体可以是 子系统，模块，组件 等
2. 个体 需要根据某种规则运作，架构需要明确个体运作和协作的规则


架构设计的主要目的是==解决复杂度带来的问题==

## 1.3 复杂度来源

### 高性能

- 单机复杂度
  操作系统，进程，进程间通信(管道，消息队列，信号量，共享存储)，并行，并发。互斥锁。 SMP对称多处理器，NUMA非一致存储访问结构，MPP海量并行处理结构
- 集群复杂度
  任务分配(分配器，通信，分配算法) 存储，运算，缓存
  任务分解， 简单的系统更容易做到高性能。

---

### 高可用

高可用指"系统无中断地执行其功能"的能力。

高可用方案，本质上都是通过"冗余"来实现高可用

计算高可用
从单机变成多台机器，复杂度：
- 需要增加一个任务分配器，这个任务分配器也需要考虑 性能，成本，可维护性，可用性
- 任务分配器 和 业务服务器之间有 连接和交互，需要选择合适的连接方式。例如 连接建立，连接检测，连接中断后如何处理
- 任务分配需 需要 分配算法。例如 常见的双机算法有主备、主主。主备方案又可以细分为冷备、温备 热备。


存储高可用
==高可用的关键点和难点就是 存储高可用==。
存储和计算的 本质区别是，将数据从一台机器搬到另一台机器，需要经过线路进行传输。传输需要耗时数ms到数秒
传输需要时间，这就意味着，在某个时间点上，系统的数据 是不一致的。   
最经典的例子，存钱存到了北京的机房，查询时 使用了上海的机房，并且查询时 数据还没有从 北京 同步到上海。  
传输路线本身也存在可用性问题。如 光缆被挖断。

存储高可用的难点 不在于如何备份数据，而是在于 ==如何减少 或规避 数据不一致 对业务造成的影响==


高可用状态决策
无论 计算高可用 还是 存储高可用，其基础都是 "状态决策"，即 系统需要能判断 当前的状态是 正常还是 异常。

独裁式
存在一个独立的 决策主体，复杂收集信息然后进行决策。  
不会出现决策混乱的问题，因为只有一个决策者，但是 决策者会导致 单点问题

协商式
2个独立的个体通过交流信息，然后根据规则进行决策。  
最常用的协商式决策就是主备决策
- 2台服务器启动时都是备机
- 2台服务器建立连接
- 2台服务器交换信息
- 某台服务器做出决策，成为主机，另一台继续保持备机身份

难点在于 2者的信息交换出现问题(如 连接中断)，决策状态应该怎么做。

民主式
多个独立的个体通过投票的方式来进行状态决策。  
如zookeeper 集群的leader选举   

和协商式比较类似，都是 独立的个体之间的交换信息，每个个体做出自己的决策，然后多数获胜。   
算法很复杂。

还有一个固有的缺陷: 脑裂。  
当集群由于连接中断，分成了2个 独立的子集群， 各自进行选举，于是选出了2个主机。

为了解决脑裂，一般是 应用 投票节点数必须超过 系统总节点数的一半 的规则。   
但是 如果很多节点真的出了问题，可能导致 投票节点数过少，无法 选出主节点。系统等于是 宕机了。

---

### 可扩展性

为了应对将来需求变化而提供的一种扩展能力。
正确预测变化，完美封装变化

预测变化


应对变化
将变化封装在一个变化层，将不变的部分封装在一个独立的稳定层

另一种：提炼出一个 抽象层，一个实现层。 抽象层是稳定的


### 低成本

低成本 给架构带来的主要复杂度体现在： 往往只有 创新 才能达到低成本目标。这里的 创新 既 包括开创一个全新的技术领域(对于绝大多数公司要求太高了)， 也包括引入新技术。

新技术，如
- NoSQL (Memcache, Redis), 解决关系型数据库无法应对高并发访问
- 全文搜索 (Sphinx, Elasticsearch, Solr), 解决关系型数据库的like的低效
- Hadoop, 解决传统文件系统无法应对海量数据存储和计算

业界创新
- Facebook 为了解决PHP的低效，使用了 HipHop PHP， HHVM
- 新浪微博，将 Redis/MC + MySQL 扩展为 Redis/MC + SSD Cache + MySQL
- Linkedin 为了解决每个5千亿个事件，开发了 kafka
- 其他类似 将 Ruby on Rails 改为 Java， Lua + Redis 改为Go 的例子还有很多


### 安全

功能安全
。。就是针对漏洞攻击
常见的 XSS攻击，CSRF攻击，SQL注入，Windows漏洞，密码破解

架构安全
。。就是暴力攻击，DDoS
传统的架构安全 主要依靠 防火墙
互联网系统的架构安全没有太好的设计手段来实现，更多是一来 运行商或云服务商 强大的带宽 和 流量清洗的能力。


### 规模

很多企业级的系统，即没有高性能要求，也没有高可用要求，也不需要扩展性，但是 就是很复杂，为什么呢？  
因为 这样的系统 功能非常多，代码逻辑复杂。不断叠加功能。

量变引起质变

功能越来越多，导致系统复杂度指数级上升
牵一发而动全身

数据越来越多，系统复杂度发生质变
数据多了以后，传统的数据收集，加工，存储，分析 已经无法适应。  
大数据理论的基础是 Google的 3篇论文，Google File System 是大数据文件存储的技术理论，Google BigTable 是列式数据存储的技术理论，Google MapReduce 是大数据运算的技术理论

即使我们的数据没有达到 大数据规模，数据的增长也会给系统带来复杂性。  
当mysql中数据量过大时
- 添加索引会很慢，可能需要几小时，这段时间内，数据库是无法插入数据的
- 修改表结构 也很慢
- 索引的性能会下降
- 数据库备份 耗时很长

因此，mysql单表数据量太大时，必须考虑将单表拆分为 多表，这个拆分过程也会引入复杂性：
- 拆表的规则是什么
  - 以用户表为例，是按照用户id拆分表 还是按照注册时间拆分
- 拆完表后查询如何处理
  - 以用户表为例，假设按照用户id拆分表，当业务需要查询 学历本科及以上的用户时，需要很多表查询才可以得到结果，怎么保证性能



# ch02 架构设计原则

编程 不存在 不确定。 一段代码，相同输入，输出必然相同，除非结果本身就和概率相关

架构，本质上是不确定的。 同一个系统，不同公司做出的 架构可能有很大的差异。  
很多时候 面临多种可能，需要进行选择。 一旦涉及选择，就很容易让 架构师 陷入 两难的境地，如
- 选择 业界最先进 还是 团队最熟悉 的技术
- 是选择 Google 的 Angular 还是 Facebook的 React
- MySQL 还是 PostgreSQL


几个共性的原则： ==合适原则，简单原则，演化原则==

## 合适原则

原则宣言： 合适优于业界领先

优秀的技术人员都有很强的技术情节，当他们做方案或架构时，总想不断挑战自己，想要达到业界领先水平。  
梦想美好，现实残酷，大多数这样想 和做的架构，最终都以失败告终。

1. 将军难打无兵之仗
十几人的团队，想做 几十人，上百人 团队的事情，而且还要做的更好，这不太可能。
。。十几个本科  vs  十几个清北博士

2. 罗马不是一天建成的
业界领先的很多方案，并不是 灵机一动 就搞出来的。 而是进过几年的发展 才逐步完善的。  
在发展过程中遇到的 挑战，踩到的坑，对 架构设计 起了很大的促进。  
单纯的 头脑风暴，不可能和 真正遇到的 挑战和问题 同日而语

3. 冰山下面才是关键
很多人以为，业界领先的方案 都是 天才创造出来的，所以自己也要设计一个 业界领先的方案，来证明自己是天才。   
这样的天才确实有，但是，更多时候，业界领先的方案 是 逼出来的。简单来说，业务发展到一定阶段，量变导致质变，出现了新的问题，已有的方案不能应对这些问题，需要新的方案，通过创新和尝试，才有了业界领先的方案。



## 简单原则

原则宣言：简单由于复杂

在软件领域，复杂代表问题。

结构的复杂性
逻辑复杂性



## 演化原则

原则宣言：演化优于一步到位

对于建筑，永恒是主题；对于软件，变化是主题。

软件架构 要根据业务发展不断变化。 所以不可能一步到位 设计一个能应对所有业务的架构。

软件架构设计的过程
- 首先，设计出来的架构要满足 当前的业务需要
- 其次，架构要不断地在实际应用中迭代，保留优秀的涉及，修复有缺陷的设计，改正错误的设计，去掉无用的设计，使得架构逐渐完善
- 最后，当业务发生变化时，架构要 扩展，重构，甚至重写。 有价值的 经验，教训，逻辑，设计等 可以在 新架构中 延续

架构师在进行架构设计时，要牢记这个原则，时刻提醒自己 ==不要贪大求全，或盲目照搬大公司的做法， 而是应该 认真分析当前业务的特点，明确业务面临的主要问题，设计合理的架构，快速落地 以满足 业务需要，然后在 运行过程中不断完善架构，不断随着业务演化架构==



# ch03 架构设计流程

技术人员 会 编程语言，数据结构，算法，OS，软件工程等，但 缺少 体系化的 架构设计的 学习。

事实上架构设计没有什么神秘和神奇的地方，也不需要架构师具有艺术家的才华，只要掌握适当的方法，逐步完善架构，“菜鸟”也能够做架构设计。
简单来说，架构设计是有套路的，按照套路去做，即使没有丰富的架构设计经验，也能做出基本可行的架构。


## 3.1 有的放矢 -- 识别复杂度

设计架构时，首先要分析系统的复杂性。

只有正确分析出来 系统的复杂性，后续的架构设计方案 才不会偏离方向。
如果对 系统的复杂性 误判了，那么后续的架构设计 再完美 再先进，都是 南辕北辙。

例如，如果一个系统的 复杂度 是 业务逻辑太复杂，功能耦合严重， 架构师 设计了一个 TPS非常高的 高性能架构，即使这个 架构 的性能再优秀 也没有任何意义，因为 架构没有解决 复杂性问题。   
。。不过 性能永远都是不够的。

架构的复杂性 主要来自 高性能，高可用，可扩展， 但 架构师在 判断 复杂性的时候，不能生搬硬套，认为 任何时候 都从这3个角度 分析复杂度就可以了。   
大多数时候，复杂度只是其中的一个，少数情况下 才包含 2个， 如果出现了3个或以上的 复杂度，那么说明 这个系统 之前做的 实在太烂了， 要么架构师的 判断出现了 严重失误。

如果运气不好，接手了一个 每个复杂度都存在问题的 系统，那应该怎么办？   
答案是，==一个一个解决==，不要幻想一次架构 解决所有问题。

对于 按照复杂度优先级解决 这种方式 有一个 普遍的 担忧：解决低优先级的方案 会推翻 之前已解决的高优先级的方案。   
这 在现实中 是不可能出现的， 因为 软件系统的 可塑性和易变性。


## 按图索骥 -- 设计备选方案

确定系统面临的 主要复杂度问题后， 方案设计 就有了明确的目标，我们就可以开始 架构方案设计的了

成熟的架构师 对 已有的技术 非常熟悉，对已经验证的 架构模式 烂熟于心，然后根据 自己对业务的理解，选择合适的架构模式进行组合，再对组合后的 方案进行 修改和调整

软件技术 几十年的发展，有很多 成熟的技术， 如
- 高可用的 主备方案，集群方案
- 高性能的 负载均衡，多路复用
- 可扩展的 分层，插件化

绝大部分时候，我们有了明确的目标后，按图索骥就可以找到 可选的解决方案了。  
当这种方式 完全无法满足需求的时候，才会考虑进行 方案的创新，但是 绝大部分情况下，创新 也是基于现有的 成熟技术
- NoSQL，k-v的存储 和 数据库的 索引其实是类似的， Memcache只是把数据库的 索引 独立出来 做了一个缓存系统。
- Hadoop，基础是 集群方案 + 数据复制方案
- Docker，基础是 LXC (Linux Containers)
- LevelDB 的文件存储结构是 Skip List


《技术的本质》中：
新技术都是在 现有技术的基础上发展起来的，现有技术又来源于先前的技术。
将技术进行功能性分组 可以大大简化设计过程，这是技术“模块化”的首要原因。
技术的“组合”和“递归”特征，将彻底改变我们对技术本质的认识。


基于已有技术或架构模式 进行组合，调整， 大部分情况下 就能得到 我们需要的方案，但 并不意味着 架构设计 是一个 很简单的事情。  
因为可选模式有很多，组合的方案更多，如何设计 最终的方案，并不是一件容易的事情，这个阶段也是很多架构师 容易犯错的 地方。

第一种常见的错误： 设计最优秀的方案  
  要考虑 简单原则

第二种常见的错误： 只做一个方案
  要 3-5个 备选方案
  备选方案的 差异要明显
  备选方案的 技术不要只局限于 已经熟悉的技术

第三种常见的错误： 备选方案过于详细
  消耗大量时间和精力
  将注意力集中到 细节，而忽略了 整体的技术设计，导致 备选方案 数量不够 或 差异不大
  评审的时候，其他人会被 细节绕进去， 评审效果差。


## 3.3 深思熟虑 -- 评估和选择备选方案

完成备选方案的设计后，如何挑选出最终的方案也是一个很大的挑战：
- 每个方案都是可行的，
- 没有哪个方案是完美的
- 评价标准主观性比较强

实践中，有如下的指导思想
- 最简派
  挑选看起来最简单的
- 最牛派
  挑选 技术上看起来最牛的
- 最熟派
  挑选 自己最熟悉的发
- 领导派
  让领导定夺(背锅)


列出我们关注的 质量属性点，然后 从这些维护 评估每个方案，再综合挑选 合适的 最优方案  
常见的方案质量属性点有：性能，可用性，硬件成本，项目投入，复杂度，安全性，可扩展性 等。   
评估这些质量属性时，要遵守 合适原则 和 简单原则

最理想的是，能简单地扩容就能跟上业务的发展的 方案。


### 具体例子

3.3.1 业务背景

20人的 创业团队 做了一个 垂直电商的 网站，开发人员是 6人。
创业初期 为了快速上线，系统架构很简单，就是一个 简单的 web网站。
单服务器，单数据库

现在 web服务器出现了 性能瓶颈，用户访问慢。

#### 方案1： 横向扩展

比较简单，就是简单地增加web 服务器， 将单服务器 扩展为 集群，前面通过 Nginx进行负载均衡
单数据库

#### 方案2： 系统拆分

参考淘宝，将电商系统拆分为 商品子系统，订单子系统，用户管理子系统
每个子系统一个独立的数据库


#### 评审

![97612ac79bc0abb2c15aca31e2ebafee.png](../_resources/97612ac79bc0abb2c15aca31e2ebafee.png)

集群方案 2个占优， 拆分方案 3个占优， 面临了 选择上的困难， 有几种看似正确 但 实际 ==错误==的做法

1. 数量对比法，简单地看 哪个方案优点多 就选哪个。 
  主要问题是，把 所有的质量属性 的 重要性 等同，而没有考虑 质量属性的 优先级。 比如，对于大公司，成本基本不是问题。
2. 加权法， 每个属性赋予一个权重。然后 相加，哪个得分高 就选哪个。  
  主要问题是 无法客观地给出每个 质量属性的 权重得分。

正确的做法是 按优先级选择。 架构师按照 业务发展情况，团队人员规模 和 技能，业务发展预测 等因素，将 质量属性 按照 优先级排序，首先 挑选满足 第一优先级的，如果 有多个方案都满足 第一优先级，那么 就再看第二优先级。  
不同的备选方案的差异要明显，这样 优缺点就不会相同。


## 3.4 精雕细琢 -- 详细方案设计

将最终确定的备选方案进行细化， 使得 备选方案 变成一个 可以落地的 设计方案。

简单来说，详细方案设计就是将方案涉及的关键技术细节给确定下来

- 假如我们使用 Elasticsearch 来做全文搜索，那么就需要明确 Elasticsearch 的索引是按照 业务划分，还是 一个大的索引就可以了， 副本数量 是2个 还是3个或4个，集群节点的数量是3个还是6个。
- 假如我们使用 MySQL 分库分表，那么就需要明确哪些表 要分库分表，按照什么维度来分库分表， 分库分表后的联合查询该怎么处理
- 加入我们使用 Nginx 来进行负载均衡，那么 Nginx 的主备怎么做，Nignx的 负载均衡策略使用 哪个

细节也需要做很多选择，但是这些选择是 轻量级的，只需要简单根据 这些技术的 适用场景 选择就可以了。

例如Nginx的负载均衡策略，按如下规则进行选择
- 轮询(默认)， 如果后端服务器宕机，能自动剔除
- 加权轮询， 适用于 后端服务器 性能不均的情况，如 新老服务器混用
- ip_hash，解决 session问题，如 购物车 类的应用
- fair，可以最大化平衡 后端服务器的压力，适用于 后端服务器性能不平均的情况，可以防止后端某台服务器在性能不足的情况下 还继续接收 请求 从而导致 雪崩效应
- url_hash，适用于 后端服务器能够将 url的响应结果 缓存的情况。


极端情况，就是 细化阶段，发现 备选方案不可行。
此时，可以通过如下方式 能有效地避免
- 架构师不但要进行备选方案的设计和选项，还需要对备选方案的细节有较深入的理解。
- 通过分步骤，分阶段，分系统 等方式，尽量降低方案复杂度，方案本身的复杂度越 高 ，某个细节推翻 整个方案的 可能性越高。 降低复杂度，可以减少这种风险
- 如果 方案本身就很复杂，那么就使用 设计团队的方式 来进行设计， 博采众长。



# 第二部分 高性能架构模式

# ch04 存储高性能

很多场景下，高性能设计的 最核心部分 就是 关系数据库的 设计。

互联网业务兴起之后，单个服务器 很难满足 业务需要，必须考虑 数据库 集群的方式 来提升性能

高性能数据库集群 
- 第一种方式 是 读写分离，本质是 将 访问压力 分散到 集群中的 多个节点， 但是没有分散 存储压力
- 第二种是 分库分表，既可以分散访问压力，也可以分散存储压力。


## 读写分离

基本原理是 将数据库 读写操作 分散到 不同的节点上。

基本实现
- 数据库服务器 搭建主从集群，一主一从，一主多从 都可以
- 主机负责读写操作，从机 只 负责读操作
- 主机 通过 复制 将数据同步到 从机，每台数据库 都存储了 所有的业务数据
- web服务器将 写操作 发给 数据库主机，将 读操作发个 从机

主从 的从机可以处理 读请求
主备 的备机 只是一个备份，不处理请求

读写分离的实现逻辑并不复杂，但是 在实际应用中需要 应对 复制延迟 带来的复杂性。

解决主从复制延迟 有几种方法
- 写操作后的 读操作 被发往 数据库主服务器。
  这种方式 和 业务强绑定，对业务的侵入 和影响较大。
- 读从机失败后 再读一次 主机
  二次读取，和业务无关，只需要 底层访问数据库的API 进行封装，实现代价小。但是如果有很多 二次读取，将增加 主机的读操作。
- 关键业务读写操作 全部使用主机， 非关键业务采用 读写分离



## 分库分表

当数据库达到千万 甚至上亿时，单台数据库服务器的存储能力 会成为系统的瓶颈，主要体现在以下的方面
- 数据量太大，读写性能都会下降，即使有索引，索引也会变得很大，性能同样会下降
- 数据文件会变得很大，数据库备份和恢复需要很长时间
- 数据文件越大， 极端情况下，丢失数据的风险越高。

要将 存储 分散到 多台数据库服务器上

常见的 分散的方法有 分库 和 分表 两大类


### 业务分库

按业务模块 将数据 分散到 不同的 数据库服务器。  
例如，一个简单的电商网站，包括 用户，商品，订单 3个业务模块，我们可以将用户数据， 商品数据，订单数据 分开 放到3台不同的服务器上。

业务分库 可以分散 存储和访问压力，但是 也带来了新的问题

- join操作问题
业务分库后，表 分散到不同数据库中，无法使用 SQL 的join查询。 只能通过代码来 手动模仿join

- 事务问题
表分散到 不同数据库后，无法通过 事务进行修改。  
虽然数据库厂商提供了一些 分布式事务的 解决方案 (如 MySQL 的 XA) ，但是性能太低。
只能通过代码来 模拟事务，失败的话 进行补偿

- 成本问题


基于上述原因，对于初创业务，不建议 一开始 就 这样拆分，因为
- 初创业务有很大的 不确定性，业务不一定能发展起来， 业务开始的时候 并没有 存储和访问压力， 业务分库 不能为 业务 带来 价值
- 业务分库后， 表之间的 join查询，事务 就无法简单实现了
- 业务分库，不同的数据操作 读写不同的数据库， 增加了工作量

10个业务有一个 业务能活下去就很不错了。   
如果业务发展的很快，那么后期 进行 业务分库 也不迟。
单台数据库 性能 没有想象的那么弱， 单台 可以支持 10万用户量 量级的业务。



### 分表

将不同业务数据 分布存储到 不同的数据库服务器，能够支持 百万，甚至千万 用户规模的业务。  
业务继续发展，同一业务的 单表数据 也会达到单台数据库服务器的 处理瓶颈。

单表数据拆分 有2种方式， 垂直分表，水平分表

垂直分表是 拆分列，不同的列 归属到不同的表中
水平分表 是拆分行， 主键1-10000的数据 到一个表， 10001-20000 的到另一个表。

单表进行切分后， ==是否要将 切分后的 多个表 分散到 不同的 数据库服务器==中，可以根据实际切分效果来确定。  
切分后的表 即使在 同一个数据库服务器中， 也可以 带来 可观的性能提升。


分表 可以分散 存储压力， 带来性能提升， 但是也会带来 复杂性


垂直分表
适合将 表中 某些 不常用的 且 占据大量空间的 列 拆分出去。  
复杂性主要体现在 表操作的数量要增加。 原来一次性就可以获得 某条数据的所有信息，现在要 2次查询。
但是 比 水平分表 的复杂性 小很多。

水平分表
特别适合于 表行数特别大的表， 如果单表行数超过5000w，就必须分表。5000w 是一个参考，关键是看 表的 访问性能，
复杂的表可能1000w 就出现访问性能下降，需要分表， 简单的表 可能1亿才出现 访问性能下降
水平分表 比 垂直分表 更复杂，体现在
- 路由
水平分表后，某条数据 具体属于哪个切分后的子表，需要增加 路由算法进行计算。
常用的路由算法
  - 范围路由，使用有序的数据列 作为路由的条件。 
    主要的复杂点在于 分段大小的选取上，一般建议 100万 - 2000万之间
    优点是，随着数据的增加 可以平滑地扩展新表
    比较隐含的缺点是，分布不均匀。
  - hash路由
    选取某个列(或某几个列) 的值 进行hash 运算，然后根据hash 的结果 分散到 不同的数据库表中。 比如 id % 10
    主要复杂点是 初始表数量的选取上，数量多，维护麻烦； 数量少，可能导致
  - 配置路由
    使用独立的表 来记录 路由信息
    设计简单，使用灵活，尤其在扩充表的时候，只需迁移指定的数据，然后修改路由表。
    缺点，查询次数+1，影响性能， 如果路由表本身就很大，那么它的性能会成为瓶颈。
- join查询
  需要在业务代码 或 数据库中间件中进行多次 join，然后 将结果合并
- count()操作
  - count()相加，业务代码或数据库中间件中 对每个表进行 count()，然后相加
  - 记录数表，新建一张表，包含 表名，行数 2个字段，每次插入或删除后 更新这个表。
- order by 操作
  需要 业务代码，中间件 分别查询每个表中的数据，然后 汇总 排序


## 实现方法

读写分离 需要 区分 读，写操作，然后访问不同的数据库服务器
分库分表 需要根据不同的数据 访问不同的数据库服务器

两者本质上都是一种 分配机制，即将不同的sql 发送到 不同的 数据库服务器

常见的分配实现方式有2种， 程序代码封装 和 中间件封装

- 程序代码封装  
代码中抽象一个数据库访问层 来实现 读写分离，分库分表。  
例如，基于hibernate 进行简单封装，就可以实现 读写分离。

特点
- 实现简单，可以根据业务做较多定制化的功能
- 每个编程语言都要自己实现一遍
- 故障情况下，如果主从发生切换，则可能需要所有系统都修改配置 并重启

目前开源的方案中 淘宝的 TDDL 比较有名。

- 中间件封装  
独立一套系统出来，实现 读写分离，分库分表。
中间件对业务服务器提供SQL兼容的协议。
对于业务服务器来说，访问中间件 和 访问数据库没有区别。

特点
- 支持多种编程语言，因为 数据库中间件 对业务服务器提供的是 标准 SQL 接口
- 数据库中间件要支持 完整的 SQL 语法 和数据库服务器的协议。很复杂，细节很多，容易出bug
- 中间件不执行 读写操作，但 所有数据库操作 都需要经过中间件，性能要求很高
- 数据库主从切换 对于 业务服务器 无感知， 中间件可以 探测到 数据库服务器的 主从状态


中间件的 复杂度比 代码封装 高， 所以 一般情况下，使用 程序语言封装， 或使用 成熟的开源数据库中间件。

目前开源的 数据库 中间件方案中，有 MySQL的 mysql-proxy，MySQL Router。  
360的 Atlas， 基于 mysql proxy 实现



## NoSQL

关系型数据库的缺点
- 存储的是 行记录，无法存储数据结构
- 关系数据库的 schema 扩展很不方便
- 在大数据场景下 IO 较高
- 全文搜索能力 较弱


常见的 NoSQL 有4类
- K-V 存储， 解决关系数据库 无法存储数据结构的问题，以 Redis 为代表
- 文档数据库， 解决关系数据库 强schema 的问题，以 MongoDB 为代表
- 列式数据库， 解决关系数据库 大数据场景下的 IO 问题，以 HBase 为代表
- 全文搜索引擎， 解决 全文搜索性能的问题，Elasticsearch



### K-V 存储

redis 是代表。
redis的 value 是 具体的数据结构，包括 string，hash, list, set, sorted set, bitmap, hyperloglog。

以 list 为例，redis 提供了 如下的典型操作
- lpop， 从list 左边出队一个元素
- lindex， 获取一个元素
- llen， 获得list 长度
- rpop， 从右边出队一个元素

如果使用关系型数据库来 实现上面的功能，会很复杂。

redis 的主要缺点 是 不支持完整的 ACID 事务。
redis 的事务只能保证 隔离性 和一致性 (I 和 C) ，无法保证 A 和D

实际上 大部分业务 也不需要 严格遵守 ACID， 需要根据业务特性 和要求 来确定是否可以使用 redis


### 文档数据库

最大的特点就是 no-schema， 可以存储 和读取 任意的数据，目前 绝大多数 文档数据库存储的 数据格式 是 json 或 bson

因为json 是自描述的，不需要在使用前 定义字段。  
读取json 中不存在的 字段也不会导致 sql 那样的 语法错误

文档数据库的 no-schema 特性，给业务开发带来 几个明显优势
- 新增字段简单
  不需要 像关系数据库那样 要先执行DDL 语句 修改表结构，程序代码直接读写即可
- 历史数据不会出错
  对于历史数据，即使没有新增的字段，也不会导致错误，只会返回空值，此时代码进行兼容处理即可。
- 可以很容易存储复杂数据
  json 是一种强大的 描述语言，能够描述复杂的 数据结构。

使用json 来描述数据，比使用关系型数据库表 来描述 数据 方便 和容易， 也更容易理解。
文档数据库的这个特点，特别适合于 电商 和 游戏 这类的 业务场景。

以电商为例，不同商品的属性差异很大。 即使同类商品 也有不同的属性。

文档数据库的 no-schema 的特性 带来的 这些优势 也是有代价的，最主要的代价就是 ==不支持 事务==。

另一个缺点是 无法实现 join操作。

文档数据库 不能 完全取代 关系数据库，更多时候 是作为 关系数据库的一种补充。  常见的电商网站设计中， 可以使用 关系数据库 存储 商品库存信息，  使用文档数据库 来存储 商品详细信息


### 列式数据库

按照 列 来存储数据的 数据库

按行来存储 的优势
- 业务同时读取多个列时 效率高， 因为这些 列 都是存储在一起的，一次磁盘操作 就可以把 一行数据中的 各个列 都读取到内存中
- 能一次性完成 对一行中 多个列的 写操作，保证了 原子性 和 一致性

如果场景变换，那么 优势会变成 劣势
如 海量数据进行统计时， 行 存储 无法发挥优势。
例如，计算某个城市 体重超重的人员数据时， 实际上 只需要 读取每个人的 体重 这一列 就可以了。  行存储 即使只用到一列，也需要 读取整行，IO增加很多。

列存储 还具有 更高的 存储压缩比，能节省 更多的存储空间。 行存储的压缩率一般是 3:1 - 5:1 , 列存储的压缩率是 8:1 - 30:1 , 因为单个列的 数据 相似度 相比行来说 更高， 能达到更高的压缩率

同样，如果场景变化，列存储的优势 会变成 列式。
如 需要频繁更新 多个列时，因为 列存储 将不同列 存储在 磁盘的 不同空间中，导致 更新多个列时 需要多次IO。

一般将 列存储 应用在 离线的 大数据分析和统计中。 因为这种场景 主要是 针对 部分列进行读写，且数据写入后 就无须 再更新，删除、


### 全文搜索引擎

数据库的缺陷
传统的关系型数据库通过 索引来达到 快速查询的目的，但是在 全文搜索的业务场景下，索引也无能为力，主要体现在：
- 全文搜索的条件可以随机排列组合，如果通过索引来满足，则 索引的数量会非常多。
- 全文搜索的模糊匹配方式， 索引无法做到，只能使用 like查询， 效率低


基本原理
倒排索引(inverted index)
基本原理是 建立单词 到文档的 索引。

正排索引是 建立 文档到单词的 索引

。就是通过 分词工具， 建立 单词 到 文档 的关联关系
。。就是 给定一个 单词，可以知道 这个单词 出现在 哪些 文档中。


## 缓存

某些复杂的 业务场景下，单纯依靠存储系统的 性能提升是不够的，典型场景如下
1. 需要经过 复杂运算 后得出的数据，存储系统无能为力
  count(*)
2. 读多写少的数据
  微博，一次insert，几十万次 select， select 的压力太大了。


缓存的基本原理是 将可能重复使用的数据放到内存中，一次生成，多次使用。

缓存能大大减轻存储系统的压力，但同时也给 架构引入了 更多的复杂性。


### 缓存穿透

缓存没有生效。 查询还是作用到 存储系统中。

1. 存储数据不存在

2. 缓存数据生成 消耗大量时间或资源
  数据存在，但是 生成缓存需要 较长时间或资源。如果 业务访问时 缓存失效，那么 缓存不会发挥作用，访问压力全部集中到 存储系统上。
。。就是 缓存击穿 ， 但是后续的例子 并不是 击穿。  缓存击穿 在 缓存雪崩中也有一些描述

典型的就是 电商的 商品分页， 我们在 电商平台上选择 手机 这个类别进行查看，由于 数据巨大，不可能把 所有数据都缓存起来， 只能按照 分页进行 缓存。  
一般情况下，用户最多看 前10页的数据。
竞争对手 通过爬虫获取数据 时，它会 爬完所有数据，但是 这些数据 基本只使用一次，每次都要访问数据库，拖累数据库。


### 缓存雪崩

缓存失效 后引起系统性能急剧下降。

- 更新锁
对缓存更新操作 进行 加锁保护，保证只有一个线程 能进行 缓存更新。
。。缓存击穿的 解决方案

- 后台更新
由后台线程来更新缓存，而不是由 业务线程来更新， 缓存本身的 有效期 设置为 永久， 后台线程 定时更新缓存。
需要考虑 缓存系统内存不够时，会 evict 一些缓存数据。 有2种方案
  - 定时读取
    后台线程要 频繁地读取缓存(1s 或 100ms 读取一次)， 如果发现 数据没有缓存，那么就立刻更新缓存。
  - 消息队列通知
    业务线程发现 缓存失效后，通过消息队列 发送消息 通知 后台线程 更新缓存。

后台更新还适合 刚上线时， 缓存预测

。。没有提到 过期时间 随机化


### 缓存热点

部分数据是热点， 大部分请求 都命中它， 导致 它所在的 缓存服务器的 压力很大。
解决方案是 复制多份缓存， 将请求 分散到 多个 缓存服务器上。
。。不过只适合于读， 不太适合 写，因为 写 会导致 缓存失效，需要 把 所有 缓存服务器上的 这份缓存 清除 或 更新。





# ch05 计算高性能

高性能是每个程序员的追求

架构师的角度，高性能架构设计 主要集中在2方面
- 尽量提升单服务的性能，将单服务器的性能发挥到极致
- 如果单服务器无法支撑性能，就设计 服务器集群方案


## 单服务器高性能

关键之一 就是 服务器采用的 网络编程模型， 有下面2个 关键设计点
- 服务器如何管理连接
- 服务器如何处理请求

以上2个设计点 最终都 和 OS的 IO模型 和 进程模型 有关
- IO模型： 阻塞，非阻塞，同步，异步
- 进程模型： 单进程，多进程，多线程


### PPC process per connection

- 父进程 accept连接
- 父进程 fork 子进程
- 子进程 处理 连接的 读写请求
- 子进程关闭连接

缺点
- fork 代价高
- 父子进程通信复杂
- 进程数量多了以后 对服务器压力较大


### prefork

PPC中，连接 accept后，才 fork。

prefork 就是 提前fork。 系统启动时就创建好 进程。

prefork 的实现关键就是 多个子进程 都 accept 同一个 socket， 有新连接进入是， OS保证 只有一个进程 accept 成功。
存在一个问题： 惊群 现象， 即 只有一个子进程能成功，但是 所有 阻塞在 accept 上的 子进程都会被唤醒， 导致 不必要的 进程调度， 上下文切换

和PPC 一样，存在 父子进程 通信复杂。

### TPC

thread per connection

和进程相比， thread更轻量，线程间通信更简单。 正好 针对了 PPC的 fork代价高， 父子进程通信复杂。

TPC流程
- 父进程accept 连接
- 父进程 创建 子线程
- 子线程处理 连接的 读写请求
- 子线程 关闭连接

问题
- 虽然创建的 代价地，但是 不是没有，高并发(上万)时，依然会有 性能问题
- 不需要 进程间通信，但是 线程间的 互斥，共享 又引入了 复杂度，可能导致死锁问题
- 多线程会相互影响， 某个线程出现异常时，可能导致 整个进程 退出

TPC 还存在 CPU线程调度 和 切换代价的问题。

在 并发几百个连接 的情况下，更多采用 PPC，因为PPC 不会死锁，不会互相影响，稳定性更高。



### prethread

和 prefork 类似， prethread 会预先创建线程。

由于线程间 数据共享 和通信比较方便，所以 prethread 有2种方式

1. 主进程 accept，然后将 连接 交给某个线程处理
2. 子线程都尝试 accept，最终只有一个 子线程 accept成功


### ==Reactor==

PPC，TPC 的主要问题就是 每个connection 都需要 创建 进程/线程， connection结束后， 进程/线程 就被销毁了。

池化技术

池化有个问题：进程如何才能 高效地 处理多个连接的业务   
一个连接一个进程时，当连接没有数据可读时，会阻塞在read上；   
如果一个进程处理多个连接，进程阻塞在 某个连接的read上，会导致 其他连接的数据无法被处理。

最简单的方法，就是 将 read 改成 非阻塞，进程不断轮询 所有连接。   
但 轮询需要消耗CPU，而且 连接数多了以后，效率很低。

一种自然而然的想法就是 只有当连接上有数据时，进程才去处理，这就是 IO 多路复用技术的 来源

==“多路”，就是指多条连接，“复用”指的是多条连接复用同一个阻塞对象==  
这个阻塞对象和具体的实现有关，以 linux为例，如果使用 select，那么 这个公共的阻塞对象就是 select用到的 fd_set； 如果使用epoll，就是 epoll_create 创建的 文件描述符。

IO多路复用技术归纳起来有如下2个关键实现点
- 多个连接 共用一个阻塞对象后，进程只需要在 这个阻塞对象上等待，无须再轮询所有连接
- 当某条连接 有数据可以处理时，OS 会通知进程，进程 从阻塞状态返回，开始进行业务处理

I/O多路复用 结合 线程池，完美地解决了 PPC，TPC 模型的问题，而且“大神们”给它取了一个很牛的名字： Reactor。  
实际上这里的“反应”是“事件反应” 意思，可以通俗地理解为“来一个事件我就有相应的反应”。  
==Reactor 模式也叫 Dispatcher 模式==（很多开源的系统里面会看到这个名称的类，其实就是实现 Reactor 模式的），更加贴近模式本身的含义 ，即 IO多路复用统一监听事件，收到事件后分配（Dispatch）给某个进程。

Reactor 模式的 核心组成部分 包括 Reactor 和处理资源池 （进程池或线程池），其中 Reactor 负责监听和分配事件，处理资源池负责处理事件

Reactor 模式的具体实现方案 灵活多变，主要体现在
- Reactor的数量可以变化，可以一个，也可以多个
- 资源池的数量可以变化， 也可以单个进程，也可以多个进程

理论上有4种选择，但是 多Reactor 单进程， 毫无意义。

最终Reactor有3种 典型的实现方案
- 单Reactor 单进程/线程
- 单Reactor 多线程
- 多Reactor 多进程/线程

。。确实没有说  单Reactor 多进程 ， 这个应该也可以啊。。 下面 具体介绍时 有说明


#### 单Reactor 单进程/线程

![3d2949fb37ebb654e458f9fa586866b6.png](../_resources/3d2949fb37ebb654e458f9fa586866b6.png)


1. Reactor对象 通过 select 监听连接事件，收到事件后通过dispatch进行分发
2. 连接建立 事件 由 Acceptor 处理，Acceptor 调用 accept方法 接受连接，并创建一个 handler 来处理后续的各种事件
3. 非 连接建立事件， 由 Reactor 调用 ==连接对应的 Handler== 来进行响应。
4. handler会完成 read - 处理 - send 的 完整业务流程。

。。第二步 应该 不会 创建 handler 吧。  应该是 app启动的时候 就 新建了 handler， 并绑定到了 。。。。 不，确实需要 新建一个东西， 不然 无法保存状态。


优点是 简单，没有进程间通信，没有进程竞争。

缺点
- 只有一个进程，无法发挥多核CPU的性能
- handler在处理某个连接的业务时，无法处理其他连接事件

所以 实际 应用场景不多，只 适用于 业务处理 非常快读的场景，   
比较著名的 是 Redis


#### 单Reactor 多线程

![f10ed79a14764b1eaf121d00fe6749e2.png](../_resources/f10ed79a14764b1eaf121d00fe6749e2.png)


1. 主线程中，Reactor对象 通过 select 监控连接事件，收到事件后 通过 dispatch 分发
2. 如果是 连接建立 事件，由 Acceptor 处理，Acceptor 通过 accept接受连接，并创建一个 handler 来处理后续的各种事件。
3. 非 连接建立 事件， Reactor 调用 ==连接对应的 handler== 来进行响应。
4. handler只负责响应事件，不进行业务处理， handler 通过 read 读取数据后，会发给 Processor 进行业务处理。
5. Processor 会在独立的 子线程中 完成 真正的业务处理，然后 将响应 发给 handler处理。 handler收到响应后，通过 send 将响应发给client


能充分利用 多核CPU 的处理能力， 但也存在下列的问题

1. 多线程 数据共享，访问 比较复杂
2. Reactor 承担 所有事件的 监听和响应，只在主线程中运行， 瞬时高并发 可能会成为 性能瓶颈


没有单Reactor 多进程，是因为： 如果使用多进程，子进程完成 业务处理后，将结果返回给 父进程，并通知父进程发给哪个client，是很麻烦的事情。因为 父进程只是通过 Reactor 监听 各个连接上的事件 然后进行分配，子进程 和 父进程 通信时 并不是一个连接。



#### 多Reactor 多进程/线程

![2e1cc38ddee31a9d78637fc624586fad.png](../_resources/2e1cc38ddee31a9d78637fc624586fad.png)


1. 父进程中 mainReactor 通过select 监听 连接==建立==事件，收到事件后 通过 Acceptor 接收，将 新的连接 分配给 某个子进程
2. 子进程的 subReactor 将 mainReactor 分配的 连接 加入 连接队列 进行监听，并创建一个 handler 来处理 连接的 各种事件。
3. 当有 新的事件发生时， subReactor 会调用 连接对应的 Handler 来进行响应。
4. handler 完成 read - 业务处理 - write 

多Reactor 看起来 比 单Reactor 复杂， 但是 ==实际实现 更简单==，因为
- 父进程，子进程的 职责明确
- 父子进程 交互很简单，父进程 只需要把 新连接 传给 子进程， 子进程无须返回任何数据
- 子进程之间是 相互独立的，无须同步共享之类的处理。


目前，多Reactor 多进程 的 有 Nginx， 多Reactor 多线程的有 Memcache， Netty

Nginx 采用的 多Reactor 多进程 和上面描述的 有一些差异：
主进程中仅仅 创建了 监听端口， 并没有创建 mainReactor 来 accept 连接。 而是由 子进程的 Reactor 来 accept连接， 通过 锁控制 一次只有一个 子进程 进行 accept。 子进程 accept 后 就放到 自己的 Reactor 中进行处理， 不会分配给 其他子进程



### ==Proactor==

Reactor是 非阻塞 同步网络模型，因为 read write 由 用户进程 同步操作。

Reactor 可以理解为“来了事件我通知你你来处理”，

Proactor 可以理解为“来了事件我来处理，==处理完了我通知你==”。
这里的“我”就是操作系统内核，“ 事件”就是有新连接、有数据可读，有数据可写 这些IO事件。

![f6de958200e567e92f155273a2e801df.png](../_resources/f6de958200e567e92f155273a2e801df.png)


1. proactor initiator 负责创建 proactor 和 handler，并将 proactor 和 handler 都通过 asynchronous operation processor ==注册到 内核==
2. async op processor 负责处理 注册请求，并完成 IO 请求
3. async op processor 完成 io 操作后 通知 Proactor
4. proactor 根据不同的 事件类型 回调 不同的 handler 进行业务处理
5. handler 完成业务处理， handler 也可以注册 新的 handler
到 内核。

理论上 Proactor 比 Reactor 效率要高一点，异步IO 可以充分利用 DMA 特性， 让 IO 操作 和 计算重叠。  
但实现 真正的 异步IO， OS 需要做大量的工作。  
目前 windows 下通过 IOCP 实现 真正的异步。   
linux 下 AIO 并不完善，所以 linux 下 基本以 Reactor 为主。   
boost asio 在 window 下使用 IOCP， 在 linux 下 使用 Reactor 模拟 出 异步模型

。。不过现在有 io_uring 了。 但是不知道怎么用。。



## 集群高性能

单服务器总有性能天花板。

高性能集群的本质很简单，通过增加更多的服务器来提升系统整体的计算能力

计算本身存在一个特点：同样的输入数据和逻辑，无论在哪台服务器上执行，都应该得到相同的输出。

因此高性能集群设计的 ==复杂度==主要体现在 ==任务分配==这部分，需要设计合理的任务分配策略，将计算任务分配到多台服务器上执行

高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法。
对于任务分配器，现在更流行的通用叫法是“负载均衡器”。


### 负载均衡分类

3种： DNS负载均衡， 硬件负载均衡， 软件负载均衡

#### DNS负载均衡

最简单，最常见。
一般用来实现 地理 级别的均衡。 如 北方的用户访问 北京的机房， 南方的用户访问 深圳的机房。  

DNS负载均衡的 本质是 ==DNS 解析同一个 域名 可以返回不同的IP==

优点
- 简单，成本低
  负载均衡的 工作交给 DNS服务器处理，无须自己开发或维护 lb
- 就近访问，提升访问速度

缺点
- 更新不及时
  DNS缓存的时间较长，修改DNS配置后，由于缓存的原因，还是有很多用户会继续访问 修改前的IP， 访问会失败，影响用户正常使用
- 扩展性差
  DNS负载均衡的 控制权 在域名商那里，无法根据 业务特点 做更多的 定制化功能
- 分配策略较简单
  DNS 负载均衡 支持的算法少，不能区分 服务器的差异， 无法感知后端服务器的状态

针对DNS负载均衡的一些缺点， 一些公司实现了 HTTP-DNS ，即 使用 http 协议实现 一个私有的 DNS 系统。 这样的方案 和 通用DNS 的 优缺点正好相反。


#### 硬件负载均衡

通过单独的硬件设备来实现负载均衡功能，

目前业界典型的硬件负载均衡设备有两款 F5 和 A10。
这类设备性能强劲，功能强大，但价格都不便宜， 一般只有“土豪” 公司才会考虑使用此类设备

一般公司 不会考虑， 一来 负担不起， 二来 业务量没有那么大

优点
- 功能强大
  全面支持 各层级的 lb， 支持全面的 lb算法，支持全局 lb
- 性能强大
  软件lb 一般 10万并发， 硬件lb 一般 100万并发
- 稳定性高
  商用硬件lb，经过 严格的 测试， 大规模使用， 稳定性很好
- 支持安全防护
  除了 lb， 还具备 防火墙，放DDoS攻击 等 安全功能


缺点
- 价格高
- 扩展能力差
  硬件设备，可以根据业务进行配置，但无法扩展和定制



#### 软件lb

Nginx ， LVS

nginx 是 7层负载均衡
LVS 是 linux 内核的 4层负载均衡

4层，7层 区别在于 协议和 灵活性。
nginx支持 http，email协议
LVS 和协议无关，几乎所有应用都可以做，例如， 聊天，数据库等。


软件，硬件 lb 的最大区别就是 性能
linux服务器上 nginx 大概是 5万/s,  
LVS shi  10万级别， 据说可以 80万/s  

F5是百万，200万 到 800万/s  都有。


优点
- 简单，  部署，维护都比较简单
- 便宜，  只需要买一个 linux服务器就可以
- 灵活，  4层，7层 lb 可以根据 业务选择； 也可以根据业务 进行扩展，比如 可以通过 nginx 插件来实现 定制化功能。

缺点
- 性能一般，  nginx大约支持 5万并发
- 功能没有 硬件lb 那么强大
- 一般不具备 防火墙 和 防DDoS



### 负载均衡框架

组合的基本原则是
- DNS lb 用于实现 地理级别的 lb
- 硬件lb 用于实现 集群级别的 lb
- 软件lb 用于实现 机器级别的 lb


![e8a9b1328620b95d0bc0fe4cb2117ee4.png](../_resources/e8a9b1328620b95d0bc0fe4cb2117ee4.png)


只有大型业务场景才会这样做。  
一般情况下，nginx 足够了。


### lb 算法

lb算法可以分为
- 任务平分类
  lb 将收到的 任务 (加权)平均分配给各个服务器
- 负载均衡类
  lb 根据服务器 负载(CPU，连接数，IO使用率，网卡吞吐量等) 进行分配
- 性能最优类
  根据 服务器响应时间 来分配
- hash类
  根据 任务中的 某些关键信息 进行hash，相同hash值 分配到同一台服务器。 常见的有  源地址hash， 目标地址hash， session id hash， 用户id hash


#### 轮询

lb 不关注 服务器状态， 但是 lb 能感知到 服务器 是否宕机。 如果 无法连通，那么要从 可分配服务器列表中 移除。

#### 加权训轮

权重一般是 根据硬件配置 进行 静态配置的


#### 负载最低优先

LVS 这种 4层网络lb， 可以以 连接数 来判断 服务器的状态

nginx 7层网络lb， 可以以 http 连接数 来判断 服务器状态

自己开发，可以根据业务特点来选择 指标， 如果是 CPU密集型，就可以 以 CPU负载 来衡量 系统压力。 如果是IO密集型，就可以以 IO负载 来衡量


负载最低优先 的算法 解决了 轮询算法中 无法感知 服务器状态的问题， 但是 复杂度增加了很多， 例如
- 最少连接数优先的 算法 要求 lb 统计 每个服务器 当前的 连接数。 应用场景 仅限于 lb 接收的 ==任何请求 都会转发给== 服务器； 如果 lb 和服务器之间 通过 ==连接池方式==， 就不适用 这种算法。
- CPU负载最低，要求 lb 以某种方式 收集每个服务器的 CPU 负载。 而且要 确定是 以 1分钟的负载为 标准，还是 15分钟的 负载为标准。


轮询可能只需要 5行代码，  负载最低 可能需要 1000行代码， 甚至 lb 和 服务器 都需要开发代码。  
==负载最低算法 如果没有 好好设计 或 场景不合适， 它 可能成为 性能的瓶颈。==



#### 性能最优类

负载最低优先 是 站在服务器的角度 来分配的。

性能最优 是站在 客户端的角度， 优先将 任务分配给 处理速度最快的服务器， 来达到 最快响应客户端的 目的。

和负载最低优先 类似， 性能最优优先 也是 感知服务器的状态， 只是通过 响应时间 这个外部标准 来衡量 服务器状态而已。  
因此 性能最优优先 和 负载最低优先 类似， 复杂度都很高，主要体现在
- lb 需要 收集 和分析 每个服务器 每个任务的 响应事件，在大量任务处理的 场景下，这种 收集 和统计 本身 也会消耗 较多的 性能
- 为了减少这种统计上的消耗，可以采取采样的方式来统计，即不统计所有任务的响应时间，而是抽样统计部分任务的响应时间来估算整体任务的响应时间 。采样统计虽然能够减少性能消耗，但使得复杂度 一步上升 因为 定合适的采样率
- 无论全部统计，还是采样统计，都需要选择合适的周期： 10 秒内性能最优，还是1分钟 内性能最优，还 5分钟内 性能最优



#### hash类

- 源地址hash
将来源于同一个源IP的 任务分配给 同一个服务器，适合于 存在 事务，会话的 业务。

- id hash
将某个ID标识的 业务 分配到 同一个服务器中处理， 这里的 id 一般是 临时数据的id (如 session id)




# 第三部分 高可用架构模式



# ch06 CAP

Robert Greiner 的文章作为参考基础。 他有2篇文章。 是对 CAP的理解，第一篇是过时的
。。下面复制的都是 第二篇的，  第一篇的 我没有复制。

## CAP 理论

```text
in a distributed system (a collection of interconnected nodes that share data.), you can only have two out of the following three guarantees across a write/read pair： Consistency, Availability, and Partition Tolerance - one of them must be sacrificed
```

简单翻译为：在一个分布式系统(指互相连接并共享数据的节点的集合)中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）3者中的两个，另外一个必须被牺牲

强调了 interconnected， share data。  
因为分布式系统 并不一定会 互联 和 共享数据。  
比如 memcached 的集群， 相互之间就没有 连接和 共享数据。

强调了 write/read pair， 即 CAP 关注的是 对数据的 读写。 zookeeper 的选举机制 就不是 cap 关注的对象


### 一致性 consistency

```text
A read is guaranteed to return the most recent write for a given client
```

简单翻译为：对某个指定的客户端来说，读操作保证能够返回最新的写操作结果

在事务执行过程中，系统其实处于一个不一致的状态，不同的节点的数据并不完全一致


### 可用性 available

```text
A non-failing node will return a reasonable response within a reasonable amount of time (no error or timeout)
```

简单翻译为， 非故障的节点 在合理的事件内返回 合理的响应 (不是错误 或超时)



### 分区容忍性 partition tolerance

```text
The system will continue to function when network partitions occur
```

简单翻译为：当出现网络分区后，系统能够继续 “履行职责”



## CAP 应用

P 是必然需要的， 因为网络无法做到 100% 可靠。

- CP


- AP

## CAP 细节

CAP关注的粒度是 数据，而不是整个系统

CAP 是忽略网络延迟的

==正常运行情况下 ，不存在 CP的AP 的选择 可以同时满足CA==
CAP理论说 只能CP，AP，这里的前提是 发生了P， 在 系统没有发生 P 时， 可以保证 CA ， 所以需要考虑：
- P发生时， CP 还是 AP， 并且怎么保证
- P没有发生时， 怎么保证 CA


放弃并不等于什么都不做，需要为分区恢复后做准备。  
最典型的就是在分区期间记录一些日志，当分区故障解决后，系统根据日志进行数据恢复




## ACID，BASE

ACID 是数据库管理系统 为了保证 事务的正确性而提出的。
- atomic
- consisitency
- isolation
- durability


BASE 是 basically available， soft state，eventually consistency 的缩写。  

其核心思想是即使无法做到强一致性(CAP一致性就是强一致性)，但应用可以采用适合的方式达到最终一致性(Eventual Consistency)

- 基本可用 basically available
分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
这里的关键是 部分，核心。 选择哪些作为可以损失的业务，哪些是必须保证的业务。
。。。但是？你说 必须保证的业务， 那就可以必须保证？这不可能啊。。 我感觉基本可用就是， 用户直接访问的 那层接口 不能抛出异常，最差也是 返回 "您的数据已提交/处理中，请稍等"，也有日志/数据库 记录, 然后等待 后续 人工修复数据。 
。。不过好像也对，换一种说法： 用户 提交任务 这个 核心 功能 必须保证， 后面的 真正的处理逻辑 再说。 

- 软状态 soft state
允许系统 存在中间状态，且 这个中间状态 不会影响 系统的 整体可用性。

- 最终一致性 eventual consistency
系统中的所有数据副本经过 时间后，最终能够达到 致的状态。


==BASE 理论 本质上是对 CAP 理论的 延伸 和补充，更具体地说，是对 AP 方案的一个补充。==
- CAP 理论 忽略延迟， 但 实际应用中无法忽略
这就意味着 CP 是不存在的。

- AP 牺牲一致性 只是指 发生P的时候， 而不是 永远放弃一致性
这就是 base 延伸的地方，分区期间 牺牲一致性，但是 分区故障恢复后，系统应该达到 最终一致性。




# ch07 FMEA

FMEA (Failure mode and effects analysis ，故障模式与影响分析)又称为失效模式与后果分析、失效模式与效应分析、故障模式与后果分析等，本节采用“故障模式与影响分析”


FMEA 是一套分析和思考的方法，而不是某个领域的技能或工具。

软件架构设计领域， FMEA 并不是指导我们如何做架构设计，而是当我们设计出一个架构后，再使用 FMEA 对这个架构进行分析，看看架构是否还存在某些可用性的隐患


## ==FMEA方法==

FMEA的具体分析方法如下
- 给出初始的架构设计图
- ==假设架构中某个部件发生故障==
- ==分析==此故障对系统功能造成的==影响==
- 根据分析结果，判断架构是否需要进行优化

FMEA 分析的方法其实很简单，就是 FMEA 分析表，常见的 FMEA 分析表格包含如下部分
。。。就是 一个 可能的bug/故障 列表，然后人脑模拟 后果。如果后果严重，那么需要 补救。
。。还有 正常的用例。

1. 功能点
从用户的角度来看，而不是从系统内部各个模块功能。 比如 对于用户来说，登录，注册 是功能点， 底层的 数据库存储功能，redis 缓存 不是功能点

2. 故障模式
系统会出什么样的bug。 我们只需要假设 故障现象。
例如，MySQL 响应时间高达 3秒， 可能是由于 磁盘坏道，慢查询 网络连接， 我们只关心 3秒，不关心 具体原因 (具体原因 是 第5点)。

3. 故障影响
发生故障时，功能点会收到什么样的影响。  
比如，功能点偶尔不可用，完全不可用，部分用户功能点不可用，响应慢，功能点出错 等

4. 严重程度
致命、高、中、低、无 5档
以用户管理系统为例， 登录功能 比 修改用户资料 要重要，  80%用户 比 20%用户 范围更大， 完全无法登录 比 登录慢 更严重
- 致命， 70% 的用户无法登录
- 高， 30%的用户无法登录
- 中， 所有用户登录耗时 超过5秒
- 低， 10%的用户 登录耗时 超过5秒
- 中， 所有用户都无法修改资料
- 低， 20%的用户无法修改头像


5. 故障原因
为什么要列出 故障原因呢？
- 不同的故障原因 发生概率不同
- 不同的故障原因 检测手段不同
- 不同的故障原因 处理措施不同

6. 故障概率
分为 高中低 3档即可
- 硬件
随着时间推移，故障概率越来越高， 使用3年的硬盘，坏道的几率 比 新硬盘 高很多
- 开源系统
成熟的开源系统 bug较低；
刚发布的 开源系统 bug 较高。
有使用经验的开源系统 bug率低
刚开始尝试使用的 开源系统 bug率较高
- 自研系统
和开源系统类似， 成熟的 bug少， 新开发的bug多

7. 风险程度
风险程序 = 严重程度 * 故障概率


8. 已有措施
针对具体的故障原因，系统现在 是否提供了 某些措施来对应， 包括 告警，容错，自恢复

- 检测告警
最简单的措施就是 检测故障，然后 告警。 系统不处理故障，需要人工干预

- 容错
检测到故障后， 系统能通过 预先准备的手段 应对。  
例如 mysql 主备机， 当 业务服务器发现 主机无法连接后， 自动连接到 备机

- 自恢复
检测到故障后，系统能够自我恢复。

9. 规避措施
为了降低故障发生概率 而做的一些事情， 可以是技术手段，也可以是 管理手段
- 为了避免 新引入的 MongoDB 丢失数据， 在 mysql 中冗余一份
- 为了避免 单条路线被 挖断，同时 开通 电信移动联通 3条线路
- 为了降低 磁盘坏道的概率， 更换 服务时间超过2年的 磁盘
- 为了降低 某些疑难bug 的出现，  每周一凌晨4点 重启机器

10. 解决措施
为了解决 问题 而做的一些事情，一般都是 技术手段
- 为了解决密码 暴力破解， 增加 密码 重试次数 限制
- 为了解决 拖库 导致数据泄漏， 将数据库中 敏感数据 加密保存
- 为了解决 非法访问，增加 白名单控制

11. 后续规划
综合前面的分析，就可以看到 哪些故障 我们目前 还缺乏应对的 措施，哪些已有措施还不够。


## 7.3 FMEA 实战

。。就是一个 用户管理系统， 一台server ，一台mysql， server 还连接了一台 memcache

![6cee96ea6acd8ecac41b25cceeb8f5ff.png](../_resources/6cee96ea6acd8ecac41b25cceeb8f5ff.png)

。。改进措施 都得花钱啊。



# ch08 存储高可用

存储高可用方案的本质都是通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，其复杂性主要体现在如何应对 复制延迟和 中断 导致的数据不一致问题  
要从以下几个方面去进行思考和分析：
- 数据如何复制
- 各个节点的职责是什么
- 如何应对复制延迟
- 如何应对复制中断

常见的高可用存储架构有 主从，主备，主主，集群，分区



## 主备复制

最常见，最简单
几乎所有存储系统都提供 主备复制功能，如mysql，redis，mongodb 等

主备方案详细设计如下
1. 主机存储数据，通过 复制通道 将数据复制到 备机
2. 正常情况下，客户端 的 所有读写操作，都发给 主机，备机不对外提供 读写服务。
3. 主机故障的情况下，客户端不会自动将请求转发给备机，此时 系统处于不可用状态，不能读写数据。
4. 如果主机能够恢复，客户端继续访问主机，主机继续将数据 复制给备机
5. 如果主机不能恢复，则需要人工操作，将 备机升级为 主机，然后 让客户端访问 新的主机， 同时为了保持 主备架构，需要 人工新增 一台备机。
6. 主机不能恢复的情况下，成功写入主机，但没有复制到 备机的数据，需要人工排查 和 恢复。可能有数据 永久丢失，业务上需要考虑如何应对此类风险。
7. 主备间数据复制存在延迟，如果 延迟较多，恰好 主机又宕机，则可能丢失 较多数据， 因此对于 复制延迟 需要进行监控，当 延迟的数据量较大时，即使 告警，由人工干预处理。


优点是简单

缺点是
- 备机仅仅是备份，没有提供读写，浪费了硬件
- 故障后 需要人工干预，无法自动恢复


## 主从复制

主机负责读写操作，从机负责读操作，不负责写操作

和主备复制差不多，区别就是 
- 客户端可以 将读请求 发完从机。
- 主机故障时，依然可以读(并且 代码逻辑中会 读从机)，但不能写

也是
- 主机不能恢复，需要人工修复
- 注意复制延迟

主从和主备相比，有以下优点
- 主机故障时，读操作不受影响
- 从机发挥了 硬件性能

缺点
- 主从复制 比主备 更复杂， 主要是 客户端需要感知 主从关系，将不同的操作 发给不同的 机器。

还有 故障时 需要人工干预
不过故障 并不频繁

一般情况下，读多 写少的 适合 主从， 比如 论坛，BBS，新闻网站等， 读操作 是 写操作的 10倍甚至 100倍。



## 主备倒换 和 主从倒换

主备复制，主从复制 有2个共同的问题
- 主机故障后，无法进行写操作
- 如果主机无法恢复，则需要人工指定 新的主机

主备倒换，主从倒换 就是 为了解决上述2个问题 而产生的。
即，在原方案上 增加 倒换 功能，即 系统自动决定 主机角色，并完成 角色切换。

要实现一个倒换方案，必须考虑：
- 主备间状态判断
  主要包括2方面：
  - 状态传递的渠道，是相互连接，还是第三方仲裁
  - 状态检测的内容，机器是否断电，进程是否存在，响应是否缓慢
- 倒换决策
  - 倒换时机
    什么情况下，备机升级为主机
  - 倒换策略
    原主机恢复后，需要再次倒换，还是保持
  - 自动程序
    完全自动(系统判断并倒换)，还是 半自动(人工判断)
- 数据冲突解决


8.3.2 常见架构

根据状态传递渠道的不同，常见的主备倒换架构有3种： 互连式，中介式，模拟式

- 互连式
主机 备机 直接建立状态传递渠道
状态传递渠道 用于 传递状态信息，有多种实现方式
- 可以是 网络连接(例如，各开一个端口)， 也可以是 非网络连接(用串口线连接)
- 可以是 主机发送状态给 备机，也可以是备机 去主机 获得状态信息
- 可以和 数据复制 共用一个通道，也可以独立一条通道。
- 状态传递通道 可以一条，也可以多条

客户端也会有相应的改变：
- 为了 倒换后 不影响客户机的访问，主机 备机 共享一个 对 客户端来说 唯一的地址。例如 虚拟IP
- 客户端同时记录 主备机的地址， 哪个能访问 就访问哪个， 备机会拒绝访问，从机会拒绝写请求。

主要缺点是， 如果 状态传递渠道 本身就有问题，那么 从机 会 认为 主机故障 从而将自己升为 主机。


中介式
引入第三方中介，主备机之间不直接连接， 而是去连接中介。通过 中介 传递状态信息

比 互连式 更简单，因为
- 连接管理更简单
  直接连中介
- 状态决策更简单
  。。跳

MongoDB 的 Replica Set 就是这种方式

开源方案已有很成熟的 解决方案，就是 zookeeper， 用于 解决分布式应用中 经常遇到的一些数据管理问题，如 统一命名服务，状态同步服务，集群管理，分布式应用配置项的管理 等

主备倒换中的中介就可以用 ZooKeeper 来做状态同步


模拟式
备机模拟成一个客户端， 向主机发起 模拟的读写请求，根据 读写操作的结果 来判断 主机的状态。


## 主主复制

主主复制架构对数据的设计有严格的要求， 般适合于那些临时性、可丢失、可覆盖的数据场景。
例如 用户登录产生的 session 数据（可以重新登录生成〉，用户行为的日志数据（ 可以丢失），论坛的草稿数据（可以丢失）等。


## 数据集群

主备、主从，主主 架构本质上都有一个隐含的假设：主机能够存储所有数据

集群可以分为两类：数据集中集群、数据分散集群

### 数据集中集群

和 主备 主从 类似， 可以称为 一主多从/备， 

数据都只能往 主机写。

复杂度体现在
- 主机如何将数据复制给备机
- 备机如何检测主机状态
- 主机故障后，如何决定新的主机

目前开源的数据集中式集群以 zookeeper 为典型， zookeeper 通过 ZAB 协议来解决 上述的问题。


### 数据分散集群

多个服务器组成一个集群， 每台服务器 负责存储 一部分数据， 备份一部分数据

数据分散集群的复杂点在于如何将数据分配到不同的服务器上 算法需要考虑如下设计点：
- 均衡性
- 容错性
- 可伸缩性

hadoop
elasticsearch


### ==分布式事务==

2PC
3PC

#### 2PC

请求 和 提交 2阶段

基于以下假设
- 分布式系统中，存在一个 节点 作为 协调者 (coordinator)， 其他结点作为参数者 (cohorts)， 且节点之间可以通信
- 所有节点采用 预写式日志，且日志被写入后 即保存在可靠的存储设备上，不会丢失
- 所有节点不会永久性损坏，即使损坏，仍然可以恢复


- 第一阶段(提交请求阶段)
1. 协调者向所有 参与者 发送 query to commit， 并询问 是否可以 执行提交事务
2. 参与者执行询问发起位置的所有事务操作，并将undo，redo 信息写入日志，返回 yes 给 协调者， 如果执行失败，返回 no 。

- 第二阶段(提交执行阶段)
全部返回yes 给 协调者时
- 协调者向 所有参与者 发送 commit 的请求
- 参与者完成 commit，结束事务
- 参与者 返回 ack 给 协调者
- 协调者 收到 所有的 ack 后 完成事务

失败
- 向所有 参与者发送 rollback
- 参与者执行 回滚，结束事务
- 参与者返回 ack
- 协调者 收到所有 ack后，取消事务


优点是简单
缺点是：
- 同步阻塞
  协调者 和参与者 互相等待 对方的消息， 阻塞，不能处理其他事情
- 状态不一致
  commit命令 丢失，导致 部分机器 commit了，部分没有
- 单点故障
  协调者故障，无法发送 commit 或 rollback，则整个集群都会一直等待。



#### 3PC

针对 2PC 的 单点故障进行改进

在第一，第二阶段 插入一个 准备阶段。 当 协调者 故障后， 参与者可以 通过 超时 提交 来避免一直阻塞


第一阶段(提交判断阶段)
- 协调者向 参与者 发送 canCommit 消息，询问参与者 是否可以提交事务
- 参与者 收到 canCommit 后， 判断自己是否可以提交该事务(。。==没有执行==，可能只是看下 余额是否够之类的。)，并返回 yes 或 no
- 协调者 收到一个 no 或超时， 则事务终止，同时会通知 参与者 终止 事务； 如果在 超时前 收到 所有yes，则进入 第二阶段

第二阶段(准备提交阶段)
- 协调者发送 preCommit 给所有参与者，告知它们 准备提交
- 参与者收到 preCommit 后，执行事务，将 undo redo 记录到事务日志，然后返回 ack

第三阶段(提交执行阶段)
- 协调者在收到所有 ack 后 发送 doCommit，告知 参与者 正式提交。否则发送 回滚
- 参与者 收到 doCommit 后，提交事务，返回 haveCommited
- 如果 参与者 ==收到了preCommit 且 回复了 ACK， 但是 等待 doCommit 超时，则 超时后 继续提交事务==


避免了 2PC 的 协调者单点故障导致 系统阻塞， 但是 同样存在 数据不一致问题


### 分布式一致性算法

Paxos
特别复杂，难以理解
确实很多细节，难以实现

Raft
为工程实践而设计的

ZAB
ZooKeeper Atomic Broadcast Protocol




## 数据分区

前面讨论的 存储高可用架构基本是 基于 硬件故障的场景 去考虑和设计的， 主要考虑 部分硬件可能损坏的情况下 系统该如何处理。

但对于一些特别大的灾难来说， 有可能 所有硬件 全部损坏， 如 地震，水灾，整个区域 停电。
这种情况下， 我们要基于 地理级别的故障 来设计 高可用架构。


### 数据量

数据量的大小直接决定了分区的规则复杂度  
例如，使用 MySQL 来存放数据，假设一台 mysql 的存储能力是 500g，那么 2T 数据至少需要 4台mysql。  但如果数据时 200T，并不是 800台 mysql那么简单。 因为 数量上升后，复杂度会发生本质的变化，如
- 800台mysql服务器 可能每周 都有 一两台服务器故障，从 800台中 定位 2台服务器 并不容易
- 新增mysql， 分区配置 需要修改， 会影响800台。
- 如此大量的数据，地理位置全部集中在 某个城市，风险很大。


### 分区规则

洲
国家
城市


### 复制规则

常规的分区复制 有3中，集中式，互备式，独立式

集中式
存在一个总的备份中心，所有的分区都将数据备份到 备份中心

优缺点
- 设计简单， 各分区之间 无直接联系，互不干扰
- 扩展容易，新增分区，只影响数据原来所在的分区( 要从原分区 复制出来)，不会影响其他分区。
- 成本较高，需要建设一个 独立的备份中心


互备式
每个分区 备份另外一个分区的数据

优缺点
- 设计比较复杂，各个分区除了要承担 业务数据存储，还要承担 备份功能，相互 影响
- 扩展麻烦，影响现有的2个分区
- 成本低


独立式
每个分区都自己独立的 备份中心 ( 这个备份中心的 地理位置 要和 分区位置 不同(至少 市级别的不同))

优缺点
- 设计简单，各分区不影响
- 扩展容易，新增的分区 只需要搭建 自己的备份中心 即可
- 成本高

。。互备式应该最好， 新增节点 完全可以 让现有的某个节点 承担 2个节点的备份。  新增节点 不承担 备份功能，   然后下次 再新增时， 就让 第一次新增的 节点 承担 本次新节点的 备份功能。
。。或者 第二次新增时， 将  之前承担  2个节点的备份  抽取一个 让 第二次新增的节点 来备份，  第二次新增节点的备份 由 第一次新增的节点 来进行。


# ch09 计算高可用

计算高可用的主要设计目标是当出现 部分 硬件损坏 时，计算任务能够继续正常运行。

计算高可用的本质是通过 冗余 来规避部分故障的风险

。。这不就是 能感知服务器状态的 lb 吗？


计算高可用架构的设计复杂度主要体现在任务管理方面 ，即当任务在某台服务器上执行失败后，如何将任务重新分配到新的服务器进行执行

计算高可用架构设计的关键点有如下两点。
- 哪些服务器可以执行任务

- 任务如何重新执行



。。跳过了。 感觉不太可能有这种问题。。 基本都是 集群，或者单机。  不可能 主备 这种 纯粹 浪费 硬件的 做法的。
。。要进太空，当我没说。 而且 太空也是 2/3/4个一起算吧

9.1 主备
9.2 主从
9.3 对称集群，  就是普通的负载均衡的集群
9.4 非对称集群， 一主多从，部分任务 只有主机能执行， 部分任务只有 从机能执行



# ch10 业务高可用

## 异地多活

活 的反义词是 备， 活是提供服务的， 备只是备份，不提供服务。

异地多活的 代价很大
- 系统复杂性发生 质的变化，需要设计复杂的异地多活架构
- 成本会上升


异地多活架构 分为 同城异区，跨城异地，跨国异地

- 同城异区
将业务部署在同一个城市不同区的多个机房
同城的2个机房，距离一般几十km，通过搭建高速网络， 2个机房能实现 和同一个机房内 几乎一样的 网络传输速度。  
这意味着虽然是 2个不同地址位置的机房，但是==逻辑上 可以看做是 同一个机房==。 这 大大降低的 复杂度。

对于超大灾难，无能为力

- 跨城异地
部署在 不同城市的 机房， 距离最好远一些。

较远距离 带来的 网络传输延迟问题，给 业务多活架构 带来了 复杂性。
业务系统需要考虑 在 数据短时间不一致的情况下， 还能 正常提供业务。

对于严格一致性的，如 金融类的系统， 对于 余额这类数据 ，不能做 跨城异地 (防止 恶意利用 延迟 来将 余额多次转出) 的 多活架构， 只能 同城异区。

对于 数据一致性 要求不那么高的，或数据不怎么变的，或数据丢失也无所谓的，可以 跨城异地多活， 如 用户登录，新闻类网站，微博类网站


- 跨国异地
主要场景
- 为不同地区用户提供服务
- 只读类业务 做多活



---

### 异地多活设计技巧

1. 保证核心业务的异地多活
2. 核心数据最终一致性
3. 采用多种手段同步数据
4. 只保证绝大部分用户的异地多活

### 异地多活设计步骤

1. 业务分级
  只为核心业务设计异地多活
  - 访问量大的业务
  - 核心业务
  - 产生大量收入的业务
2. 数据分类
  - 数据量
  - 唯一性
  - 实时性
  - 可丢失性
  - 可恢复性
3. 数据同步
  - 存储系统同步
  - 消息队列同步
  - 重复生成
4. 异常处理


## 接口级的故障应对方案

典型表现就是 系统没有宕机，网络也没有中断，但是业务出现了问题。例如，业务响应慢，大量访问超时，大量访问出现异常。  
这类问题的原因在于 系统压力太大，负载太高，导致无法快速处理业务请求，由此引发更多的 后续问题。

原因一般有
- 内部原因
  程序bug导致死循环，程序逻辑不完善导致耗尽内存。
- 外部原因
  黑客攻击，促销，抢购 引入了 超出平常几倍 甚至 几十倍的用户。 第三方系统大量请求， 第三方系统响应缓慢 等

解决接口级故障的 核心思想 和 异地多活 基本类似： 优先保证 核心业务，优先保证 绝大部分用户。

### 降级

系统将某些业务 或接口的 功能降低，可以是 只提供部分功能，也可以完全停掉所有功能

实现降级的方式
- 系统后门降级
  系统预留了 后门用于降级操作。 例如，系统提供一个 降级URL，当访问这个url时，就相当于 执行降级指令，具体的降级指令通过 url 的参数 传入。
  有一定的安全隐患。
  成本低，但是 如果服务器数量多，需要一台一台操作，效率较低 (。。直接脚本 访问所有的服务就可以了吧)。

- 独立降级系统
  将降级操作 独立到一个 单独的系统中，可以实现 复杂的 权限管理，批量操作等。


### 熔断

降级的目的是 应对系统自身的故障。  
熔断的目的是 应对依赖的外部系统发生了故障的情况

比如，A服务 x功能 依赖了 B服务的某个接口， 当B服务的 接口响应很慢时， A服务的 x功能 响应也很慢。  
这时就需要 熔断机制， 即A 不再调用 B 的接口， 而是直接返回错误。

熔断机制实现的关键是需要有一个统一的 API 调用层，由 API 调用层来进行采样或统计，如果接口调用散落在代码各处就没法进行统一处理


### 限流

降级是从 系统功能优先级的角度 考虑如何应对故障。  
限流 是从用户访问压力的角度 来考虑如何应对故障。

限流指只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃

- 基于请求限流
从外部访问的请求角度考虑限流，常见的方式有限制总量和限制时间量
限制总量的方式是限制某一指标的累积上限，常见的是限制当前系统服务的用户总量
限制时间量指限制一段时间内某个指标的上限， 例如，1分钟内只1万个用户访问，每秒请求峰值最高为 10 万

实现简单，但是 实践中 的主要问题是 难以找到 合适的阈值

为了找到合理的阈值，需要进行 性能压测。

基于阈值来限制访问量的方式 更多适用于 业务功能比较简单的系统，如 LB，gateway，抢购系统


- 基于资源限流
基于请求限流 是从系统外部考虑的，
基于资源限流 是从系统内部考虑的。
找到 系统内部 影响性能的 关键资源，对其使用上限进行限制。
常见的内部资源有 连接数，文件句柄，线程数，请求队列 等。

实践中也有2个难点
- 如何确定关键资源
- 如何确定关键资源的阈值

通常，这是一个 逐步调优的过程。


### 排队

是限流的一个变种， 限流 直接拒绝用户， 排队是 让用户等待很长时间。

排队需要 临时缓存大量的业务请求， 单个系统内部 无法缓存那么多的数据，一般情况下，要使用 独立的系统实现。 例如 kafka 这类 消息队列




# 第四部分 可扩展架构模式

# ch11 可扩展模式

基本思想可以总结为一个字 ==拆==

拆，就是将 大系统 拆分为 多个 小系统， 扩展时 只修改其中一部分即可，无须整个系统到处修改。 通过 拆分为小系统 来减少改动范围，降低改动风险

==常见的拆分思路==
- 面向流程拆分
  将整个业务流程 拆分为 几个阶段，每个服务作为一部分
- 面向服务拆分
  将系统提供的服务拆分，每个服务作为一部分
- 面向功能拆分
  将系统提供的功能拆分，每个功能作为一部分

关键在于如何理解 ==流程，服务，功能== 3者的联系和区别。

从范围上来说， 流程 -> 服务 -> 功能 。

以TCP/IP为例，来说明 流程，服务，功能的 区别和联系

![89c345c76e098ae34b9c1be7f218aa72.png](../_resources/89c345c76e098ae34b9c1be7f218aa72.png)


- 流程
TCP/IP 的流程是 应用层-传输层-网络层-物理链路层  
无论最上层的 应用层是什么， 这个流程 都不会变

- 服务
对应应用层的 http,ftp,smtp， http提供web服务，ftp提供文件服务，smtp提供邮件服务

- 功能
每个服务都会提供相应的功能。 http提供get，post， ftp提供上传下载功能，smtp提供邮件发送和收取功能


## 可扩展方式

合理的拆分 能够强制保证 即使程序员出错，出错的范围也不会很大。

不同拆分 应对扩展时的 优势如下
- 面向流程拆分
  扩展时，大多数情况下 只需要修改某一层。
- 面向服务拆分
  对某个服务扩展，或增加新的服务时， 只需要扩展相关服务即可
- 面向功能拆分
  对某个功能扩展，或增加新功能时，只需要扩展相关功能即可


不同的拆分，会得到不同的系统架构，典型的可扩展系统架构如下
- 面向流程拆分： 分层架构
- 面向服务拆分， SOA，微服务
- 面向功能拆分， 微内核架构



# ch12 分层架构

很常见的架构模式， 也叫 N 层架构。
如 C/S，B/S，MVC，MVP，逻辑分层架构


## 12.2 分层架构详解

无论何种分层， ==分层架构设计 最核心的一点就是 需要保证 各层之间的 差异 足够清晰，边界足够明显==

分层的差异不明显，就会 有些程序员认为应该放 这层，有些认为应该放 那层， 导致 分层混乱， 会失去 分层的意义


分层架构之所以能够较好地支撑系统扩展，本质在于：隔离关注点 (separation of concerns), 即每个层中的组件只处理本层的逻辑

分层时要保证层与层之间的依赖是稳定的，才能真正支撑快速扩展

分层结构的另外一个特点就是层层传递，也就是说一旦分层确定，整个业务流程是按照层进行依次传递的，不能在层之间进行跳跃。

分层结构的这种约束，好处在于强制将 分层依赖限定为 两两依赖，降低了整体系统复杂度




# ch13 SOA架构

service oriented architecture
面向服务的架构

SOA 更多是在传统企业（例如，制造业、金融业等）落地和推广，在互联网行业并没有大规模地实践和推广

SOA 提出的背景是企业内部的 IT 系统重复建设且效率低下， 主要体现在
- 企业各部分 有独立的 IT系统， 如 人力资源系统，财务系统，销售系统，这些系统 可能都涉及人员管理。 各IT部分 需要重复开发人员管理功能
- 随着业务发展，复杂度越来越高，更多的流程和业务 需要 多个 IT系统合作完成。
- 各个独立的 IT系统可能采购于不同的供应商，实现技术不同。


## SOA详解

对于 传统IT系统 存在的 问题，SOA提出 3个关键概念

- 服务
  所有业务功能都是一项服务，服务就意味着 需要对外提供开放的能力。其他系统可以使用这项功能，并且不需要 定制化开发
- ESB
  enterprise service bus
  企业服务总线
  将企业中各个不同的服务连接在一起， 各个服务是异构的，没有同一个标准，需要使用 ESB 来屏蔽异构系统 对外系统提供的各种不同的接口方式
- 松耦合
  减少各个服务间的 依赖和相互影响， 但 要做到 完全向后兼容 并不容易


SOA 解决了传统 IT 系统重复建设和扩展效率低的问题，但其本身也引入了更多的复杂性。
SOA 最广为人垢病的就是 ESB, ESB 需要实现与各种系统间的协议转换、数据转换、透明的动态路由等功能


如果我们是重新构建整个企业的 IT 系统，完全可以从开始就制定好各种规范，那么 SOA ESB 就无须存在了



# ch 14 微服务


## 微服务和SOA的关系

SOA 和 微服务 的关系和区别， 大概分为几个典型的观点

- 微服务是 SOA的实现方式
这种观点认为 SOA 是一个架构理念， 微服务是 SOA 理念的一种 具体实现方法。 例如， 微服务就是 使用 http restful协议来实现 ESB的 SOA。

- 微服务是 去掉 ESB 后的 SOA

- 微服务是 一种和 SOA 相似 但本质上不同的 架构理念


我们先对比下 SOA 和微服务的一些具体做法
- 服务粒度
整体上来说，SOA的服务粒度要粗一些， 微服务的粒度要细一些。
比如，对于大型企业来说， 员工管理系统就是一个 SOA架构中的服务， 但是 采用 微服务的话， 员工管理系统 会被拆分出 更多的服务，如 员工信息管理，员工考勤管理，员工假期管理，员工福利管理 等
- 服务通信
SOA采用 ESB 作为 服务间通信的关键组件 ，负责 服务定义，服务路由，消息装换，消息传递， 总体上是 重量级的实现。
微服务 推荐使用 统一的协议和格式， 如 restful 协议， RPC 协议。
- 服务交付
SOA 对服务的交付没有特殊要求， 因为SOA 更多是 考虑 兼容已有的服务
微服务 要求 快速交付，相应地要求 采用 ==自动化测试，持续集成，自动化部署等 敏捷开发相关的最佳实践==
如果没有这些基础能力支持， 微服务的 规模一旦变大 (例如，超过20个微服务)，整体就难以达到 快速交付的要求。 这也是很多企业 在 实行 微服务时 踩到的坑，就是 系统拆分为 微服务后， 部署的成本 指数上升
- 应用场景
SOA 更适合 庞大，复杂，异构的 企业级系统。
微服务更适合 快速，轻量级，基于web的 互联网系统， 这些 系统 业务变化快，需要快速尝试，快速交付


SOA 和微服务是两种不同理念的架构模式，并不存在孰优孰劣，而只是应用场景不同而己


## ==微服务的陷阱==

微服务的坑

- 服务划分过细，服务间关系复杂
服务划分过细，单个服务的复杂度确实下降了，但是整个系统的复杂度却上升了。
- 服务数量太多，团队效率急剧下降
有的团队人员只有5-6个，但是拆分出 30多个微服务，平均一个人要维护5个以上的微服务。 工程师需要在 设计，开发，测试，部署 不同角色间切换， 甚至并行。
- 调用链太长，性能下降
微服务之间通过http 或 rpc 调用，每次调用必然通过网络，一般线上 的 业务接口之间的 调用，平均响应事件是 50ms，如果 一个请求需要 经过 6次微服务调用，则性能就是 300ms。
- 调用链太长，问题定位困难
- 没有自动化支持，无法快速交付
  - 没有自动化测试，人工每次测试 需要测试大量接口
  - 没有自动化部署， 人工每次部署6-7个服务，几十台机器。
  - 没有自动化监控， 人工 定位故障
- 没有服务治理，微服务数量多了以后 管理混乱
随着微服务种类和数量 越来越多，如果没有服务治理系统 进行支持，微服务提倡的 lightweight 就会成为问题，主要问题如下
  - 服务路由， 假设微服务有60个节点，部署在20台服务器上，其他依赖的微服务如何知道 这个部署情况？
  - 服务故障隔离， 上述60个节点中有5个节点出了故障，依赖的微服务 如何处理这种情况？
  - 服务注册和发现， 从60个节点 扩容到 80个，或 缩减为40个，新增或减少的节点 如何让 依赖的服务知道呢？



## 14.4 微服务最佳实践

综上所述，微服务的坑可以提炼为以下几点
- 微服务拆分过细，过分强调 small
- 微服务基础设施不健全，忽略了 automated
- 微服务并不轻量，规模大了以后，lightweight不再适应

### 服务粒度

针对微服务拆分过细导致的问题，建议基于团队规模进行拆分，比如 2个披萨理论(团队人数不能多到 2个披萨不够吃的地步)。

笔者给出 3个火枪手 的微服务拆分粒度原则，即一个微服务3个人负责开发。 当团队规模扩大时， 将现有微服务继续拆分。

3个火枪手 主要用于 微服务的设计 和开发阶段。 进入稳定期 和维护期后，无须太多开发，1人维护一个，或多个微服务都是可以的。

### 拆分方法

基于三个火枪手的理论，我们可以计算出 拆分后 合适的服务数量。

拆分方式有如下几种
- 基于业务逻辑拆分
最常见的拆分方式，将系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务。 
基于3个火枪手的原则计算大概的服务数量，然后 确定合适的 职责范围。

- 基于可扩展拆分
将系统中的业务模块按照稳定性进行排序，将已经成熟的 和 改动不大的服务拆分为 稳定服务，将经常变化的 和迭代的 服务拆分为 变动服务。
稳定服务的粒度可以粗一些，即使逻辑上没有关联的服务，也可以放一起，比如 日志服务 和 升级服务 放在同一个子系统中。

- 基于可靠性拆分
将系统中的业务模块按照优先级排序，将可靠性要求高 的核心服务 和 可靠性要求低的 非核心服务 拆分开。
重点保证核心服务的 高可用。
这样的好处：
  - 避免非核心服务故障影响 核心服务
  - 核心服务高可用方案可以更简单
  - 能够降低高可用成本

- 基于性能拆分
和基于可靠性拆分类似，将 性能要求高 或 性能压力大的 模块拆分出来，避免 性能压力大的服务 影响其他服务。


### ==基础设施==

如果 automated 相关的基础设施不健全，那么补起来 不是1天2天的事，是以 年 为单位的。
为什么要这么长？ 因为 基础设施太多了：
- 服务发现
- 服务路由
- 服务容错
- 服务监控
- 服务跟踪
- 服务安全
- 自动化测试
- 自动化部署
- 配置中心
- 接口框架
- API网关

这里简单介绍下每个 基础设置的主要作用

- 自动化测试
拆分后，接口数量大增， 微服务提倡 快速交付，版本周期短，更新频繁， 靠人工回归测试 效率太低，无法 快速交付。
自动化测试，包括， 单元测试， 单个系统级的集成测试， 系统间的接口测试

- 自动化部署
微服务需要部署的节点很多，频率也大幅上升。
自动化部署包括 版本管理，资源管理，部署操作，回退功能

- 配置中心
微服务节点很多，登录机器 手工修改 太慢。
配置中心包括 配置版本管理( 一个微服务，10个节点是 移动用户的， 10个节点是联通的，配置项一样，但是值不同)，修改配置，节点管理，配置同步，配置推送

- 接口框架
微服务提倡轻量级的通信方式，一般使用 http 或 rpc。
实践过程中，还需要 统一接口传递的 数据格式， 如 json 以及 json的数据规范。

- API 网关
内部的微服务是互联互通的。
外部系统想要调用 系统的某个功能，不可能直接调用 某个微服务的 某个接口， 因为 对于外部系统来说， 几十个微服务 很难分辨。 所以需要一个 统一的 api网关 来为外部提供 服务，进行统一的 安全和权限。
所有外部系统 都要通过 api 网关，主要包括 鉴权， 权限控制， 传输加密，请求路由，流量控制等功能

- 服务发现
微服务种类和数量很多，如果 全部靠 手工配置，那么 工作量太大。
需要一套 服务发现 来 支撑 微服务的 自动注册和发现
服务发现主要有2种： 自理式 和代理式
自理式，就是指每个微服务自己完成服务发现， 即 自己访问 服务注册中心 获得 实例信息，然后自己决定 访问哪个实例。 比较简单
代理式， 微服务之间有一个 LB， 由lb 完成 微服务之间的 服务发现。看起来更清晰，微服务实现也简单很多， 但是 存在单点问题，一旦 lb 故障，会 影响 微服务之间的调用，  还有 性能问题，所有的 流量 都要经过 lb。 lb变成集群，又增加复杂度。
。。自理式 只要能访问到一次 服务注册中心， 就可以缓存到本地了， 后续 服务注册中心 宕机了 也无所谓。
服务发现的核心功能就是服务注册表，注册表记录了所有的服务节点的配置和状态


- 服务路由
有了服务发现后，微服务可以获得 相关的配置信息， 但是具体进行 调用时，还需要从 所有符合条件的 可用实例 中 挑选一个 发起请求，这就是 服务路由 需要完成的功能
服务路由和服务发现 紧密相关。
对于自理式，服务路由 由 微服务内部实现
对于代理式， 由 lb 实现。
服务路由的核心功能就是 路由算法，常见的有： 随机路由，轮询路由，最小压力路由，最小连接数路由 等

- 服务容错
微服务的故障，如果不及时处理，扩扩散出来。
因此需要 微服务能自动应对这种出错场景。
常见的服务容错包括 请求重试，流控，服务隔离

- 请求重试
请求重放 和 请求重试 类似，不同在于， 重试是 向一个 微服务的节点 重新发送请求， 重放 是向不同节点 重新发送请求
通常情况下， 请求重试 由 微服务节点 或 代理节点 实现

- 流控
出现异常情况，导致某个 或某类微服务的请求数量突增 或 爆发，由于 系统容量限制，无法快速应对突发流量，导致整个微服务响应变慢 甚至瘫痪，此时需要 拒绝一部分流量
通常情况下，流控 由 各个微服务节点自己实现，可以将流控策略包装成 公共库 给每个微服务使用。

- 服务隔离
当某个节点故障时，最简单，最快的方法是 将故障节点下线，避免故障扩散， 这就是 服务隔离需要实现的功能
通常情况下，服务隔离分为 主动隔离，被动隔离，手动隔离
主动隔离，指 微服务节点 自己判断 自己异常后，主动从 服务发现系统中注销
被动隔离，服务发现 系统 根据预设的规则 判断 微服务节点故障后，将其 从服务发现 系统中注销
手动隔离，人工判断 系统故障后， 手动从 服务发现系统中 注销。

- 服务监控
节点数量大大增加，需要监控的机器，网络，进程，接口调用 等 监控对象 大大增加。
主要作用
  1. 实时收集信息并分析，避免故障后 才进行分析， 减少处理时间
  2. 服务监控可以在实时分析的基础上进行预警，在问题萌芽节点 发现并预警，降低 问题影响的范围和时间
通常情况下，服务监控需要搜集并分析 大量的数据，因此建议做出独立的系统，而不要 集成到 服务发现，api网关中

- 服务跟踪
服务监控可以做到 微服务节点级别的 监控和信息收集，但如果我们需要跟踪某个请求 在 微服务中的 完整路径， 服务监控 是难以实现的。
目前无论分布式跟踪，还是微服务的服务跟踪，绝大部分请求跟踪的 实现技术 都基于 google 的 Dapper 论文 《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》
关键技术有如下几个
  1. 标注点
  又称 植入点 或 埋点。 通过在 app 中中间件中 明确定义 一个 全局的 标注( 特殊id)， 跟踪系统根据这个 标注 将 整个 业务请求串联起来
  需要代码植入。可以作为一个 小的通用组件。
  2. 跟踪树和span
  分布式跟踪通过跟踪树来表示一个完整的跟踪流程，其中某个服务从接到请求到返回响应这个时间跨度范围被称为 span ，一个 span 内，服务本身又会发起多次到其他服务的调用
服务跟踪一般用于2个目的
采样跟踪
  根据一定概率 对请求进行采样跟踪，分析，可以用于发现系统问题。 但 它更通常用于 探测性能不足。
染色跟踪
  线上可能出现比较奇怪的问题，即同样的功能，大多数用户没有问题，很少一部分用户总出问题，从日志，代码 看不出问题原因， 此时需要 针对 单个用户的 特定请求 进行 全链路跟踪，这就是 染色跟踪


- 服务安全
从业务角度，部分敏感数据 只能部分微服务访问。
分为3部分
接入安全
只有经过允许，某个微服务才可以访问另外一个微服务
数据安全
某些数据相关的操作 只允许授权的 微服务进行访问
传输安全
敏感数据在传输过程中要 防窃取，防篡改， 保证数据的真实性，有效性
通常情况下，服务安全可以 集成到 配置中心中




# ch15 微内核架构

microkernel architecture

也称为插件化架构

核心系统，插件模块

核心系统负责 与具体业务无关的通用功能，例如 模块加载，模块间通信  
插件模块负责 实现具体的业务逻辑。

核心系统功稳定，不会因为业务功能扩展 而不断修改。
插件模块可以根据业务功能的需要 不断扩展。 

微内核架构通过隔离变化到 插件的 方式提供了灵活性， 可扩展性

。。这就是 微服务啊。 看后续的 规则引擎，像是 模板方法

## 设计关键点

- 插件管理
核心系统需要知道 当前 哪些插件可用，如何加载，什么时候加载。
常见的实现方法是 插件注册表 机制。 插件注册表 包含 每个插件模板的信息，包括 名字，位置，加载时机(启动加载，需要时加载) 等
- 插件连接
插件如何连接到 核心系统。
常见的连接机制有 OSGi，消息模式，依赖注入，甚至使用分布式协议(如 http，rpc)
- 插件通信
插件间的通信。


## OSGi 架构

1. 模块层
插件管理功能， 
OSGi中，插件被称为bundle， 每个bundle都是一个 jar文件，包含一个 MANIFEST.MF 包含 名称，描述，开发商，classpath，需要导入，输出的包
2. 生命周期层
完成插件连接功能
提供执行时模块管理，模块对底层OSGi 框架的访问。 
精确定义了bundle生命周期的操作： 安装，更新，启动，停止，卸载
3. 服务层
完成插件通信功能
OSGi 提供一个 服务注册的功能， 每个插件 将自己能提供的服务 注册到 OSGi 核心的 服务注册中心。


## 规则引擎架构简析

执行引擎 可以看做微内核
规则引擎在 计费，保险，促销 等业务领域应用比较多。

例如，电商促销，常见的促销规则有
满100送50
3件减50
3件8折
第三件免费
跨店满200减20
新用户减20

规则引擎可以很灵活地应对这种需求，因为
- 可扩展
- 易理解
- 高效率

规则引擎的基本架构
1. 将业务功能分解提炼为 多个规则，保存到 规则库
2. 根据业务，将规则排列组合，配制成 业务流程，保存到 业务库
3. 规则引擎 执行 业务流程，实现业务功能

目前常用的 规则引擎 是 开源的 JBoss Drools



# 第五部分 架构实战

# ch16 消息队列设计实战

识别复杂度

设计备选方案
- kafka
- 集群+mysql
- 集群+自研存储

评估和选择备选方案

细化方案


# ch17 互联网架构演进


如何推动技术的发展

- 潮流派
热衷于新技术，紧跟潮流
- 保守派
稳定压倒一切
- 跟风派
跟着 竞争对手的步子。

淘宝
qq
微信



---

初创期
发展期
架构期
竞争期
成熟期

---

用户规模 影响 性能，可用性



# ch18 互联网架构模板

互联网标准技术架构如下

![89d40128c95be9bd95424c7ddb42b363.png](../_resources/89d40128c95be9bd95424c7ddb42b363.png)


## 存储层技术

### sql

数据库拆分， DBProxy，TDDL，MySQL Router，Atlas


### nosql

memcache 的 k-v
redis 的复杂数据结构
mongodb 的文档数据结构

nosql无一例外都 将 性能作为 卖点

200台机器，利用率提高10%，就可以节约20台

统一存储平台(至少 上百台nosql 后需要) 实现：
- 资源动态按需分配
- 资源自动化管理
- 故障自动化处理


### 小文件存储

1mb以下，数量巨大

底层平台 HBase，Hadoop，Hypertable，FastDFS

开源方案 淘宝的TFS，京东的JFS，Facebook的Haystack



### 大文件存储

视频，海量日志数据

Hadoop, HBase, Storm, Hive

淘宝 云梯系统，腾讯TDW


## 开发层技术

### 开发框架

java 的 ssh， spring mvc， play
ruby 的 RoR
php 的 ThinkPHP
python 的 Django

原则：优选成熟的框架，避免盲目追逐新技术


### web服务器

java 的tomcat，jboss，resin
php/python 的 nginx

最保险使用 apache，支持多种语言。
apache性能无所谓，当 apache性能无法满足 业务的时候， 你有足够的钱，人 来进行 更新


### 容器

docker

千万不要以为 Docker 只是一个虚拟化或容器技术，它将在很大程度上改变目前的技术形式
- 运维方式发生革命性变化
  docker启动快，几乎不占资源，随时启停， 基于docker 打造自动化运维，智能化运维 将 成为主流方式
- 设计模式发生本质上的变化
  启动一个新的容器实例的 代价如此低， 将鼓励 设计思路 朝 微服务 方向发展


## 服务层技术

### 配置中心

集中管理各个系统的配置

配置分散的坏处
- 上线时，检查配置 需要较多时间
- 处理线上问题时， 查找配置  低效
- 没有自动校验机制


配置中心的好处
- 集中配置多个系统，效率高
- 所有配置都在一个地方，检查方便，协作效率高
- 配置中心可以实现 程序化的规则检查，避免常见错误 (最小值，最大值，是否IP地址，是否URL地址，都可以用正则表达式完成)
- 配置中心相当于 备份了系统的配置。  要 搭建新环境时，直接拷贝一份。

通过 系统标识 + host + port  来唯一 标识一个系统的运行实例


### 服务中心

系统数量不多时，系统间的调用一般都是 直接通过 配置文件 记录在 各系统内部。 数量多了以后，这种方式 就存在问题
- 一旦一个接口被替换，需要修改很多地方。 
- 部分实例故障后，需要修改配置 才能 调用其他实例

服务中心 用于解决 跨系统依赖的 配置 和 调度问题
一般有2种方式， 服务名字系统， 服务总线系统

服务名字系统
将 service 名字 解析为 host + port + 接口， 然后 调用方 调用 host+port+接口

服务总线系统
调用方 调用 service 的名字， 由 服务总线 自动选择 host + port + 端口。

。。就是之前 服务发现 的 自理式， 代理式


### 消息队列

传统的 异步通知 是 消息生产者直接调用 消息消费者， 当 业务变大，子系统数量增多， 会导致 系统间交互 变得复杂。

消息队列 就是为了实现这种 跨系统异步通知的 中间件系统。
消息队列 可以 一对一， 也可以一对多。

引入消息队列后
- 整体结构 从 网状 变成了 线性结构
- 消息 生产 和 消费 解耦
- 增加新消费者，不需要 修改 生产者， 扩展方便
- 消息队列 系统可以做 高可用，高性能，避免 各业务系统 独立做一套。
- 业务系统 只需要 聚焦业务即可

RocketMQ，kakfa，ActiveMQ，RabbitMQ

如果业务 对 消息的 可靠性，时序，事务性 要求较高，则要深入研究，否则容易踩坑


## 网络层技术

### 负载均衡

DNS 地理位置lb
F5  集群lb
nginx，LVS 机器lb



### CDN




### 多机房

同城多机房

跨城多机房

跨国多机房

### 多中心


## ==用户层技术==

### 用户管理

SSO， 实现手段有 cookie，token， 最有名的是 CAS

授权登录

### 消息推送

消息推送主要包含3个功能： 设备管理(唯一标识，注册，注销)， 连接管理，消息管理

挑战：
- 海量设备和用户管理
- 连接保活
- 消息管理


### 存储云和图片云


都是基于 CDN + 小文件存储


## 业务层技术

业务越来越多，降低复杂性的最好方式就是 拆

子系统越来越多，几百上千个后， 没人说的清 业务的调用流程，排查也困难。 这次 合 。  将关联性较强的子系统 合成一个 虚拟业务域 ，类似于 设计模式 Facade


## 平台技术


### 运维平台

核心职责： 配置，部署，监控，应急

设计要素： 标准化，平台化，自动化，可视化


### 测试平台

单元测试，集成测试，接口测试，性能测试

用例管理
资源管理
任务管理
数据管理

### 数据平台

核心职责： 数据管理，数据分析，数据应用

![ff84b9ce2e3b00b2388afc92caf09f5d.png](../_resources/ff84b9ce2e3b00b2388afc92caf09f5d.png)



### 管理平台

核心是 权限管理

分为2部分，身份认证，权限控制










# ch19 架构重构 

有的放矢

。跳

# ch20开源系统

。跳



2024.06.12 09:24

